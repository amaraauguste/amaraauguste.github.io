<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <!-- Default CSS style for dark mode -->
  <link id="dark-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/base16/bright.css" />

  <!-- CSS style for light mode -->
  <link id="light-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/felipec.css" disabled />

  <link rel="stylesheet" href="../3620style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

  <script>
    hljs.highlightAll();
  </script>
  <title>CISC 3620</title>
</head>

<body>
  <nav id="navbar">
    <header class="header-link">
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" style="color: inherit; text-decoration: none;"
        class="header-text">CISC 3620</a>
    </header>
    <div class="mode">
      Dark mode:
      <span class="change" onclick="toggleMode()">OFF</span>
    </div>
    <hr />

    <ul>
      <!--Chapter 1 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">Introduction</a>
        <div class="dropdown-container">
          <a href="#What_is_Computer_Graphics?" onclick="closeDropdown()">What is Computer Graphics?</a>
          <a href="#Types_of_Computer_Graphics" onclick="closeDropdown()">Types of Computer Graphics</a>
          <a href="#Applications_of_Computer_Graphics" onclick="closeDropdown()">Applications of Computer Graphics</a>
          <a href="#The_Computer_Graphics_System" onclick="closeDropdown()">The Computer Graphics System</a>
        </div>
      </li>
      <!--Chapter 2 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">2D Computer Graphics</a>
        <div class="dropdown-container container2">
          <a href="#Types_of_2D_Graphics" onclick="closeDropdown2()">Types of 2D Graphics</a>
          <a href="#Pixels_and_Coordinate_Systems" onclick="closeDropdown2()">Pixels and Coordinate Systems</a>
          <a href="#Color_Models" onclick="closeDropdown2()">Color Models</a>
          <a href="#Shapes" onclick="closeDropdown2()">Shapes</a>
          <a href="#JavaScript" onclick="closeDropdown2()">Basic JavaScript</a>
          <a href="#Intro_to_HTML5_Canvas" onclick="closeDropdown2()">Intro to HTML5 Canvas</a>
          <a href="#Polygons_and_Curves" onclick="closeDropdown2()">Polygons and Curves</a>
          <a href="#Mouse_Events" onclick="closeDropdown2()">Mouse Events</a>
          <a href="#Additional_Events" onclick="closeDropdown2()">Additional Events</a>
          <a href="#Transforms" onclick="closeDropdown2()">Transforms</a>
          <!--<a href="#Linear_Algebra" onclick="closeDropdown2()">Linear Algebra</a>
          <a href="#Variable_Scope" onclick="closeDropdown2()">Variable Scope</a>
          <a href="#Global_Variables" onclick="closeDropdown2()">Global Variables</a>
          <a href="#Operators" onclick="closeDropdown2()">Operators</a>
          <a href="#Floating-Point_Numbers" onclick="closeDropdown2()">Floating-Point Numbers</a>
          <a href="#Math_Methods" onclick="closeDropdown2()">Math Methods</a>
          <a href="#Error_Types" onclick="closeDropdown2()">Error Types</a>-->
        </div>
      </li>

      <!--Chapter 3 Dropdown-->
      <!--<li>
        <a class="dropdown-btn" href="#Input_and_Output" style="font-size: 20px">Input and Output</a>
        <div class="dropdown-container container3">
          <a href="#System_Class" onclick="closeDropdown3()">System Class</a>
          <a href="#Data_Types" onclick="closeDropdown3()">Data Types</a>
          <a href="#Reading_Input" onclick="closeDropdown3()">Reading Input</a>
          <a href="#Literals_and_Constants" onclick="closeDropdown3()">Literals and Constants</a>
          <a href="#Putting_it_all_Together" onclick="closeDropdown3()">Putting it all Together</a>
          <a href="#Program_Structure" onclick="closeDropdown3()">Program Structure</a>
          <a href="#Using_Files" onclick="closeDropdown3()">Using Files</a>
        </div>
      </li>-->

      <li><a href="#Reference" style="font-size: 20px">Reference</a></li>
      <!-- add more links here -->
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" class="previous backbutton"
        style="font-size: 20px;">&laquo; Back</a>
      <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
    </ul>
  </nav>

  <main id="main-doc">
    <!-- WEEK 1 NOTES -->
    <!-- WEEK 1 DAY 1-->


    <section class="main-section" id="What_is_Computer_Graphics?">
      <br />
      <header><b>What is Computer Graphics?</b></header>
      <article>
        <p>The term "Computer Graphics" is concerned with all aspects of producing pictures or images using a computer.
        </p>
        <p>It encompasses the <b>creation</b>, <b>manipulation</b>, and <b>representation of images and animations</b>
          on computers.</p>
      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Types_of_Computer_Graphics">
      <br />
      <header><b>Types of Computer Graphics</b></header>
      <article>
        <p>Computer graphics can be broadly classified into two types: two-dimensional (2D) and three-dimensional (3D)
          graphics.</p>
        <p><b>2D Graphics:</b> are digital images that are computer-based. </p>
        <p>They include 2D geometric models, such as image compositions,
          pixel art, digital art, photographs, and text.</p>
        <p>2D graphics or computer generated images are used everyday on traditional printing and drawing. </p>
        <p><b>3D Graphics:</b> are graphics that use 3D representation of geometric data.</p>
        <p>This geometric data is then manipulated by computers via 3D computer graphics software in order to customize
          their display,
          movements, and appearance.</p>
        <p>3D computer graphics are often referred to as 3d models. A 3d model is a mathematical representation of
          geometric data that is contained in a data file. 3D models, can be used for real-time 3D viewing in
          animations, videos,
          movies, training, simulations, architectural visualizations or for display as 2D rendered images (2D renders)
        </p>
      </article>
      <br />
    </section>
    <section class="main-section" id="Applications_of_Computer_Graphics">
      <br />
      <header><b>Applications of Computer Graphics</b></header>
      <article>
        <p>The development of computer graphics has been driven both by the needs of the user
          community and by advances in hardware and software. The applications of computer
          graphics are many and varied; we can, however, divide them into <b>four</b> major areas:</p>
        <ol>
          <li>Display of information</li>
          <li>Design</li>
          <li>Simulation and animation</li>
          <li>User interfaces</li>
        </ol>
        <p>Although many applications span two or more of these areas, the development of the
          field was based on separate work in each.</p>
        <h2>1. Display of Information:</h2>
        <p>One of the most common uses of computer graphics is to display information in a
          pictorial or graphical form. This includes the generation of charts, graphs, and maps,
          as well as the visualization of scientific data. For example, medical imaging techniques
          such as MRI and CT scans use computer graphics to create detailed images of the human
          body.</p>
        <h2>2. Design:</h2>
        <p>Computer graphics is widely used in design and modeling applications, such as
          computer-aided design (CAD) for engineering and architectural design. It allows
          designers to create and manipulate 3D models of objects and structures, visualize
          designs from different angles, and simulate how they will look and function in the real
          world.</p>
        <h2>3. Simulation and Animation:</h2>
        <p>Computer graphics is also used to create realistic simulations and animations for
          various purposes, including entertainment, training, and scientific visualization. This
          includes the creation of 3D animations for movies and video games, as well as
          simulations for training pilots, surgeons, and other professionals.</p>
        <h2>4. User Interfaces:</h2>
        <p>Computer graphics plays a crucial role in the design of user interfaces for
          software applications. It allows developers to create visually appealing and intuitive
          interfaces that enhance the user experience. This includes the design of icons, buttons,
          menus, and other graphical elements that users interact with.</p>


      </article>
      <br />
    </section>
    <section class="main-section" id="The_Computer_Graphics_System">
      <br />
      <header><b>The Computer Graphics System</b></header>
      <article>
        <p>A computer graphics system is a computer system; as such, it must have all the
          components of a general-purpose computer system. There are six major elements in our system:</p>
        <ol>
          <li>Input devices</li>
          <li>Central Processing Unit</li>
          <li>Graphics Processing Unit</li>
          <li>Memory</li>
          <li>Frame buffer</li>
          <li>Output devices</li>
        </ol>

        <p>These components are shown in the figure below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20computer%20graphics%20system.png?raw=true"
          alt="The computer graphics system">

        <p>This model is general enough to include workstations and personal computers,
          interactive game systems, mobile phones, GPS systems, and sophisticated imagegeneration systems.
          Although most of the components are present in a standard computer, it is the way each element is specialized
          for
          computer graphics that characterizes this diagram as a portrait of a graphics system.</p>

        <h2>Input Devices</h2>

        <p>Input devices are used to capture data (and images) from the real world and convert them into a form that can
          be processed by the computer.</p>

        <p>Most graphics systems provide a keyboard and at least one other input device. The
          most common input devices are the mouse, the joystick, and the data tablet. Each
          provides positional information to the system, and each usually is equipped with one
          or more buttons to provide signals to the processor. Often called pointing devices,
          these devices allow a user to indicate a particular location on the display.</p>

        <p>Modern systems, such as game consoles, provide a much richer set of input
          devices, with new devices appearing almost weekly. In addition, there are devices
          which provide three- (and more) dimensional input. Consequently, we want to provide a
          flexible model for incorporating the input from such devices into our graphics
          programs</p>

        <p>We can think about input devices in two distinct ways. The obvious one is to look
          at them as <b>physical devices</b>, such as a keyboard or a mouse, and to discuss how they
          work. Certainly, we need to know something about the physical properties of our input devices,
          so such a discussion is necessary if we are to obtain a full understanding
          of input. However, from the perspective of an application programmer, we should not
          need to know the details of a particular physical device to write an application program.</p>

        <p>Rather, we prefer to treat input devices as <b>logical devices</b> whose properties are
          specified in terms of what they do from the perspective of the application program. A
          logical device is characterized by its high-level interface with the user program rather
          than by its physical characteristics. </p>

        <p>Logical devices are familiar to all writers of highlevel programs.
          For example, data input and output in Java are done through classes
          such as System.out for output, PrintWriter for writing to files, and Scanner for input, whose methods use the
          standard Java data types. When we output a string using System.out.println or PrintWriter.println, the
          physical device
          on which the output appears could be a printer, a terminal, or a disk file. This output could even be the
          input to another
          program. The details of the format required by the destination device are of minor
          concern to the writer of the application program.</p>

        <p>In computer graphics, the use of logical devices is slightly more complex because
          the forms that input can take are more varied than the strings of bits or characters
          to which we are usually restricted in nongraphical applications. For example, we can
          use the mouse—a physical device—either to select a location on the screen of our
          CRT or to indicate which item in a menu we wish to select. In the first case, an x, y
          pair (in some coordinate system) is returned to the user program; in the second, the
          application program may receive an integer as the identifier of an entry in the menu.
          The separation of physical from logical devices allows us to use the same physical
          devices in multiple markedly different logical ways. It also allows the same program
          to work, without modification, if the mouse is replaced by another physical device,
          such as a data tablet or trackball.</p>

        <h3>Physical Input Devices</h3>

        <p>From the physical perspective, each input device has properties that make it more
          suitable for certain tasks than for others. We take the view used in most of the workstation
          literature that there are <b>two</b> primary types of physical devices: <b>pointing devices</b>
          and <b>keyboard devices</b></p>

        <p>The pointing device allows the user to indicate a position on
          the screen and almost always incorporates one or more buttons to allow the user to
          send signals or interrupts to the computer.</p>

        <p>The keyboard device is almost always a physical keyboard but can be generalized to include any device that
          returns character
          codes. We use the American Standard Code for Information Interchange (ASCII) in
          our examples. ASCII assigns a single unsigned byte to each character. Nothing we do
          restricts us to this particular choice, other than that ASCII is the prevailing code used.
          Note, however, that other codes, especially those used for Internet applications, use
          multiple bytes for each character, thus allowing for a much richer set of supported
          characters.</p>

        <p>The mouse and trackball are similar in use and often
          in construction as well. A typical mechanical mouse when turned over looks like a
          trackball. In both devices, the motion of the ball is converted to signals sent back to
          the computer by pairs of encoders inside the device that are turned by the motion of
          the ball. The encoders measure motion in two orthogonal directions</p>

        <p>There are many variants of these devices. Some use optical detectors rather than
          mechanical detectors to measure motion. Small trackballs are popular with portable
          computers because they can be incorporated directly into the keyboard. There are
          also various pressure-sensitive devices used in keyboards that perform similar functions
          to the mouse and trackball but that do not move; their encoders measure the
          pressure exerted on a small knob that often is located between two keys in the middle
          of the keyboard</p>

        <p>We can view the output of the mouse or trackball as two independent values
          provided by the device. These values can be considered as positions and converted—
          either within the graphics system or by the user program—to a two-dimensional
          location in a convenient coordinate system. If it is configured in this manner, we can
          use the device to position a marker (cursor) automatically on the display; however,
          we rarely use these devices in this direct manner.</p>

        <p>It is not necessary that the output of the mouse or trackball encoders be interpreted as a position.
          Instead, either the device driver or a user program can interpret
          the information from the encoder as two independent velocities. The computer can
          then integrate these values to obtain a two-dimensional position.</p>

        <p>Thus, as a mouse moves across a surface, the integrals of the velocities yield x, y
          values that can be converted to indicate the position for a cursor on the screen, as shown below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/cursor%20positioning.png?raw=true"
          alt="cursor positioning">

        <p>By interpreting the distance traveled by the ball as a velocity, we can use the device
          as a variable-sensitivity input device. Small deviations from rest cause slow or small
          changes; large deviations cause rapid large changes.</p>

        <p>With either device, if the ball does not rotate, then there is no change in the integrals and a cursor
          tracking the position of the mouse will not move. </p>

        <p>In this mode, these devices are <b>relative-positioning</b> devices because changes in the position of
          the ball yield a position in the user program; the absolute location of the ball (or the mouse) is not used by
          the application
          program.</p>

        <p>Relative positioning, as provided by a mouse or trackball, is <b>not always desirable</b>.</p>

        <p>In particular, these devices are not suitable for an operation such as tracing a diagram.
          If, while the user is attempting to follow a curve on the screen with a mouse, she
          lifts and moves the mouse, the absolute position on the curve being traced is lost.
        </p>

        <p><b>Data tablets</b> provide <b>absolute positioning</b>. A typical data tablet has rows
          and columns of wires embedded under its surface. The position of the stylus is
          determined through electromagnetic interactions between signals traveling through
          the wires and sensors in the stylus. Touch-sensitive transparent screens that can be
          placed over the face of a CRT have many of the same properties as the data tablet.
          Small, rectangular, pressure-sensitive touchpads are embedded in the keyboards of
          many portable computers. These touchpads can be configured as either relative- or
          absolute-positioning devices.</p>

        <h3>Logical Devices</h3>

        <p>Two major characteristics describe the logical behavior
          of an input device: (1) the measurements that the device returns to the user program
          and (2) the time when the device returns those measurements.</p>

        <p>The logical <b>string</b> device in Java is similar to using character input through Scanner or
          BufferedReader.
          A physical keyboard will return a string of characters to an application program; the same string might be
          provided from a file, or the user may see a virtual keyboard displayed on the output and use a pointing device
          to generate the
          string of characters. Logically, all three methods are examples of a string device, and application code for
          using such input can be
          the same regardless of which physical device is used.</p>

        <p>The physical pointing device can be used in a variety of logical ways. As a <b>locator</b>
          it can provide a position to the application in either a device-independent coordinate
          system, such as world coordinates, as in OpenGL, or in screen coordinates, which the
          application can then transform to another coordinate system. A logical <b>pick</b> device
          returns the identifier of an object on the display to the application program. It is
          usually implemented with the same physical device as a locator but has a separate
          software interface to the user program.</p>

        <p>A <b>widget</b> is a graphical interactive device, provided by either the window system
          or a toolkit. Typical widgets include menus, scrollbars, and graphical buttons. Most
          widgets are implemented as special types of windows. Widgets can be used to provide
          additional types of logical devices. For example, a menu provides one of a number of
          <b>choices</b> as may a row of graphical buttons. A logical <b>valuator</b> provides analog input
          to the user program, usually through a widget such as a slidebar, although the same
          logical input could be provided by a user typing numbers into a physical keyboard.
        </p>

        <h2>The CPU and The GPU</h2>
        <p>In a simple system, there may be only one processor, the central processing unit
          (CPU) of the system, which must do both the normal processing and the graphical processing.
          The main graphical function of the processor is to take specifications of graphical primitives (such as lines,
          circles, and polygons)
          generated by application programs and to assign values to the pixels in the frame buffer that best represent
          these entities.</p>

        <p> For example, a triangle is specified by its three vertices, but to display
          its outline by the three line segments connecting the vertices, the graphics system
          must generate a set of pixels that appear as line segments to the viewer.
          The conversion of geometric entities to pixel colors and locations in the frame buffer is known
          as <b>rasterization</b>, or <b>scan conversion</b>.</p>

        <p> In early graphics systems, the frame buffer was
          part of the standard memory that could be directly addressed by the CPU. Today,
          virtually all graphics systems are characterized by special-purpose <b>graphics processing
            units (GPUs)</b>, custom-tailored to carry out specific graphics functions. The GPU can
          be either on the mother board of the system or on a graphics card. The frame buffer
          is accessed through the graphics processing unit and usually is on the same circuit
          board as the GPU.</p>

        <p>GPUs have evolved to where they are as complex or even more complex than
          CPUs. They are characterized by both special-purpose modules geared toward graphical operations
          and a high degree of parallelism—recent GPUs contain over 100 processing units, each of which is user
          programmable. GPUs are so powerful that they can often be used as mini supercomputers for general purpose
          computing.</p>


        <h2>Output Devices</h2>
        <p>Until recently, the dominant type of display (or monitor) was the <b>cathode-ray tube
            (CRT)</b>. A simplified picture of a CRT is shown below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20cathode%20ray%20tube.png?raw=true"
          alt="the cathode-ray tube">

        <p>When electrons strike the phosphor coating on the tube, light is emitted. The direction of the beam is
          controlled
          by two pairs of deflection plates. The output of the computer is converted, by digitalto-analog converters,
          to voltages across the x and y deflection plates. Light appears
          on the surface of the CRT when a sufficiently intense beam of electrons is directed at
          the phosphor.</p>

        <p>If the voltages steering the beam change at a constant rate, the beam will trace
          a straight line, visible to a viewer. Such a device is known as the <b>random-scan</b>,
          <b>calligraphic</b>, or <b>vector</b> CRT, because the beam can be moved directly from any
          position to any other position. If intensity of the beam is turned off, the beam can
          be moved to a new position without changing any visible display. This configuration
          was the basis of early graphics systems that predated the present raster technology
        </p>

        <p>A typical CRT will emit light for only a short time—usually, a few milliseconds—
          after the phosphor is excited by the electron beam. For a human to see a steady,
          flicker-free image on most CRT displays, the same path must be retraced, or <b>refreshed</b>,
          by the beam at a sufficiently high rate, the <b>refresh rate</b>. In older systems,
          the refresh rate is determined by the frequency of the power system, 60 cycles per
          second or 60 Hertz (Hz) in the United States and 50 Hz in much of the rest of the world.
          Modern displays are no longer coupled to these low frequencies and operate at rates
          up to about 85 Hz.</p>

        <p>In a raster system, the graphics system takes pixels from the frame buffer and
          displays them as points on the surface of the display in one of two fundamental
          ways.</p>

        <p>In a <b>noninterlaced</b> system, the pixels are displayed row by row, or scan line
          by scan line, at the refresh rate.</p>

        <p>In an <b>interlaced</b> display, odd rows and even rows are refreshed alternately.
          Interlaced displays are used in commercial television. In an interlaced display operating at 60 Hz,
          the screen is redrawn in its entirety only 30 times per second, although the visual system is tricked
          into thinking the refresh rate is 60 Hz rather than 30 Hz. Viewers located near the screen, however, can tell
          the difference between the interlaced and noninterlaced displays. Noninterlaced displays
          are becoming more widespread, even though these displays process pixels at twice the
          rate of the interlaced display</p>

        <p>Color CRTs have three different colored phosphors (red, green, and blue), arranged in small groups.
          One common style arranges the phosphors in triangular groups called <b>triads</b>, each triad consisting of
          three phosphors,
          one of each primary. Most color CRTs have three electron beams, corresponding to the three types of phosphors.
        </p>
        <p>In the shadow-mask CRT, a metal screen with small holes—the
          shadow mask—ensures that an electron beam excites only phosphors of the proper
          color:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shadowmask%20CRT.png?raw=true"
          alt="Shadowmask CRT">

        <p>Although CRTs are still common display devices, they are rapidly being replaced
          by flat-screen technologies. Flat-panel monitors are inherently raster based. Although
          there are multiple technologies available, including light-emitting diodes (LEDs),
          liquid-crystal displays (LCDs), and plasma panels, all use a two-dimensional grid
          to address individual light-emitting elements.</p>

        <p>The following shows a generic flat-panel monitor:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/flat%20panel%20display.png?raw=true"
          alt="Flat panel display">

        <p>The two outside plates each contain parallel grids of wires that are oriented
          perpendicular to each other. By sending electrical signals to the proper wire in each
          grid, the electrical field at a location, determined by the intersection of two wires, can
          be made strong enough to control the corresponding element in the middle plate.
          The middle plate in an LED panel contains light-emitting diodes that can be turned
          on and off by the electrical signals sent to the grid. In an LCD display, the electrical
          field controls the polarization of the liquid crystals in the middle panel, thus turning
          on and off the light passing through the panel. A plasma panel uses the voltages on the
          grids to energize gases embedded between the glass panels holding the grids. The
          energized gas becomes a glowing plasma.</p>

        <p>Most projection systems are also raster devices. These systems use a variety of
          technologies, including CRTs and digital light projection (DLP). From a user perspective,
          they act as standard monitors with similar resolutions and precisions. Hard-copy
          devices, such as printers and plotters, are also raster based but cannot be refreshed.</p>


        <h2></h2>

      </article>
      <br />
    </section>

    <section class="main-section" id="Types_of_2D_Graphics">
      <br />
      <header><b>Types of 2D Graphics</b></header>
      <article>

        <p>There are two kinds of 2D computer graphics: raster graphics and vector graphics.</p>

        <h2>1. Raster Graphics</h2>

        <p><b>Raster graphics</b>, also known as <b>bitmap graphics</b>, are images that are made up of a grid of
          <b>pixels</b>.
        </p>

        <p>The pixels are small enough that they are not easy to see individually. In fact, for many very
          high-resolution displays, they
          become essentially invisible. Each pixel in the grid has a specific color value, and together they form the
          complete image.</p>

        <p>Modern screens typically use <b>24-bit color</b>, where each color is defined by three 8-bit numbers
          representing the levels of red, green, and blue. These three primary colors combine to create any color
          displayed on the
          screen. Such systems are known as <b>true-color</b>, <b>RGB-color</b>, or <b>full-color systems</b> because
          each
          pixel's color is determined by the combination of red, green, and blue values.</p>

        <p> Other formats are possible, such as <b>grayscale</b>, where each pixel is some shade of gray and the pixel
          color is given by one number that specifies the level of gray on a black-to-white scale. Typically, 256 shades
          of gray are used.</p>

        <p>Early computer screens used <b>indexed color</b>, where only a small set of colors, usually 16 or
          256, could be displayed. For an indexed color display, there is a numbered list of possible colors,
          and the color of a pixel is specified by an integer giving the position of the color in the list.</p>

        <p>In any case, the color values for all the pixels on the screen are stored in a large block of
          memory known as a <b>frame buffer</b>. Changing the image on the screen requires changing color
          values that are stored in the frame buffer. The screen is redrawn many times per second, so
          that almost immediately after the color values are changed in the frame buffer, the colors of
          the pixels on the screen will be changed to match, and the displayed image will change.</p>

        <p>In a very simple system, the frame buffer holds only the colored pixels that are
          displayed on the screen. In most systems, the frame buffer holds far more information,
          such as depth information needed for creating images from three-dimensional
          data. In these systems, the frame buffer comprises multiple buffers, one or more of
          which are color buffers that hold the colored pixels that are displayed. For now, we
          can use the terms frame buffer and color buffer synonymously without confusion.</p>

        <p>A computer screen used in this way is the basic model of <b>raster graphics</b>. The term
          "raster" technically refers to the mechanism used on older vacuum tube computer monitors:
          An electron beam would move along the rows of pixels, making them glow. The beam was
          moved across the screen by powerful magnets that would deflect the path of the electrons. The
          stronger the beam, the brighter the glow of the pixel, so the brightness of the pixels could be
          controlled by modulating the intensity of the electron beam. The color values stored in the
          frame buffer were used to determine the intensity of the electron beam. (For a color screen,
          each pixel had a red dot, a green dot, and a blue dot, which were separately illuminated by the
          beam.)</p>

        <p>Virtually all modern graphics systems are raster based. The image we see on the output device is an array—the
          <b>raster</b>—of
          picture elements, or pixels, produced by the graphics system.
        </p>

        <p>Raster graphics are best suited for representing complex images with many colors and gradients, such as
          photographs and detailed
          illustrations.</p>

        <h2>2. Vector Graphics</h2>

        <p>Although images on the computer screen are represented using pixels, specifying individual
          pixel colors is not always the best way to create an image. Another way is to specify the basic
          geometric objects that it contains, shapes such as lines, circles, triangles, and rectangles. This
          is the idea that defines <b>vector graphics</b>: Represent an image as a list of the geometric shapes
          that it contains.</p>

        <p>To make things more interesting, the shapes can have attributes, such as
          the thickness of a line or the color that fills a rectangle. Of course, not every image can be
          composed from simple geometric shapes. This approach certainly wouldn't work for a picture
          of a beautiful sunset (or for most any other photographic image). However, it works well for
          many types of images, such as architectural blueprints and scientific illustrations.</p>

        <p>In fact, early in the history of computing, vector graphics was even used directly on computer
          screens. When the first graphical computer displays were developed, raster displays were too
          slow and expensive to be practical. Fortunately, it was possible to use vacuum tube technology
          in another way: The electron beam could be made to directly draw a line on the screen, simply
          by sweeping the beam along that line. A vector graphics display would store a display list
          of lines that should appear on the screen. Since a point on the screen would glow only very
          briefly after being illuminated by the electron beam, the graphics display would go through the
          display list over and over, continually redrawing all the lines on the list. To change the image,
          it would only be necessary to change the contents of the display list. Of course, if the display
          list became too long, the image would start to flicker because a line would have a chance to
          visibly fade before its next turn to be redrawn</p>

        <p>But here is the point: For an image that can be specified as a reasonably small number of
          geometric shapes, the amount of information needed to represent the image is much smaller
          using a vector representation than using a raster representation. Consider an image made up
          of one thousand line segments. For a vector representation of the image, We only need to store
          the coordinates of two thousand points, the endpoints of the lines. <b>This would take up only a
            few kilobytes of memory. To store the image in a frame buffer for a raster display would require
            much more memory.</b> Similarly, a vector display could draw the lines on the screen more quickly
          than a raster display could copy the same image from the frame buffer to the screen. (As soon
          as raster displays became fast and inexpensive, however, they quickly displaced vector displays
          because of their ability to display all types of images reasonably well.)
        </p>

        <p>Unlike raster graphics, vector graphics are resolution-independent, meaning that they can be scaled to any
          size without losing quality.
          This is because instead of pixels, vector graphics use points, lines, and curves to represent elements.
          This allows for scalable graphics that can be resized without loss of quality. </p>

        <p>Vector graphics are best suited for representing simple images with solid colors and sharp edges, such as
          logos and icons, and
          widely used in graphic design, architectural design, and illustration industries.</p>

        <p>In summary, raster graphics are made up of pixels and are best suited for complex images with many colors,
          while vector graphics
          are made up of lines and curves and are best suited for simple images with solid colors.</p>

        <h2>So What's The Difference?</h2>

        <p>The divide between raster graphics and vector graphics persists in several areas of computer
          graphics.</p>

        <p> For example, it can be seen in a division between two categories of programs that
          can be used to create images: <b>painting programs</b> and <b>drawing programs</b></p>

        <h2>Painting Programs</h2>

        <p>In a painting program, the image is represented as a grid of pixels, and the user creates an image by
          assigning colors to pixels. This might be done by using a "drawing tool" that acts like a painter's brush,
          or even by tools that draw geometric shapes such as lines or rectangles. But the point in a
          painting program is to color the individual pixels, and it is only the pixel colors that are saved.
          To make this clearer, suppose that We use a painting program to draw a house, then draw a
          tree in front of the house. If We then erase the tree, We'll only reveal a blank background, not
          a house. In fact, the image never really contained a "house" at all—only individually colored
          pixels that the viewer might perceive as making up a picture of a house</p>

        <h2>Drawing Programs</h2>

        <p>In a drawing program, the user creates an image by adding geometric shapes, and the image
          is represented as a list of those shapes. If We place a house shape (or collection of shapes making
          up a house) in the image, and We then place a tree shape on top of the house, the house is
          still there, since it is stored in the list of shapes that the image contains. If We delete the tree,
          the house will still be in the image, just as it was before We added the tree. Furthermore, We
          should be able to select one of the shapes in the image and move it or change its size, so drawing
          programs offer a rich set of editing operations that are not possible in painting programs. (The
          reverse, however, is also true.)</p>

        <p>A practical program for image creation and editing might <b>combine elements of painting and
            drawing</b>, although one or the other is usually dominant.</p>

        <p>For example, a drawing program might allow the user to include a raster-type image, treating it as one shape.
          A painting program
          might let the user create “layers,” which are separate images that can be layered one on top of
          another to create the final image. The layers can then be manipulated much like the shapes in
          a drawing program (so that We could keep both Wer house and Wer tree in separate layers,
          even if in the image of the house is in back of the tree).</p>

        <p>Two well-known graphics programs are <b>Adobe Photoshop</b> and <b>Adobe Illustrator</b>. Photoshop
          is in the category of painting programs, while Illustrator is more of a drawing program. In
          the world of free software, the GNU image-processing program, Gimp, is a good alternative to
          Photoshop, while Inkscape is a reasonably capable free drawing program</p>

        <h2>File Formats</h2>

        <p>The divide between raster and vector graphics also appears in the field of graphics file
          formats. There are many ways to represent an image as data stored in a file. If the original
          image is to be recovered from the bits stored in the file, the representation must follow some
          exact, known specification.</p>
        <p>Such a specification is called a <b>graphics file format</b>.</p>
        <p>Some popular graphics file formats include GIF, PNG, JPEG, WebP, and SVG. Most images used on the
          Web are GIF, PNG, or JPEG, but most browsers also have support for SVG images and for
          the newer WebP format</p>

        <img class="center" style="width: 65%; height: 65%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/raster%20vs%20vector.png?raw=true"
          alt="raster vs vector">

        <p><b>GIF, PNG, JPEG, and WebP are raster graphics formats; an image is specified
            by storing a color value for each pixel.</b></p>

        <p><b>JPEG (Joint Photographic Experts Group)</b> allows <b>up to 16 million colors</b> and is <b>best for
            images with many
            colors or color gradations</b>, especially photographs. JPEG is a <b>"lossy" format, meaning each time the
            image is
            saved and compressed, some image information is lost, degrading quality</b>. JPEG images allow for various
          levels of compression.</p>
        <p>Low compression means high image quality, but large file size.
          High compression means lower image quality, but smaller file size.</p>

        <p><b>GIF (Graphics Interchange Format)</b> is a <b>"lossless"</b> format, meaning <b>image quality is not
            degraded through
            compression</b>. However, GIFs are <b>limited to a 256-color palette</b>, making them suitable for
          <b>simpler graphics with
            fewer colors</b>. GIFs also <b>support transparent backgrounds and simple animations</b>.
        </p>

        <p><b>PNG (Portable Network Graphics)</b> combines features of <b>both JPEG and GIF</b>. PNG <b>supports
            millions of colors and
            transparent backgrounds</b>. It uses <b>lossless compression, ensuring no quality loss</b>. However, PNGs
          may not be
          supported by older web browsers.</p>

        <p>WebP is a modern format that supports both lossless and lossy compression, providing a balance between image
          quality and file size.</p>

        <p>The amount of data necessary to represent a raster image can be quite large. However, the data usually
          contains a lot of redundancy and can be compressed to reduce its size. GIF and PNG use lossless compression,
          meaning the original image can be perfectly recovered. JPEG uses lossy compression, which allows for greater
          reduction in file size but at the cost of some image quality. WebP supports both types of compression.</p>


        <p><b>SVG, on the other hand, is fundamentally a vector graphics format (although SVG images
            can include raster images).</b> SVG is actually an XML-based language for describing twodimensional vector
          graphics images.</p>
        <p>"SVG" stands for "Scalable Vector Graphics" and the term "scalable" indicates one of the advantages of vector
          graphics: There is no loss of quality when the size of the image is increased. A line between two points can
          be
          represented at any scale, and it is still the same perfect geometric line. If We try to greatly increase the
          size of
          a raster image, on the other hand, We will find that We don't have enough color values for
          all the pixels in the new image; each pixel from the original image will be expanded to cover a
          rectangle of pixels in the scaled image, and We will get multi-pixel blocks of uniform color. The
          scalable nature of SVG images make them a good choice for web browsers and for graphical
          elements on Wer computer's desktop. And indeed, some desktop environments are now using
          SVG images for their desktop icons.
        </p>

        <p>A digital image, no matter what its format, is specified using a coordinate system. A
          coordinate system sets up a correspondence between numbers and geometric points. In two
          dimensions, each point is assigned a pair of numbers, which are called the coordinates of the
          point. The two coordinates of a point are often called its x -coordinate and y-coordinate,
          although the names "x" and "y" are arbitrary.</p>

        <p>A raster image is a two-dimensional grid of pixels arranged into rows and columns. As
          such, it has a natural coordinate system in which each pixel corresponds to a pair of integers
          giving the number of the row and the number of the column that contain the pixel. (Even in
          this simple case, there is some disagreement as to whether the rows should be numbered from
          top-to-bottom or from bottom-to-top.)</p>

        <p>For a vector image, it is natural to use real-number coordinates. The coordinate system for
          an image is arbitrary to some degree; that is, the same image can be specified using different
          coordinate systems.</p>

      </article>
      <br />
    </section>

    <section class="main-section" id="Pixels_and_Coordinate_Systems">
      <br />
      <header><b>Pixels and Coordinate Systems</b></header>
      <article>

        <p>As previously mentioned, most images viewed online are raster-based. Raster images are created with
          pixel-based
          software or captured with a camera or scanner. They are more common in general such as jpg, gif, png, and are
          widely
          used on the web.</p>

        <p>To create these two-dimensional images, each point in the image is assigned a color.</p>

        <p> A point in 2D can be identified by a pair of numerical coordinates. Colors can also be specified
          numerically. </p>

        <p>However, the assignment of numbers to points or colors is somewhat arbitrary.
          So we need to spend some time studying coordinate systems, which associate numbers to
          points, and color models, which associate numbers to colors.</p>

        <p>A digital image is made up of rows and columns of pixels. A pixel in such an image can be
          specified by saying which column and which row contains it. In terms of coordinates, a pixel
          can be identified by a pair of integers giving the column number and the row number.</p>

        <p> For example, the pixel with coordinates (3,5) would lie in column number 3 and row number 5.</p>

        <p>Conventionally, columns are numbered from left to right, starting with zero. Most graphics
          systems (like HTML Canvas), number rows from top to bottom starting from zero. </p>

        <p>Some, including OpenGL, number the rows from bottom to top instead.</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pixel%20grids.png?raw=true"
          alt="Pixel grids">

        <p>Note in particular that the pixel that is identified by a pair of coordinates (x,y) depends on the
          choice of coordinate system. We always need to know what coordinate system is in use before
          We know what point We are talking about.</p>

        <p>Row and column numbers identify a pixel, not a point. A pixel contains many points;
          mathematically, it contains an infinite number of points. The goal of computer graphics is not
          really to color pixels—it is to create and manipulate images. In some ideal sense, an image
          should be defined by specifying a color for each point, not just for each pixel. Pixels are an
          approximation. If we imagine that there is a true, ideal image that we want to display, then
          any image that we display by coloring pixels is an approximation. This has many implications.</p>

        <p>Suppose, for example, that we want to draw a line segment. A mathematical line has no
          thickness and would be invisible. So we really want to draw a thick line segment, with some
          specified width.</p>
        <p>Let's say that the line should be one pixel wide.</p>
        <p>The problem is that, <b>unless
            the line is horizontal or vertical, we can't actually draw the line by coloring pixels</b>. A diagonal
          geometric line will cover some pixels only partially. It is not possible to make part of a pixel
          black and part of it white. When We try to draw a line with black and white pixels only,
          the result is a jagged staircase effect.</p>

        <p>This effect is an example of something called <b>"aliasing"</b>.</p>
        <p>Aliasing can also be seen in the outlines of characters drawn on the screen and in diagonal or
          curved boundaries between any two regions of different color. (The term aliasing likely comes
          from the fact that ideal images are naturally described in real-number coordinates. When We
          try to represent the image using pixels, many real-number coordinates will map to the same
          integer pixel coordinates; they can all be considered as different names or "aliases" for the same
          pixel.)
        </p>

        <h3>Anti-Aliasing</h3>

        <p>Anti-aliasing is a fundamental technique employed in graphics production that allows for smoother and more
          realistic images.
          This technology is used to reduce the jagged edges or "jaggies" that are commonly seen in computer-generated
          images,
          allowing them to appear as they would in real life.</p>
        <p>It was presented by the Architecture Machine Group team, which later became known as the Media Lab,
          a laboratory engaged in research and development in the field of technology, science, art, design, and
          medicine,
          in 1972 at the Massachusetts Institute of Technology.</p>
        <p>The idea is that when a pixel is only partially covered by a shape, the color of the pixel should be
          a mixture of the color of the shape and the color of the background. When drawing a black line
          on a white background, the color of a partially covered pixel would be gray, with the shade of
          gray depending on the fraction of the pixel that is covered by the line. (In practice, calculating
          this area exactly for each pixel would be too difficult, so some approximate method is used.)
        </p>
        <p>At its core, <b>anti-aliasing (also known as AA) is a method of manipulating pixels so that they appear
            smoother
            than they actually are</b>.
          To achieve this effect, the software or hardware being used will <b>sample adjacent pixels and create an
            average
            color value
            between them</b>. This helps the image appear more natural and realistic since it blends together sharp
          pixel
          lines into one
          continuous line instead of several distinct pixelated lines.</p>

        <p>So why does the "jagged" effect occur? Modern monitors and screens of mobile devices consist of quadrangular
          elements - pixels.
          This means that, in fact, only horizontal or vertical lines can be displayed in straight lines with clear
          boundaries.
          Angled curves are displayed as "steps". For example, the line in the picture below appears straight, but as
          We zoom in,
          it becomes clear that it is not.</p>

        <p>Here, for example, is a geometric line, shown on the left, along with two approximations of that
          line made by coloring pixels. The lines are greatly magnified so that We can see the individual
          pixels. The line on the right is drawn using anti-aliasing, while the one in the middle is not:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/antialiasing%201.png?raw=true"
          alt="Antialiasing 1">

        <p>Note that anti-aliasing does not give a perfect image, but it can reduce the "jaggies" that are
          caused by aliasing (at least when it is viewed on a normal scale).</p>

        <p>Anyone who has played older games is familiar with the distinctive pixelated and blocky aesthetic.
          "Jaggedness" occurs due to the lack of smooth transitions between colors, and anti-aliasing helps to mitigate
          this issue.</p>

        <p>Jagged edges, or aliasing, occur when real-world objects with smooth, continuous curves are rasterized using
          pixels.
          This problem arises from <b>undersampling</b>, which happens when the sampling frequency is lower than the
          <a
            href="https://www.gatan.com/nyquist-frequency#:~:text=The%20Nyquist%2DShannon%20sampling%20theorem,shown%20in%20the%20figures%20below.">Nyquist
            Sampling Frequency</a>,
          leading to a loss of information about the image.
        </p>

        <p>Anti-aliasing works by sampling multiple points within and around each pixel, then calculating an average
          color value.
          This process effectively blurs the edges of objects, creating the illusion of smoother lines and reducing
          visible pixelation.</p>

        <p>While anti-aliasing improves image quality, it also increases the load on the processor and graphics card,
          as they need to render additional shades and expend more power resources.</p>

        <p>One way to reduce jagged edges is to increase the resolution, as higher resolution images have smaller
          pixels,
          making the blocky appearance less noticeable. However, resolution alone is not always sufficient,
          and software developers use various anti-aliasing techniques to further improve image quality.</p>

        <h3>Methods of Anti-Aliasing (AA)</h3>

        <p>There are essentially four methods of Anti-Aliasing:</p>
        <ol>
          <li>High-Resolution Display</li>
          <li>Post-Filtering (Supersampling)</li>
          <li>Pre-Filtering (Area Sampling)</li>
          <li>Pixel Phasing</li>
        </ol>

        <h4>High-Resolution Display</h4>

        <p>Using a high-resolution display is one of the simplest methods of anti-aliasing.
          By increasing the resolution, more pixels can be used to represent the image, reducing the appearance of
          jagged edges.
          However, this method is limited by the physical resolution of the display and may not be practical for all
          applications.</p>

        <h4>Post-Filtering (Supersampling)</h4>

        <p>Post-filtering, also known as supersampling, involves treating the screen as if it has a finer grid,
          effectively reducing the pixel size.
          The average intensity of each pixel is calculated from the intensities of subpixels, and the image is
          displayed at the screen resolution.
          This method is called post-filtering because it is done after generating the rasterized image.</p>

        <h4>Pre-Filtering (Area Sampling)</h4>

        <p>Pre-filtering, or area sampling, calculates pixel intensities based on the areas of overlap between each
          pixel and the objects to be displayed.
          The final pixel color is an average of the colors of the overlapping areas. This method is called
          pre-filtering because it is done before generating the rasterized image.</p>

        <h4>Pixel Phasing</h4>

        <p>Pixel phasing involves shifting pixel positions to approximate the positions near object geometry.
          Some systems allow the size of individual pixels to be adjusted to distribute intensities, which helps in
          pixel phasing.</p>

        <h3>Types of Anti-Aliasing (AA)</h3>

        <p>Generally all anti-aliasing methods can be classified into two classifications:</p>
        <ol>
          <li>Spatial Anti-Aliasing</li>
          <li>Post Process Anti-Aliasing</li>
        </ol>

        <h4>1. Spatial Anti-Aliasing</h4>

        <p>Spatial anti-aliasing techniques work by sampling multiple points within each pixel and averaging the colors
          to reduce jagged edges.</p>

        <h4>Supersampling Anti-Aliasing (SSAA)</h4>

        <p><b>Supersampling Anti-Aliasing (SSAA)</b>, also called full-scene anti-aliasing (FSAA), works by <b>rendering
            the
            image at a higher resolution and then downsampling it to the display resolution</b>.
          This method reduces jagged edges by averaging colors near the edges.</p>
        <p> In this approach, a 512x512 image is first computed at higher resolution, such as 2048x2048, for example.
          It is then reduced through averaging or filtering to produce a 512x512 image.</p>
        <p>While effective, SSAA is <b>computationally
            intensive and can heavily load the GPU</b>.</p>

        <h4>Multi-Sample Anti-Aliasing (MSAA)</h4>

        <p><b>Multi-Sample Anti-Aliasing (MSAA)</b> improves performance compared to SSAA by <b>sampling multiple points
            within
            each pixel only at the edges of polygons</b>.</p>
        <p> Images are computed for 4 (or 8) subpixel sample points, followed by averaging. It is slow, since the frame
          rate is
          reduced by a factor of 4 (or 8). It works well for horizontal and vertical triangle edges.
          For other edge angles, the gaps between subpixels can cause narrow face breakups.</p>
        <p>This method <b>reduces the computational load while still providing good anti-aliasing quality</b>.</p>

        <h4>Coverage Sampling Anti-Aliasing (CSAA)</h4>

        <p><b>Coverage Sampling Anti-Aliasing (CSAA)</b> is an Nvidia-specific technique that improves upon MSAA by
          increasing
          the number of coverage samples
          without significantly increasing the number of color/depth samples. This method <b>provides better edge
            quality
            with less performance impact</b>.</p>

        <h4>2. Post-Processing Anti-Aliasing</h4>

        <p>Post-processing anti-aliasing techniques are applied after the image has been rendered to smooth out jagged
          edges.</p>

        <h4>Fast Approximate Anti-Aliasing (FXAA)</h4>

        <p>Fast Approximate Anti-Aliasing (FXAA) is a post-processing technique, created by Timothy Lottes at Nvidia,
          that smooths edges by <b>analyzing the final image and blending colors at the edges</b>.</p>
        <p>This is the <b>cheapest and simplest smoothing algorithm</b>.</p>
        <p>In layman's terms, FXAA is applied to Wer final rendered image and works based on pixel data, not geometry.
          GPU's are particularly fast at executing these shader algorithms in parallel, thus it's very quick to render.
        </p>
        <p>FXAA is less computationally intensive than SSAA and MSAA, making it <b>suitable for real-time applications
            like
            video games</b>.</p>

        <h4>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</h4>

        <p><b>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</b> is a logical development of the FXAA algorithm.
          This post effect is used in post-processing the final image that combines edge
          detection and blending to reduce aliasing.
          SMAA provides <b>high-quality anti-aliasing with a lower performance cost compared to SSAA and MSAA</b>.</p>

        <h4>Temporal Anti-Aliasing (TAA) </h4>

        <p>Temporal anti-aliasing techniques use information from previous frames to reduce aliasing in the current
          frame.</p>

        <p><b>Temporal Anti-Aliasing (TAA)</b> reduces aliasing by <b>using information from previous frames to smooth
            edges in
            the current frame.</b>
          TAA is effective at reducing flickering and shimmering in moving images, but it <b>can introduce ghosting
            artifacts</b> (visual distortions that appear in images due to a variety of factors, including movement,
          refraction, and sampling errors)
          if not implemented correctly.</p>






        <p>There are other issues involved in mapping real-number coordinates to pixels.</p>
        <p>For example,
          which point in a pixel should correspond to integer-valued coordinates such as (3,5)? The center
          of the pixel? One of the corners of the pixel? In general, we think of the numbers as referring
          to the top-left corner of the pixel.</p>
        <p>Another way of thinking about this is to say that integer coordinates refer to the lines between pixels,
          rather than to the pixels themselves. But that still doesn't determine exactly which pixels are affected when
          a geometric shape is drawn.</p>

        <p>For example, here are two lines drawn using HTML canvas graphics, shown greatly magnified. The
          lines were specified to be colored black with a one-pixel line width:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/antialiasing%202.png?raw=true"
          alt="Antialiasing 2">

        <p>The top line was drawn from the point (100,100) to the point (120,100).</p>

        <p>In canvas graphics, integer coordinates correspond to the lines between pixels, but when a one-pixel line is
          drawn,
          it extends one-half pixel on either side of the infinitely thin geometric line.</p>

        <p>So for the top line, the line as it is drawn lies half in one row of pixels and half in another row. The
          graphics
          system, which uses anti-aliasing, rendered the line by coloring both rows of pixels gray.</p>

        <p>The bottom line was drawn from the point (100.5,100.5) to (120.5,100.5). In this case, the line lies
          exactly along one line of pixels, which gets colored black. The gray pixels at the ends of the
          bottom line have to do with the fact that the line only extends halfway into the pixels at its
          endpoints. Other graphics systems might render the same lines differently</p>

        <p>All this is complicated further by the fact that pixels aren't what they used to be. Pixels
          today are smaller!</p>

        <h3>Understanding Resolution</h3>

        <p>The <b>resolution</b> of a display device can be measured in terms of the number
          of pixels per inch on the display, a quantity referred to as <b>PPI (pixels per inch)</b> or sometimes
          <b>DPI (dots per inch)</b>.
        </p>

        <h4>PPI vs DPI</h4>

        <p>While PPI (Pixels Per Inch) and DPI (Dots Per Inch) are often used interchangeably, they refer to different
          concepts and are used in different contexts.</p>

        <h4>Pixels Per Inch (PPI)</h4>

        <p>PPI is a measure of the <b>pixel density of a digital display</b>, such as a computer monitor, smartphone
          screen, or
          television. It indicates the number of pixels present in one inch of the display. <b>Higher PPI values mean
            more
            pixels are packed into each inch, resulting in sharper and more detailed images</b>.</p>

        <p>For example, a display with a resolution of 1920x1080 pixels and a diagonal size of 15.6 inches has a PPI of
          approximately 141. This means there are 141 pixels in each inch of the display.</p>

        <h4>Dots Per Inch (DPI)</h4>

        <p>DPI is a measure of the <b>resolution of a printed image</b>, indicating the number of individual dots of ink
          or
          toner that a printer can produce within one inch. <b>Higher DPI values result in finer detail and smoother
            gradients in printed images</b>.</p>

        <p>For example, a printer with a resolution of 300 DPI can produce 300 dots of ink per inch, resulting in
          high-quality prints suitable for photographs and detailed graphics.</p>

        <p>Both measures are important for ensuring high-quality visuals, but they apply to different
          mediums.</p>

        <p>Early screens tended to have resolutions of somewhere close to 72 PPI.
          At that resolution, and at a typical viewing distance, individual pixels are clearly visible. For a
          while, it seemed like most displays had about 100 pixels per inch, but high resolution displays
          today can have 200, 300 or even 400 pixels per inch. At the highest resolutions, individual
          pixels can no longer be distinguished.
        </p>

        <p>The fact that pixels come in such a range of sizes is a problem if we use coordinate systems
          based on pixels. An image created assuming that there are 100 pixels per inch will look tiny on a
          400 PPI display. A one-pixel-wide line looks good at 100 PPI, but at 400 PPI, a one-pixel-wide
          line is probably too thin</p>

        <p>In fact, in many graphics systems, "pixel" doesn't really refer to the size of a physical
          pixel. Instead, it is just <b>another unit of measure</b>, which is set by the system to be something
          appropriate. (On a desktop system, a pixel is usually about one one-hundredth of an inch. On
          a smart phone, which is usually viewed from a closer distance, the value might be closer to
          1/160 inch. Furthermore, the meaning of a pixel as a unit of measure can change when, for
          example, the user applies a magnification to a web page.)
        </p>

        <p>Pixels cause problems that have not been completely solved. Fortunately, they are less of a
          problem for vector graphics.</p>
        <p>For vector graphics, pixels only become an issue during rasterization, the step in which a vector image is
          converted into pixels for display. The vector image itself can be created using any convenient coordinate
          system. It represents an idealized, resolution-independent image.</p>
        <p>A rasterized image is an approximation of that ideal image, but how to do the approximation can be left to
          the display
          hardware.
        </p>

        <h2>Real-number Coordinate Systems</h2>

        <p>When doing 2D graphics, We are given a rectangle in which We want to draw some graphics
          primitives. Primitives are specified using some coordinate system on the rectangle. It should
          be possible to select a coordinate system that is appropriate for the application. For example, if
          the rectangle represents a floor plan for a 15 foot by 12 foot room, then We might want to use
          a coordinate system in which the unit of measure is one foot and the coordinates range from 0
          to 15 in the horizontal direction and 0 to 12 in the vertical direction. The unit of measure in
          this case is feet rather than pixels, and one foot can correspond to many pixels in the image.
          The coordinates for a pixel will, in general, be real numbers rather than integers. In fact, it's
          better to forget about pixels and just think about points in the image. A point will have a pair
          of coordinates given by real numbers.</p>

        <p>To specify the coordinate system on a rectangle, We just have to specify the horizontal
          coordinates for the left and right edges of the rectangle and the vertical coordinates for the top
          and bottom. Let's call these values left, right, top, and bottom. Often, they are thought of as
          xmin, xmax, ymin, and ymax, but there is no reason to assume that, for example, top is less
          than bottom. We might want a coordinate system in which the vertical coordinate increases
          from bottom to top instead of from top to bottom. In that case, top will correspond to the
          maximum y-value instead of the minimum value.</p>

        <p>To allow programmers to specify the coordinate system that they would like to use, it would
          be good to have a subroutine such as:</p>

        <p class="center"><b>setCoordinateSystem(left,right,bottom,top)</b></p>

        <p>The graphics system would then be responsible for automatically transforming the coordinates
          from the specified coordinate system into pixel coordinates. Such a subroutine might not be
          available, so it's useful to see how the transformation is done by hand. Let's consider the general
          case. Given coordinates for a point in one coordinate system, we want to find the coordinates
          for the same point in a second coordinate system. (Remember that a coordinate system is just
          a way of assigning numbers to points. It's the points that are real!)</p>

        <p>Suppose that the horizontal and vertical limits are oldLeft, oldRight, oldTop, and oldBottom for the first
          coordinate system,
          and are newLeft, newRight, newTop, and newBottom for the second. Suppose that a point
          has coordinates (oldX,oldY ) in the first coordinate system. We want to find the coordinates
          (newX,newY ) of the point in the second coordinate system</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinates.png?raw=true"
          alt="coordinates">

        <p>Formulas for newX and newY are then given by: </p>

        <p class="center"><b>newX = newLeft + ((oldX - oldLeft) / (oldRight - oldLeft)) * (newRight - newLeft)</b></p>
        <p class="center"><b>newY = newTop + ((oldY - oldTop) / (oldBottom - oldTop)) * (newBottom - newTop)</b></p>

        <p>The logic here is that oldX is located at a certain fraction of the distance from oldLeft to
          oldRight. That fraction is given by:</p>

        <p class="center"><b>(oldX - oldLeft) / (oldRight - oldLeft)</b></p>

        <p>The formula for newX just says that newX should lie at the same fraction of the distance from
          newLeft to newRight. We can also check the formulas by testing that they work when oldX is
          equal to oldLeft or to oldRight, and when oldY is equal to oldBottom or to oldTop.</p>

        <p>As an example, suppose that we want to transform some real-number coordinate system
          with limits left, right, top, and bottom into pixel coordinates that range from 0 at left to 800 at
          the right and from 0 at the top 600 at the bottom. In that case, newLeft and newTop are zero,
          and the formulas become simply:</p>

        <p class="center"><b>newX = ((oldX - left) / (right - left)) * 800</b></p>
        <p class="center"><b>newY = ((oldY - top) / (bottom - top)) * 600</b></p>

        <p>Of course, this gives newX and newY as real numbers, and they will have to be rounded
          or truncated to integer values if we need integer coordinates for pixels. The reverse
          transformation—going from pixel coordinates to real number coordinates—is also useful.</p>

        <p>For example, if the image is displayed on a computer screen, and We want to react to mouse clicks
          on the image, We will probably get the mouse coordinates in terms of integer pixel coordinates,
          but We will want to transform those pixel coordinates into Wer own chosen coordinate system.</p>

        <p>In practice, though, We won't usually have to do the transformations Werself, since most
          graphics APIs provide some higher level way to specify transforms.</p>

        <h2>Aspect Ratio</h2>

        <p>The <b>aspect ratio</b> of a rectangle is the ratio of its width to its height. For example an aspect
          ratio of 2:1 means that a rectangle is twice as wide as it is tall, and an aspect ratio of 4:3 means
          that the width is 4/3 times the height. Although aspect ratios are often written in the form
          width:height, I will use the term to refer to the fraction width/height. A square has aspect ratio
          equal to 1. A rectangle with aspect ratio 5/4 and height 600 has a width equal to 600*(5/4),
          or 750.</p>

        <p>A coordinate system also has an aspect ratio. If the horizontal and vertical limits for the
          coordinate system are left, right, bottom, and top, as above, then the aspect ratio is the absolute
          value of:</p>

        <p class="center"><b>(right - left) / (top - bottom)</b></p>

        <p>If the coordinate system is used on a rectangle with the same aspect ratio, then when viewed in
          that rectangle, one unit in the horizontal direction will have the same apparent length as a unit
          in the vertical direction. If the aspect ratios don't match, then there will be some distortion.</p>

        <p>For example, the shape defined by the equation x^2 + y^2 = 9 should be a circle, but that will
          only be true if the aspect ratio of the (x,y) coordinate system matches the aspect ratio of the
          drawing area.</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/aspect%20ratio%201.png?raw=true"
          alt="aspect ratio 1">

        <p>It is not always a bad thing to use different units of length in the vertical and horizontal
          directions. However, suppose that We want to use coordinates with limits left, right, bottom,
          and top, and that We do want to preserve the aspect ratio.</p>

        <p>In that case, depending on the shape of the display rectangle, We might have to adjust the values either of
          left and right or
          of bottom and top to make the aspect ratios match:</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/aspect%20ratio%202.png?raw=true"
          alt="aspect ratio 2">

      </article>
      <br />
    </section>

    <section class="main-section" id="Color_Models">
      <br />
      <header><b>Color Models</b></header>
      <article>

        <p>We are talking about the most basic foundations of computer graphics. One of those is
          coordinate systems. The other is color.</p>

        <p>Red, Yellow, and Blue — Primary colors. Or at least, that's what we have been told since kindergarten, isn't
          it?
          But there is more to it.</p>

        <p>The colors on a computer screen are produced as combinations of <b>red, green, and blue light</b>.</p>

        <p>Now the question is — if RYB is the primary color set then why do computers use RGB instead?</p>

        <p>Going deep into the line, we first need to understand the color theory.</p>
        <p>There are two different theories: </p>
        <ol>
          <li>Additive</li>
          <li>Subtractive</li>
        </ol>

        <h2>Additive</h2>

        <p>Different colors are produced by varying the intensity of each type of light. A color can be
          specified by three numbers giving the intensity of red, green, and blue in the color. Intensity
          can be specified as a number in the range zero, for minimum intensity, to one, for maximum
          intensity.</p>

        <p>The additive is the case of the projection of one or more colored lights (wavelengths). These are the colors
          when mixed produce more light.</p>

        <p>This method of specifying color is called the <b>RGB color model</b>, where RGB stands
          for Red/Green/Blue.</p>


        <p>The red, green, and blue values for a color are called the color components of
          that color in the RGB color model and when mixed produces lighter colors, resulting in white light at the end.
          That's how our computer, TV, and other light-emitting screen works.</p>

        <p>Each parameter (red, green, and blue) defines the intensity of the color with a value <b>between 0 and
            255</b>.</p>

        <p>This means that there are 256 x 256 x 256 = 16777216 possible colors!</p>

        <p>For example, rgb(255, 0, 0) is displayed as red, because red is set to its highest value (255), and the other
          two (green and blue) are set to 0.</p>

        <p>Another example, rgb(0, 255, 0) is displayed as green, because green is set to its highest value (255), and
          the other two (red and blue) are set to 0.</p>

        <p>To display black, set all color parameters to 0, like this: rgb(0, 0, 0).</p>

        <p>To display white, set all color parameters to 255, like this: rgb(255, 255, 255).</p>

        <h3>Shades of Gray</h3>

        <p>Shades of gray are often defined using equal values for all three parameters:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shades%20of%20gray.png?raw=true"
          alt="shades of gray">

        <p>Light is made up of waves with a variety of wavelengths. A pure color is one for which
          all the light has the same wavelength, but in general, a color can contain many wavelengths—
          mathematically, an infinite number. How then can we represent all colors by combining just
          red, green, and blue light? In fact, we can't quite do that.</p>

        <p>We might have heard that combinations of the three basic, or "primary" colors are sufficient
          to represent all colors, because the human eye has three kinds of color sensors that detect red,
          green, and blue light. However, that is only an approximation. The eye does contain three
          kinds of color sensors. The sensors are called "cone cells."</p>

        <p>However, cone cells do not respond exclusively to red, green, and blue light. Each kind of cone cell
          responds, to a varying degree,
          to wavelengths of light in a wide range. A given mix of wavelengths will stimulate each type
          of cell to a certain degree, and the intensity of stimulation determines the color that we see. A
          different mixture of wavelengths that stimulates each type of cone cell to the same extent will
          be perceived as the same color.</p>

        <p>So a perceived color can, in fact, be specified by three numbers
          giving the intensity of stimulation of the three types of cone cell. However, it is not possible
          to produce all possible patterns of stimulation by combining just three basic colors, no matter how those
          colors are chosen.
          This is just a fact about the way our eyes actually work; it might have been different. </p>

        <p>Three basic colors can produce a reasonably large fraction of the set of
          perceivable colors, but there are colors that We can see in the world that We will never see on
          Wer computer screen. (This whole discussion only applies to people who actually have three
          kinds of cone cell. Color blindness, where someone is missing one or more kinds of cone cell, is
          surprisingly common.)
        </p>

        <p>The range of colors that can be produced by a device such as a computer screen is called
          the <b>color gamut</b> of that device. Different computer screens can have different color gamuts,
          and the same RGB values can produce somewhat different colors on different screens. The color
          gamut of a color printer is noticeably different—and probably smaller—than the color gamut
          of a screen, which explains why a printed image probably doesn't look exactly the same as it
          did on the screen.
        </p>

        <h2>Subtractive</h2>

        <p>When we mix paints or inks, subtractive mixing results. Paints or inks are non-emissive objects here. They
          reflect when light falls on them.
          Molecules of paint absorb some of the wavelengths of light and reflect rest. That's how we see such objects.
        </p>

        <p>Printers, by the way, make colors differently from the way a screen does it.
          Whereas a screen combines light to make a color, a printer combines inks or dyes. Because of
          this difference, colors meant for printers are often expressed using a different set of basic colors.</p>

        <p>The primary colors of the subtractive mix are CMYK — Cyan, Magenta, Yellow, and K which stands for black ( To
          distinguish it from B for Blue.
          Just a convention.)</p>

        <p>When the CMY (not K) gets mixed, it produces brownish color — a bit muddy. To get the more blackish color,
          the additional K for black is used.
          CMYK — the model used by printers & publishing houses.</p>

        <img class="center" style="width: 75%; height: 75%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/additive%20and%20subtractive.png?raw=true"
          alt="additive and subtractive color">

        <p>In any case, the most common color model for computer graphics is RGB. RGB colors are
          most often represented using 8 bits per color component, a total of 24 bits to represent a color.
          This representation is sometimes called "24-bit color."" An 8-bit number can represent 28, or
          256, different values, which we can take to be the positive integers from 0 to 255. A color is
          then specified as a triple of integers (r,g,b) in that range.</p>

        <p>This representation works well because 256 shades of red, green, and blue are about as many
          as the eye can distinguish. In applications where images are processed by computing with color
          components, it is common to use additional bits per color component to avoid visual effects
          that might occur due to rounding errors in the computations. Such applications might use a
          16-bit integer or even a 32-bit floating point value for each color component. On the other
          hand, sometimes fewer bits are used.</p>

        <p> For example, one common color scheme uses 5 bits for
          the red and blue components and 6 bits for the green component, for a total of 16 bits for a
          color. (Green gets an extra bit because the eye is more sensitive to green light than to red or
          blue.) This “16-bit color” saves memory compared to 24-bit color and was more common when
          memory was more expensive.
        </p>

        <p>There are many other color models besides RGB. RGB is sometimes criticized as being
          unintuitive. For example, it's not obvious to most people that yellow is made of a combination
          of red and green.</p>

        <h2>Hues, Saturation, and Values (Lightness)</h2>

        <p> The closely related color models <b>HSV</b> and <b>HSL</b> describe the same set of
          colors as RGB, but attempt to do it in a more intuitive way. (HSV is sometimes called HSB,
          with the "B" standing for "brightness" HSV and HSB are exactly the same model.)
        </p>

        <p>The "H" in these models stands for <b>"hue", a basic spectral color</b>. As H increases, the color
          changes from red to yellow to green to cyan to blue to magenta, and then back to red. The
          value of H is often taken to range from 0 to 360, since the colors can be thought of as arranged
          around a circle with red at both 0 and 360 degrees.</p>

        <p>The "S" in HSV and HSL stands for <b>"saturation"</b> and is taken to <b>range from 0 to 1</b>. A
          saturation of 0 gives a shade of gray (the shade depending on the value of V or L). A saturation
          of 1 gives a "pure color" and decreasing the saturation is like adding more gray to the color.</p>

        <p>"V" stands for <b>"value"</b> and "L" stands for <b>"lightness"</b>. They determine how bright or dark the
          color is. The main difference is that in the HSV model, the pure spectral colors occur for V=1,
          while in HSL, they occur for L=0.5.
        </p>

        <img class="center" style="width: 50%; height: 50%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/HSV.png?raw=true"
          alt="HSV explained">

        <p>Let's look at some colors in the HSV color model. The illustration below shows colors with
          a full range of H-values, for S and V equal to 1 and to 0.5. Note that for S=V=1, We get
          bright, pure colors. S=0.5 gives paler, less saturated colors. V=0.5 gives darker colors.
        </p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/HSV%20color%20model.png?raw=true"
          alt="HSV color model">

        <p> In the simple scale diagrams below, the first model indicates amount of black, white, or grey pigment added
          to the hue.
          The second model illustrates the same scale but explains the phenomenon based on light [spectral] properties.
        </p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pigment%20and%20light%20scale.png?raw=true"
          alt="pigment and light scale">

        <p>Regardless of the two Additive and Subtractive color models, all color is a result of how our eyes physically
          process light waves.
          So let's start with the light Additive model to see how it filters into the Subtractive model and to see how
          hues,
          values and saturation interact to produce unique colors.</p>

        <h3>Hues</h3>

        <p>The three primary hues in light are red, green, and blue. Thus, that is why televisions, computer monitors,
          and other full-range,
          electronic color visual displays use a triad of red, green, and blue phosphors to produce all electronically
          communicated color.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%201.png?raw=true"
          alt="hues 1">

        <p>As we mentioned before, in light, all three of these wavelengths added together at full strength produces
          pure white light.
          The absence of all three of these colors produces complete darkness, or black.</p>

        <h3>Mixing Adjacent Primaries = Secondary Hues</h3>

        <h4>Making Cyan, Magenta, and Yellow</h4>

        <p>Although additive and subtractive color models are considered their own unique entities for screen vs. print
          purposes,
          the hues CMY do not exist in a vacuum.</p>

        <p>They are produced as secondary colors when RGB light hues are mixed, as follows:</p>

        <ul>
          <li>Green + Red light → Yellow</li>
          <li>Red + Blue light → Magenta</li>
          <li>Blue + Green light → Cyan</li>
        </ul>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%202.png?raw=true"
          alt="hues 2">

        <h3>Overview of Hues</h3>

        <p>The colors on the outermost perimeter of the color circle are the "hues", which are colors in their purest
          form. This process can continue
          filling in colors around the wheel. The next level colors, the tertiary colors, are those colors between the
          secondary and primary colors.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%203.png?raw=true"
          alt="hues 3">

        <h3>Saturation</h3>

        <p>Saturation is also referred to as "intensity" and "chroma". It refers to the dominance of hue in the color.
          On the outer edge of the hue wheel are the 'pure' hues.
          As We move into the center of the wheel, the hue we are using to describe the color dominates less and less.
          When We reach the center of the wheel, no hue dominates. These colors directly on the central axis are
          considered <b>desaturated</b>.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/saturation%201.png?raw=true"
          alt="saturation 1">

        <p>Naturally, the opposite of the image above is to saturate color.</p>
        <p>The first example below describes the general direction color must
          move on the color circle to become more saturated (towards the outside). The second example depicts how a
          single color looks completely
          saturated, having no other hues present in the color.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/saturation%2002.png?raw=true"
          alt="saturation 2">


        <h3>Value</h3>

        <p>Now let's add "value" to the HSV scale. Value is the dimension of lightness/darkness. In terms of a spectral
          definition of color,
          value describes the overall intensity or strength of the light. If hue can be thought of as a dimension going
          around a wheel,
          then value is a linear axis running through the middle of the wheel, as seen below:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%201.png?raw=true"
          alt="value 1">

        <p>To better visualize even more, look at the example below showing a full color range for a single hue:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%202.png?raw=true"
          alt="value 2">

        <p>Now, if We imagine that each hue was also represented as a slice like the one above, we would have a solid,
          upside-down cone of colors.
          The example above can be considered a slice of the cone. Notice how the right-most edge of this cone slice
          shows the greatest amount of the
          dominant red hue (least amount of other competing hues), and how as We go down vertically, it gets darker in
          "value".</p>
        <p>Also notice that as we travel from right to left in the cone, the hue becomes less dominant and eventually
          becomes completely desaturated
          along the vertical center of the cone. This vertical center axis of complete desaturation is referred to as
          <b>grayscale</b>.
        </p>
        <p>See how this slice below translates into some isolated color swatches:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%203.png?raw=true"
          alt="value 3">


        <p>
          Often, a fourth component is added to color models. The fourth component is called <b>alpha</b>,
          and color models that use it are referred to by names such as <b>RGBA</b> and <b>HSLA</b>. Alpha is not a
          color as such. It is usually used to represent transparency.</p>

        <p>A color with maximal alpha value is
          fully opaque; that is, it is not at all transparent. A color with alpha equal to zero is completely
          transparent and therefore invisible. Intermediate values give translucent, or partly transparent, colors.
        </p>

        <p>Transparency determines what happens when We draw with one color (the foreground
          color) on top of another color (the background color). If the foreground color is fully opaque,
          it simply replaces the background color. If the foreground color is partly transparent, then it
          is blended with the background color.</p>

        <p>Assuming that the alpha component ranges from 0 to 1,
          the color that We get can be computed as: </p>

        <p class="center"><b>new_color = (alpha)*(foreground_color) + (1 - alpha)*(background_color)</b></p>

        <p>This computation is done separately for the red, blue, and green color components. This is
          called <b>alpha blending</b>. The effect is like viewing the background through colored glass; the
          color of the glass adds a tint to the background color. This type of blending is not the only
          possible use of the alpha component, but it is the most common.</p>

        <p>An RGBA color model with 8 bits per component uses a total of 32 bits to represent a color.
          This is a convenient number because integer values are often represented using 32-bit values. A
          32-bit integer value can be interpreted as a 32-bit RGBA color.</p>

        <p>How the color components are arranged within a 32-bit integer is somewhat arbitrary.</p>

        <p>The most common layout is to store the alpha component in the eight high-order bits,
          followed by red, green, and blue. (This should probably be called ARGB color.) However, other layouts are also
          in use.</p>

      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="Shapes">
      <br />
      <header><b>Shapes</b></header>
      <article>

        <p>We have been talking about low-level graphics concepts like pixels and coordinates, but
          fortunately we don't usually have to work on the lowest levels. Most graphics systems let us
          work with higher-level shapes, such as triangles and circles, rather than individual pixels.</p>

        <p>In a graphics API, there will be certain basic shapes that can be drawn with one command,
          whereas more complex shapes will require multiple commands. Exactly what qualifies as a
          basic shape varies from one API to another.</p>

        <p>For example, the HTML5 canvas API provides commands to draw rectangles, circles, and
          lines, but not triangles. To draw a triangle, We have to draw three lines.</p>

        <p>By "line", we really mean line segment, that is a straight line segment connecting two given
          points in the plane. <b>A simple one-pixel-wide line segment, without anti-aliasing, is the most
            basic shape</b>. It can be drawn by coloring pixels that lie along the infinitely thin geometric line
          segment.</p>

        <p>An algorithm for drawing the line has to decide exactly which pixels to color. One of
          the first computer graphics algorithms, Bresenham's algorithm for line drawing, implements
          a very efficient procedure for doing so.</p>

        <p>In any case, lines are typically more complicated. Anti-aliasing is one
          complication. Line width is another. A wide line might actually be drawn as a rectangle.</p>

        <p>Lines can have other attributes, or properties, that affect their appearance. One question
          is, what should happen at the end of a wide line?</p>

        <p>Appearance might be improved by adding
          a rounded "cap" on the ends of the line. A square cap—that is, extending the line by half of
          the line width—might also make sense.</p>

        <p>Another question is, when two lines meet as part of a
          larger shape, how should the lines be joined? And many graphics systems support lines that
          are patterns of dashes and dots.</p>

        <p>This illustration shows some of the possibilities:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20lines.png?raw=true"
          alt="Types of Lines">

        <p>On the left are three wide lines with no cap, a round cap, and a square cap. The geometric line
          segment is shown as a dotted line. (The no-cap style is called “butt.”) To the right are four
          lines with different patterns of dots and dashes. In the middle are three different styles of line
          joins: mitered, rounded, and beveled.</p>

        <p>The basic rectangular shape has sides that are vertical and horizontal. (A tilted rectangle
          generally has to be made by applying a rotation.) Such a rectangle can be specified with two
          points, (x1,y1) and (x2,y2), that give the endpoints of one of the diagonals of the rectangle.
          Alternatively, the width and the height can be given, along with a single base point, (x,y). In
          that case, the width and height have to be positive, or the rectangle is empty. The base point
          (x,y) will be the upper left corner of the rectangle if y increases from top to bottom, and it will
          be the lower left corner of the rectangle if y increases from bottom to top.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20rectangles.png?raw=true"
          alt="Types of Rectangles">

        <p>Suppose that We are given points (x1,y1) and (x2,y2), and that We want to draw the rectangle
          that they determine. And suppose that the only rectangle-drawing command that We have
          available is one that requires a point (x,y), a width, and a height. For that command, x must
          be the smaller of x1 and x2, and the width can be computed as the absolute value of x1 minus
          x2. And similarly for y and the height.</p>

        <p>In pseudocode,</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rectangle%20pseudocode.png?raw=true"
          alt="Rectangle Pseudocode">

        <p>A common variation on rectangles is to allow rounded corners. For a “round rect,” the
          corners are replaced by elliptical arcs. The degree of rounding can be specified by giving the
          horizontal radius and vertical radius of the ellipse.</p>

        <p> Here are some examples of round rects. For
          the shape at the right, the two radii of the ellipse are shown:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rounded%20rectangles.png?raw=true"
          alt="Rounded Rectangles">

        <p>Our final basic shape is the oval. (An oval is also called an ellipse.) An oval is a closed curve
          that has two radii. For a basic oval, we assume that the radii are vertical and horizontal. An
          oval with this property can be specified by giving the rectangle that just contains it. Or it can
          be specified by giving its center point and the lengths of its vertical radius and its horizontal
          radius. </p>

        <p>In this illustration, the oval on the left is shown with its containing rectangle and with
          its center point and radii:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20ovals.png?raw=true"
          alt="Types of Ovals">

        <p>The oval on the right is a circle. A circle is just an oval in which the two radii have the same
          length.</p>

        <p>If ovals are not available as basic shapes, they can be approximated by drawing a large
          number of line segments. The number of lines that is needed for a good approximation depends
          on the size of the oval. It's useful to know how to do this. Suppose that an oval has center
          point (x,y), horizontal radius r1, and vertical radius r2. Mathematically, the points on the oval
          are given by:</p>

        <text class="center"><b>( x + r1*cos(angle), y + r2*sin(angle) )</b></text>

        <p>where angle takes on values from 0 to 360 if angles are measured in degrees or from 0 to 2π if
          they are measured in radians. Here sin and cos are the standard sine and cosine functions. To
          get an approximation for an oval, we can use this formula to generate some number of points
          and then connect those points with line segments.</p>

        <p>In pseudocode, assuming that angles are
          measured in radians and that pi represents the mathematical constant π,</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/oval%20pseudocode.png?raw=true"
          alt="Oval Pseudocode">

        <p>For a circle, of course, We would just have r1 = r2. This is the first time we have used the
          sine and cosine functions, but it won't be the last. These functions play an important role in
          computer graphics because of their association with circles, circular motion, and rotation. We
          will meet them again when we talk about transforms later.</p>

        <h2>Stroke and Fill</h2>

        <p>There are two ways to make a shape visible in a drawing.</p>

        <p>We can <b>stroke</b> it. Or, if it is a closed
          shape such as a rectangle or an oval, We can <b>fill</b> it.</p>

        <p>Stroking a line is like dragging a pen along
          the line. Stroking a rectangle or oval is like dragging a pen along its boundary.</p>

        <p>Filling a shape
          means coloring all the points that are contained inside that shape.</p>

        <p>It's possible to both stroke
          and fill the same shape; in that case, the interior of the shape and the outline of the shape can
          have a different appearance.
        </p>

        <p>When a shape intersects itself, like the two shapes in the illustration below, it's not entirely
          clear what should count as the interior of the shape. In fact, there are at least two different
          rules for filling such a shape.</p>

        <p> In fact, there are at least two different
          rules for filling such a shape. Both are based on something called the <b>winding number</b>. The
          winding number of a shape about a point is, roughly, <b>how many times the shape winds around
            the point in the positive direction</b>, which we'll take here to be counterclockwise.</p>

        <p>Winding number
          can be negative when the winding is in the opposite direction.</p>

        <p>In the illustration, the shapes on
          the left are traced in the direction shown, and the winding number about each region is shown
          as a number inside the region.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/understanding%20winding%20number.png?raw=true"
          alt="Understanding Winding Number">

        <p>The shapes are also shown filled using the two fill rules.</p>

        <p>For the shapes in the center, the fill
          rule is to color any region that has a non-zero winding number.</p>

        <p>For the shapes shown on the
          right, the rule is to color any region whose winding number is odd; regions with even winding
          number are not filled.
        </p>

        <p>There is still the question of what a shape should be filled with. Of course, it can be filled
          with a color, but other types of fill are possible, including <b>patterns</b> and <b>gradients</b>.</p>

        <p> A pattern
          is an image, usually a small image. When used to fill a shape, a pattern can be repeated
          horizontally and vertically as necessary to cover the entire shape.</p>

        <p>A gradient is similar in that
          it is a way for color to vary from point to point, but instead of taking the colors from an image,
          they are computed. There are a lot of variations to the basic idea, but there is always a line
          segment along which the color varies. The color is specified at the endpoints of the line segment,
          and possibly at additional points; between those points, the color is interpolated. The color
          can also be extrapolated to other points on the line that contains the line segment but lying
          outside the line segment; this can be done either by repeating the pattern from the line segment
          or by simply extending the color from the nearest endpoint.</p>

        <p>For a <b>linear gradient</b>, the color
          is constant along lines perpendicular to the basic line segment, so we get lines of solid color
          going in that direction.</p>

        <p>In a radial gradient, the color is constant along circles centered at
          one of the endpoints of the line segment.</p>

        <p>And that doesn't exhaust the possibilities. To give
          we an idea what patterns and gradients can look like, here is a shape, filled with two gradients
          and two patterns:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/patterns%20and%20gradients.png?raw=true"
          alt="Patterns and Gradients">

        <p>The first shape is filled with a simple linear gradient defined by just two colors, while the second
          shape uses a radial gradient.</p>

        <p>Patterns and gradients are not necessarily restricted to filling shapes. Stroking a shape is,
          after all, the same as filling a band of pixels along the boundary of the shape, and that can be
          done with a gradient or a pattern, instead of with a solid color.</p>

        <p>Finally, a string of text can be considered to be a shape for the purpose
          of drawing it. The boundary of the shape is the outline of the characters. The text is drawn
          by filling that shape. </p>

        <p>In some graphics systems, it is also possible to stroke the outline of the
          shape that defines the text.</p>

        <p>In the following illustration, the string "Graphics" is shown, on
          top, filled with a pattern and, below that, filled with a gradient and stroked with solid black:</p>


        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/stroke%20and%20fill%20text.png?raw=true"
          alt="Stroke and Fill Text">







      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="JavaScript">
      <br />
      <header><b>JavaScript</b></header>
      <article>
        <p>JavaScript is a dynamic programming language that's used for web development, in web applications, for game
          development, and lots more. It allows we to implement dynamic features on web pages that cannot be done with
          only HTML and CSS.</p>

        <p>Many browsers use JavaScript as a scripting language for doing dynamic things on the web. Any time we see a
          click-to-show dropdown menu, extra content added to a page, and dynamically changing element colors on a page,
          to name a few features, we're seeing the effects of JavaScript.</p>

        <h2>How JavaScript Makes Things Dynamic</h2>
        <p>HTML defines the structure of our web document and the content therein. CSS declares various styles for the
          contents provided on the web document.</p>

        <p>HTML and CSS are often called markup languages rather than programming languages, because they, at their
          core, provide markups for documents with very little dynamism.</p>

        <p>JavaScript, on the other hand, is a dynamic programming language that supports Math calculations, allows we
          to dynamically add HTML contents to the DOM, creates dynamic style declarations, fetches contents from another
          website, and lots more.</p>

        <p>Here's a basic breakdown of JavaScript fundamentals:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/javascript%20cheat%20sheet.png?raw=true"
          alt="JavaScript Cheat Sheet" />


      </article>
      <br />
    </section>

    <hr />




    <section class="main-section" id="Intro_to_HTML5_Canvas">
      <br />
      <header><b>HTML5 Canvas</b></header>
      <article>
        <p>HTML5 (Hypertext Markup Language 5) is a markup language used for structuring and presenting hypertext
          documents on
          the World Wide Web. It was the <b>fifth and final major HTML version</b> that is now a retired World Wide Web
          Consortium (W3C)
          recommendation.
          The current specification is known as the <b>HTML Living Standard</b>.</p>

        <p>Canvas is a new element in HTML5, which provides APIs used to draw raster graphics on a web application.
          The presence of the Canvas API for HTML5, strengthens the HTML5 platform by providing two drawing contexts: 2D
          and 3D.
          These capabilities are supported on most modern operating systems and browsers.</p>

        <p>However, we will begin with 2D graphics.</p>

        <p>The <b>HTML5 canvas element</b> is used to draw graphics, on the fly, via JavaScript. The canvas element is
          only a container
          for graphics. We must use a <b>script</b> to actually draw the graphics.</p>

        <p>Canvas has several methods for drawing paths, boxes, circles, text, and adding images.</p>

        <p>HTML Canvas can:</p>

        <ul>
          <li><b>draw colorful text</b>, with or without animation</li>
          <li><b>draw graphics</b> using great features for graphical data presentation with an imagery of graphs and
            charts
          </li>
          <li><b>be animated</b> - everything is possible: from simple bouncing balls to complex animations</li>
          <li><b>be interactive</b> - canvas can respond to JavaScript events to any user action (key clicks, mouse
            clicks,
            button clicks, finger movement)</li>
          <li><b>be used in games</b> - canvas' methods for animations, offer a lot of possibilities for HTML gaming
            applications</li>
        </ul>

        <p>Here is an example of a canvas element:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;&lt;/canvas&gt;</code></pre>

        <ul>
          <li>The <b>id attribute is required</b> (so it can be referred to by JavaScript)</li>
          <li>The width and height attribute defines the size of the canvas (the default size of the canvas is 300px
            (width) x 150px (height))</li>
          <li>The canvas element requires the closing tag</li>
          <p>Unlike the &lt;img&gt; element, The &lt;canvas&gt; element requires the closing tag &lt;/canvas&gt;. Any
            content between the
            opening and closing tags is fallback content that will display only if the browser doesn't support the
            &lt;canvas&gt; element.</p>
          <p>For example:</p>
          <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;The browser doesn't support the canvas element&lt;/canvas&gt;</code></pre>
          <p>However, nowadays, most modern web browsers support the &lt;canvas&gt; element.</p>
          <li>We can have multiple &lt;canvas&gt; elements on one HTML page.</li>
          <li>By default, the &lt;canvas&gt; element has no border and no content.</li>
        </ul>

        <p>Dimensions of canvas element can either be set statically in HTML, or dynamically using JavaScript, or a
          combination of both.</p>

        <p>To add a border, use a style attribute:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <p>Canvas consists of a drawable region defined in HTML code with height and width attributes.
          JavaScript code may access the area through a full set of drawing functions, allowing for dynamically
          generated graphics.</p>

        <p>The drawing on the canvas is done with JavaScript.</p>

        <p>The canvas is initially blank. To display something, a script is needed to access the rendering context and
          draw on it.</p>

        <p>The following example draws a red rectangle on the canvas, from position (0,0) with a width of 150 and a
          height of 75:</p>

        <h3>Step 1: Find the Canvas Element</h3>

        <p>Initially, the canvas is blank. To draw something, We need to access the rendering context and use it to
          draw on the canvas.</p>

        <p>First, we need to find the &lt;canvas&gt; element.</p>

        <p>We access a &lt;canvas&gt; element with the HTML DOM method getElementById():</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");</code></pre>

        <p>The getElementById() method of the Document interface returns an Element object representing the element
          whose id property matches the specified string.</p>

        <p>To set the dimensions dynamically with JavaScript, we can access the width and height as follows: </p>

        <pre><code class="javascript">//To set width and height of current viewport
canvas.width = window.innerWidth; //or to set a specific width i.e 200
canvas.height = window.innerHeight; //or to set a specific height i.e 300</code></pre>

        <h3>Step 2: Create a Drawing Object</h3>

        <p>Secondly, we need a drawing object for the canvas.</p>

        <p>The getContext() method returns an object with tools (properties and methods) for drawing:</p>

        <pre><code class="javascript">const ctx = canvas.getContext("2d");</code></pre>

        <h3>Step 3: Draw on the Canvas</h3>

        <p>Finally, we can draw on the canvas.</p>

        <p>Set the fill-color to red with the fillStyle property:</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)";</code></pre>

        <p>The fillStyle property can be a color, a gradient, or a pattern. The default fillStyle is black.</p>

        <p>The fillRect(x, y, width, height) method draws the rectangle, filled with the fill style color, on the
          canvas:</p>

        <pre><code class="javascript">ctx.fillRect(0, 0, 150, 75);</code></pre>

        <h2>Canvas Fill and Stroke</h2>

        <p>To define fill-color and outline-color for shapes/objects in canvas, we use the following properties:</p>
        <ul>
          <li>fillStyle - Defines the color, gradient, or pattern used to fill shapes</li>
          <li>strokeStyle - Defines the color, gradient, or pattern used for strokes</li>
        </ul>

        <h3>The fillStyle Property</h3>

        <p>The fillStyle property defines the fill-color of the object.</p>
        <p>The fillStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
          
//How can we set the fill-color to blue? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>The strokeStyle Property</h3>

        <p>The strokeStyle property defines the color of the outline.</p>

        <p>The strokeStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
                      
//How can we set the stroke-color to yellow? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>Combining fillStyle and strokeStyle</h3>

        <p>It is perfectly legal to combine the previous two rectangles: </p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// the filled rectangle
ctx.fillStyle = "rgb(0 0 255)";
ctx.fillRect(10,10, 100,100);

// the outline rectangle
ctx.strokeStyle = "rgb(255 255 0)";
ctx.strokeRect(10,10, 100,100);</code></pre>

        <h3>Gradients</h3>

        <p>Gradients let us display smooth transitions between two or more specified colors.</p>

        <p>Gradients can be used to fill rectangles, circles, lines, text, etc.</p>

        <p>There are two methods used for creating gradients:</p>

        <ul>
          <li>createLinearGradient() - creates a linear gradient</li>
          <li>createRadialGradient() - creates a radial/circular gradient</li>
        </ul>

        <h3>Linear Gradient</h3>

        <p>The createLinearGradient() method is used to define a linear gradient.</p>

        <p>A linear gradient changes color along a linear pattern (horizontally/vertically/diagonally).</p>

        <p>The createLinearGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting point</li>
          <li>y0 - The y-coordinate of the starting point</li>
          <li>x1 - The x-coordinate of the ending point</li>
          <li>y1 - The y-coordinate of the ending point</li>
        </ul>

        <p>The gradient object requires two or more color stops.</p>

        <p>The addColorStop() method specifies the color stops, and its position along the gradient. The positions can
          be anywhere between 0 and 1.</p>

        <p>To use the gradient, assign it to the fillStyle or strokeStyle property, then draw the shape (rectangle,
          circle, shape, or text).</p>

        <p>Here is an example of a linear gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create gradient
const grd = ctx.createLinearGradient(0, 0, 200, 0);
grd.addColorStop(0, "red");
grd.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd;
ctx.fillRect(10, 10, 150, 80);</code></pre>

        <h3>Radial Gradient</h3>

        <p>The createRadialGradient() method is used to create a radial/circular gradient.</p>

        <p>A radial gradient is defined by two circles, one smaller and one larger.</p>

        <p>The createRadialGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting circle</li>
          <li>y0 - The y-coordinate of the starting circle</li>
          <li>r0 - The radius of the starting circle</li>
          <li>x1 - The x-coordinate of the ending circle</li>
          <li>y1 - The y-coordinate of the ending circle</li>
          <li>r1 - The radius of the ending circle</li>
        </ul>

        <p>Here is an example of a radial gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");

const ctx = canvas.getContext("2d");

// Create gradient
const grd2 = ctx.createRadialGradient(85, 140, 0, 85, 140, 100);

// Add color stops
grd2.addColorStop(0, "red");
grd2.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd2;
ctx.fillRect(10, 100, 150, 80);</code></pre>

        <h3>Patterns</h3>

        <p>Patterns are used to fill shapes with images (instead of colors).</p>

        <p>There are two methods used for creating patterns:</p>

        <ul>
          <li>createPattern(image, type) - creates a pattern from an image</li>
          <li>createPattern(canvas, type) - creates a pattern from another canvas</li>
        </ul>

        <p>The createPattern() method has the following parameters:</p>

        <ul>
          <li>image - Specifies the image to use</li>
          <li>type - Repeat the pattern (repeat, repeat-x, repeat-y, no-repeat)</li>
        </ul>

        <p>Here is an example of a pattern:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create a pattern
const img = new Image();
img.src = "https://www.w3schools.com/tags/img_the_scream.jpg";
img.onload = function() {
  const pat = ctx.createPattern(img, "repeat");
  ctx.fillStyle = pat;
  ctx.fillRect(10, 10, 150, 80);
};</code></pre>

        <h3>The clearRect() Method</h3>

        <p>The clearRect() method is used to clear a rectangular area of the canvas. The cleared rectangle is
          transparent.</p>

        <p>The clearRect() method has the following parameters:</p>

        <ul>
          <li>x - The x-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>y - The y-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>width - The width of the rectangle to clear (in pixels)</li>
          <li>height - The height of the rectangle to clear (in pixels)</li>
        </ul>


        <p>Here we use fillRect() to draw a filled 150*100 pixels rectangle, starting in position (10,10). Then use
          clearRect() to clear a rectangular area in the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "pink";
ctx.fillRect(10,10,150,100);
            
ctx.clearRect(60,35,50,50);</code></pre>

        <p>And here is an example of clearing the entire canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas"); 
const ctx = canvas.getContext("2d");

// Clear the canvas
ctx.clearRect(0, 0, canvas.width, canvas.height);</code></pre>



        <p></p>

        <h2>Canvas Coordinates</h2>

        <p>As previously mentioned, The HTML canvas is a two-dimensional grid.</p>


        <p>It is important to understand the coordinate space of canvas, if We want elements to be positioned as
          desired. Top left of canvas represents (0,0) or origin coordinate.</p>

        <p>All the elements on canvas are placed with reference to this origin.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinate%20grid%20space.png?raw=true"
          alt="The Grid or Coordinate Space 1">

        <p>1 point on grid is roughly equivalent to 1px.</p>

        <p>At the example above further elaborates: we have red border around our canvas and we have drawn a rectangle
          of 100px width and height with a stroke of blue.</p>

        <p>This can be achieved by the following code: </p>

        <h3>HTML</h3>

        <pre><code class="html">&lt;canvas id="myCanvas" width="320" height="320" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <h3>JavaScript</h3>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(0, 0, 100, 100); //draws a rectangle with a blue stroke, starting at (0,0) with a width and height of 100 pixels</code></pre>

        <p>Providing x and y coordinates would translate the element relative to the canvas' origin coordinates.</p>
        <p>As shown in the image below, our rectangle has moved 20 pixels to the right and bottom as we have provided
          the values of x and y as 20.</p>


        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinate%20grid%20space%202.png?raw=true"
          alt="The Grid or Coordinate Space 2">

        <h3>Let's Look and Real Time Coordinates</h3>

        <p class="codepen" data-height="600" data-theme-id="dark" data-default-tab="result" data-slug-hash="emOoLbp"
          data-pen-title="Understanding Coordinates" data-user="amaraauguste"
          style="height: 300px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;">
          <span>See the Pen <a href="https://codepen.io/amaraauguste/pen/emOoLbp">
              Understanding Coordinates</a> by Amara (<a href="https://codepen.io/amaraauguste">@amaraauguste</a>)
            on <a href="https://codepen.io">CodePen</a>.</span>
        </p>
        <script async src="https://public.codepenassets.com/embed/index.js"></script>

        <h2>Shapes</h2>

        <p>It is easy to draw basic shapes like rectangle, triangle, square, circle, polygon or a just a simple line
          between two points. But by default Canvas provides a method only to draw rectangle.</p>

        <p>However, rest of shapes can be created by joining points using path API, and a combination of line and arc
          APIs.</p>

        <h3>Rectangle</h3>

        <p>The three most used methods for drawing rectangles in canvas are:</p>

        <ul>
          <li>The rect(x, y, width, height) method</li>
          <li>The fillRect(x, y, width, height) method</li>
          <li>The strokeRect(x, y, width, height) method</li>
        </ul>

        <p>The rect() method defines a rectangle. Note: the rect() method does not draw the rectangle (it just defines
          it). So, in addition, We have to use the stroke() method (or the fill() method) to actually draw it.</p>


        <p>fill and stroke are ink methods and each case it means to draw a rectangle with a filled color, or to draw a
          rectangular outline of a color. The default color is black.</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)"; //red color for fill
ctx.fillRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a red rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <p>Similarly, we can draw a rectangle with a stroke:</p>

        <pre><code class="javascript">ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a blue rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <h3>Circle</h3>

        <p>As we mentioned earlier there is no straight forward method to create a circle, but we can use a combination
          of path APIs and arc method to draw our circle. Let's understand a little more about path:</p>

        <p>"A path is list of points connected to form different shapes."</p>

        <p>This means a path can be formed between two given points on screen. It can be a straight line or curved arc
          or can be any shape or color.</p>

        <p>There are three steps to follow to create a shape using path:</p>

        <ol>
          <li>Invoke beingPath() method on context a create a new path. Once a path is created all future commands to
            draw are applied on
            this path.</li>
          <li>Next create a path using drawing methods, like lineTo, moveTo, arc, rect, etc.
            <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D#paths">Refer MDN</a> for
            list of all
            available methods that used with path.
          </li>
          <li>Once path has been created it needs to actually be rendered on canvas; we can do that using ink methods
            fill and stroke.</li>
        </ol>

        <p>Let's draw a circle:</p>

        <p>We have to use arc(x, y, radius, startAngle, endAngle) method on context to draw our circle.</p>

        <p> If we try to recollect basic geometry, to draw a circle using a protractor we need a radius, and a start &
          end angle. A semi-circle starts at angle 0 and ends at 180 degree or PI radians. So a full-circle extends
          further and just ends at 2*PI or 360 degrees.</p>

        <p>This exact concept can be used to draw a circle using arc method.</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, 2 * Math.PI);
ctx.stroke();</code></pre>

        <p>Here, we have drawn a circle with a center at (100, 75) and a radius of 50 pixels.</p>

        <h3>Draw a Half Circle</h3>

        <p>To draw a half circle, we change the endAngle to PI (not 2 * PI):</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, Math.PI);
ctx.fill();</code></pre>

        <h3>More About the Angles of an Arc</h3>

        <p>The following image shows some of the angles in an arc:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/angles%20of%20an%20arc.png?raw=true"
          alt="Angles of an Arc">

        <ul>
          <li>Center: arc(<text style="color: rgb(0, 223, 0)">100, 75</text>, 50, 0 * Math.PI, 1.5 * Math.PI)</li>
          <li>Start angle: arc(100, 75, 50, <text style="color: red">0</text>, 1.5 * Math.PI)</li>
          <li>End angle: arc(100, 75, 50, 0 * Math.PI, <text style="color: blue">1.5 * Math.PI</text>)</li>
          <li>Circle : arc(100, 75, 50, 0, 2 * Math.PI)</li>
        </ul>

        <h3>Line</h3>

        <p>To draw a line between two points we use moveTo(x, y)and lineTo(x, y)methods.</p>

        <p>If we consider two points A & B with x and y coordinates respectively,
          then moveTo acts as a position of A on canvas, while lineTo as position of point B.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line
ctx.stroke();</code></pre>

        <h3>The lineWidth Property</h3>

        <p>The lineWidth property defines the width of the line.</p>

        <p>It must be set before calling the stroke() method.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line 
ctx.lineWidth = 10; //width of line
ctx.stroke();</code></pre>


        <h3>Triangle</h3>

        <p>A triangle is simply three lines connected together.</p>

        <p>So, to draw a triangle we can use lineTo method to connect three points.</p>

        <p>We are going to use a special path method called as closePath() to complete our triangle. closePath basically
          adds a straight line from end coordinate to the start coordinate inside a path.</p>

        <p>If we assume a triangle is made of three points A, B & C, then we can draw our triangle like:</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(235, 114); // Point A
ctx.lineTo(135, 349); // Point B
ctx.lineTo(335, 349); // Point C
            
ctx.closePath(); // Join C & A
ctx.strokeStyle = "rgb(31 24 88)";
ctx.stroke();</code></pre>

        <h3>To Summarize Basic Shape Drawing</h3>

        <p>Apart from drawing specific rectangles and circles, drawing must be broken down into four distinct steps:</p>

        <ol>
          <li>ctx.beginPath(), to let the computer know We're beginning a new line/path</li>
          <li>ctx.moveTo(x, y), to move the 'cursor' to a specific point on the canvas without 'drawing' anything or
            recording any path</li>
          <li>ctx.lineTo(x, y), tells the computer to record a path from the current context position, in this case the
            point described by the ctx.moveTo(x, y) function—to the new coordinates provided</li>
          <li>ctx.stroke(), to then fill the described path. This is the step that actually 'draws' something onto the
            canvas</li>
        </ol>

        <p>In essence, We move the cursor to a starting position, tell the computer We're about to draw, record a path
          to a declared location, and then finally fill that path in.</p>

        <h2>Drawing Text</h2>

        <p>To draw text on the canvas, the most important property and methods are:</p>

        <ul>
          <li>font - defines the font properties for the text</li>
          <li>fillText(text, x, y) - fills a given text at the given (x, y) position</li>
          <li>strokeText(text, x, y) - strokes a given text at the given (x, y) position</li>
        </ul>

        <p>The font property defines the font to be used and the size of the font. The default value for this property
          is "10px sans serif".</p>

        <p>Both methods include an optional fourth parameter: maxwidth, which represents the maximum width of the
          text-string.</p>

        <p>Here is an example of drawing text on a canvas:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.font = "30px Arial";
ctx.fillText("Hello World", 10, 50);</code></pre>

        <p>The fillText() method draws filled text on the canvas. The default color of the text is black.</p>

        <p>The strokeText() method draws text on the canvas (no fill). The default color of the text is black.</p>


        <p><a href="https://codepen.io/amaraauguste/pen/KwPYByQ" target="_blank">Let's take a look at some example
            code</a></p>

        <hr />

        <h2>Exercise: Smiley Face</h2>

        <p>Knowing what we know now about drawing shapes in Canvas</p>

        <p>and given a canvas sized 350x350 ... </p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="350" height="350" style="border:1px solid rgb(0,0,0);"&gt;&lt;/canvas&gt;</code></pre>


        <p>How can we can create this smiley face in the center of the canvas?</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/smiley%20face%20canvas%20example.png?raw=true"
          alt="Smiley Face">


      </article>
      <br />
    </section>


    <hr />
    <section class="main-section" id="Polygons_and_Curves">
      <br />
      <header><b>Polygons and Curves</b></header>
      <article>
        <p>It is impossible for a graphics API to include every possible shape as a basic shape, but there is
          usually some way to create more complex shapes.</p>

        <p>For example, consider polygons. A <b>polygon</b>
          is a closed shape consisting of a sequence of line segments.</p>

        <p>Each line segment is joined to the
          next at its endpoint, and the last line segment connects back to the first. The endpoints are
          called the vertices of the polygon, and a polygon can be defined by listing its vertices.</p>

        <p>In a <b>regular polygon</b>, all the sides are the same length and all the angles between sides are
          equal. Squares and equilateral triangles are examples of regular polygons.</p>

        <p> A <b>convex polygon</b>
          has the property that whenever two points are inside or on the polygon, then the entire line
          segment between those points is also inside or on the polygon. Intuitively, a convex polygon
          has no "indentations" along its boundary. (Concavity can be a property of any shape, not just
          of polygons.)
        </p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/convex%20polygons.png?raw=true"
          alt="Convex Polygons">


        <p>Sometimes, polygons are required to be "simple", meaning that the polygon has no selfintersections. That is,
          all the vertices are different, and a side can only intersect another
          side at its endpoints.</p>

        <p>And polygons are usually required to be "planar", meaning that all the
          vertices lie in the same plane. (Of course, in 2D graphics, everything lies in the same plane, so
          this is not an issue. However, it does become an issue in 3D.)
        </p>


        <p>How then should we draw polygons? That is, what capabilities would we like to have in a
          graphics API for drawing them</p>

        <p>One possibility is to have commands for stroking and for filling
          polygons, where the vertices of the polygon are given as an array of points or as an array of
          x-coordinates plus an array of y-coordinates.</p>

        <p>In fact, that is sometimes done; for example, the
          Java graphics API includes such commands. </p>

        <p>Another, more flexible, approach is to introduce
          the idea of a "path."</p>

        <p>Java, SVG, and the HTML canvas API all support this idea. A path is
          a general shape that can include both line segments and curved segments. Segments can, but
          don't have to be, connected to other segments at their endpoints. </p>

        <p>A path is created by giving a series of commands that tell, essentially, how a pen would be moved to draw the
          path.</p>

        <p>While
          a path is being created, there is a point that represents the pen’s current location. There will
          be a command for moving the pen without drawing, and commands for drawing various kinds
          of segments</p>

        <p>For drawing polygons, we need commands such as: </p>

        <ul>
          <li><b>beginPath()</b> — start a new, empty path</li>
          <li><b>moveTo(x,y)</b> — move the pen to the point (x,y), without adding a segment to the path;
            that is, without drawing anything</li>
          <li><b>lineTo(x,y)</b> — add a line segment to the path that starts at the current pen location
            and ends at the point (x,y), and move the pen to (x,y)</li>
          <li><b>closePath()</b> — add a line segment from the current pen location back to the starting
            point, unless the pen is already there, producing a closed path.</li>
        </ul>

        <p>(For closePath, I need to define “starting point.” A path can be made up of “subpaths” A
          subpath consists of a series of connected segments. A moveTo always starts a new subpath.
          A closePath ends the current segment and implicitly starts a new one. So “starting point”
          means the position of the pen after the most recent moveTo or closePath.)
        </p>

        <p>Suppose that we want a path that represents a five sided polygon (a pentagon!).</p>

        <p>First of all, let's consider the unit circle. This is a circle centred at (0, 0) with radius of 1 unit.</p>


        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%201.png?raw=true"
          alt="Pentagon 1">

        <p>A regular polygon, such as a pentagon, can be drawn inside the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%202.png?raw=true"
          alt="Pentagon 2">

        <p>In order to draw the pentagon we need to be able to identify the 5 points on the unit circle and rotate and
          draw lines between them.</p>

        <p>This is where some understanding of trigonometry comes in useful.</p>

        <p>Let's consider some point, (a, b) on the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%203.png?raw=true"
          alt="Pentagon 3">

        <p>We know the radius (r), in this case it is 1 because it is the unit circle. However, it could be any length
          we choose.</p>

        <p>The point (a, b) can be written in terms of trigonometric ratios as follows:</p>

        <p>The x-ordinate is given by a = <b>r cos 𝛳</b></p>

        <p>The y-ordinate is given by b = <b>r sin 𝛳</b></p>

        <p>In Javascript we can identify the first point as:</p>

        <text class="center"><b>(x + radius * Math.cos(angle), y + radius * Math.sin(angle))</b></text>

        <p>Note that we add the (x, y) ordinate values since we will not necessarily be centering the circle at (0, 0).
        </p>

        <p>Since we are drawing a pentagon we know that the angle we will need to rotate through will be 360o / 5.
          However, all angles must be given in radians. So the angle will be 2*Pi / 5. In Javascript this is written as:
        </p>

        <text class="center"><b>angle = 2*Math.PI/numberOfSides</b></text>

        <p>We can now declare some variables:</p>

        <ul>
          <li>Since we are drawing a pentagon we set the number of sides to 5</li>
          <li>We define a radius for our circle. The pentagon will be drawn inside the circle, each vertex of the
            pentagon will be on the circumference of the circle.</li>
          <li>We set the x-ordinate of the centre of the circle</li>
          <li>We set the y-ordinate of the centre of the circle</li>
          <li>We calculate the size of the external angle of the pentagon. This is the angle we will need to rotate
            through after each line is drawn</li>
        </ul>

        <p>We can now begin the path and set up a loop to draw each line of the polygon:</p>

        <p>We move to the first point which is directly across from the centre of the circle (indicated in red below):
        </p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%204.png?raw=true"
          alt="Pentagon 4">

        <p>We set up a loop to draw each line and finally stroke the path when we are done.</p>

        <pre><code class="javascript">ctx.beginPath(); // start a new path
let numberOfSides = 5; // a pentagon
let radius=100; // radius of the circle
let x = 125; // center (x) of the circle
let y = 125; // center (y)  of the circle
let angle = 2*Math.PI/numberOfSides; // angle between sides
ctx.beginPath(); // start a new path
ctx.moveTo (x + radius*Math.cos(0), y + radius*Math.sin(0)); // first vertex      
for (let i = 1; i <= numberOfSides; i++) { // loop through each vertex
  ctx.lineTo (x + radius*Math.cos(i * angle), y + radius*Math.sin(i * angle)); // draw line to next vertex
}
ctx.stroke();</code></pre>

        <h2>Curves</h2>

        <p>As noted above, a path can contain other kinds of segments besides lines.</p>

        <p>For example,
          it might be possible to include an arc of a circle as a segment. </p>

        <p>Another type of curve is a
          <b>Bezier curve</b>. Bezier curves can be used to create very general curved shapes. They are fairly
          intuitive, so that they are often used in programs that allow users to design curves interactively.
        </p>

        <p>Mathematically, Bezier curves are defined by parametric polynomial equations, but you don’t
          need to understand what that means to use them.</p>

        <p>There are two kinds of Bezier curve in
          common use, cubic Bezier curves and quadratic Bezier curves; they are defined by cubic and
          quadratic polynomials respectively.</p>

        <p>When the general term "Bezier curve" is used, it usually
          refers to cubic Bezier curves.</p>

        <p>A cubic Bezier curve segment is defined by the two endpoints of the segment together with
          two control points. To understand how it works, it's best to think about how a pen would
          draw the curve segment.</p>

        <p>The pen starts at the first endpoint, headed in the direction of the
          first control point. The distance of the control point from the endpoint controls the speed of
          the pen as it starts drawing the curve. The second control point controls the direction and
          speed of the pen as it gets to the second endpoint of the curve. There is a unique cubic curve
          that satisfies these conditions.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/curves.png?raw=true"
          alt="Bezier Curves">


        <p>The illustration above shows three cubic Bezier curve segments.</p>

        <p>The two curve segments on
          the right are connected at an endpoint to form a longer curve. The curves are drawn as thick
          black lines. The endpoints are shown as black dots and the control points as blue squares, with
          a thin red line connecting each control point to the corresponding endpoint. (Ordinarily, only
          the curve would be drawn, except in an interface that lets the user edit the curve by hand.)</p>

        <p>Note that at an endpoint, the curve segment is tangent to the line that connects the endpoint
          to the control point. Note also that there can be a sharp point or corner where two curve
          segments meet. However, one segment will merge smoothly into the next if control points are
          properly chosen.
        </p>

        <!--ADD EXAMPLE -->

        <p><b>Quadratic Bezier</b> curve segments are similar to the cubic version, but in the quadratic case,
          there is only one control point for the segment. The curve leaves the first endpoint heading
          in the direction of the control point, and it arrives at the second endpoint coming from the
          direction of the control point. The curve in this case will be an arc of a parabola.</p>

        <p>The three most used methods for drawing curves in canvas are:</p>

        <ul>
          <li>The arc() method (which we learned to use to draw circles)</li>
          <li>The quadraticCurveTo() method</li>
          <li>The bezierCurveTo() method</li>
        </ul>

        <h3>The quadraticCurveTo() Method</h3>

        <p>The quadraticCurveTo() method is used to define a quadratic Bezier curve.</p>

        <p>The quadraticCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx - The x-coordinate of the control point</li>
          <li>cpy - The y-coordinate of the control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a quadratic Bezier curve:</p>

        <pre><code class="javascript"
>const canvas = document.getElementById("myCanvas");  
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.quadraticCurveTo(60, 10, 100, 100);
ctx.stroke();</code></pre>

        <h3>The bezierCurveTo() Method</h3>

        <p>The bezierCurveTo() method is used to define a cubic Bezier curve.</p>

        <p>The bezierCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx1 - The x-coordinate of the first control point</li>
          <li>cpy1 - The y-coordinate of the first control point</li>
          <li>cpx2 - The x-coordinate of the second control point</li>
          <li>cpy2 - The y-coordinate of the second control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a cubic Bezier curve:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.bezierCurveTo(20, 10, 100, 10, 100, 100);
ctx.stroke();</code></pre>

        <p></p>




      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="Mouse_Events">
      <br />
      <header><b>Mouse Events</b></header>
      <article>

        <p>Okay, so we can draw a some shapes, lines, text, and curves. That's great and all, but how do we get from
          that to actually drawing on the
          screen with our mouse?</p>

        <p>Since our use of Canvas is beginning to get more complex, it might help to start using functions.</p>

        <h2>JavaScript Functions</h2>

        <p>Functions are a way to group together a set of instructions that perform a specific task. They allow us to
          reuse code, make our code more organized, and easier to read and maintain.</p>

        <p>Let's start by creating a function that will draw a line on the canvas. We'll call this function drawLine.
        </p>

        <pre><code class="javascript">function drawLine(x1, y1, x2, y2) {
  ctx.beginPath(); // Start a new path
  ctx.moveTo(x1, y1); // Move the pen to the starting point
  ctx.lineTo(x2, y2); // Draw a line to the ending point
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take four arguments: x1, y1, x2, and y2, which represent the starting and ending points
          of
          the line. We then use the canvas API to draw a line between these two points.</p>

        <p>Now that we have our drawLine function, we can use it to draw lines on the canvas. For example:</p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)</code></pre>

        <p>Now, let's create a function that will draw a circle on the canvas. We'll call this function drawCircle.</p>

        <pre><code class="javascript">function drawCircle(x, y, radius) {
  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, radius, 0, Math.PI * 2); // Draw a circle
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take three arguments: x, y, and radius, which represent the center of the circle and its
          radius. We then use the canvas API to draw a circle at the specified location.</p>

        <p>Now that we have our drawCircle function, we can use it to draw circles on the canvas. For example:</p>

        <pre><code class="javascript">drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>By creating functions like drawLine and drawCircle, we can easily draw shapes on the canvas and reuse our
          code
          to create more complex drawings.</p>

        <p>Now that we know the structure of a basic JavaScript function, let's talk about event listeners.</p>

        <h2>Event Listeners</h2>

        <p>Event listeners are a way to listen for and respond to events that occur in the browser. They allow us to
          create interactive web applications that respond to user actions such as clicks, key presses, and mouse
          movements.</p>

        <p>When an event occurs, the browser triggers the event listener, which calls a function that performs a
          specific
          action. This allows us to create dynamic and interactive web pages that respond to user input.</p>

        <p>There are many different types of events that can be listened for, such as:</p>

        <ul>
          <li>Click events - Triggered when an element is clicked</li>
          <li>Mouseover events - Triggered when the mouse pointer enters an element</li>
          <li>Keydown events - Triggered when a key is pressed down</li>
          <li>Submit events - Triggered when a form is submitted</li>
          <li>Scroll events - Triggered when the user scrolls the page</li>
        </ul>

        <p>Event listeners are added to elements in the DOM using the addEventListener method. This method takes two
          arguments: the name of the event to listen for, and the function to call when the event occurs.</p>

        <p>For example, to listen for a click event on a button element, we can use the following code:</p>

        <pre><code class="javascript">const button = document.getElementById('myButton'); // Get the button element   
button.addEventListener('click', () => { // Listen for click event
  console.log('Button clicked!'); // Log a message to the console
});</code></pre>

        <p>Let's add a button to clear the canvas: </p>

        <p>First, let's create a new canvas and a button:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;
&lt;button id="clearButton"&gt;Clear Canvas&lt;/button&gt;</code></pre>

        <p>Next, let's add an event listener to the button that clears the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas'); // Get the canvas element
const ctx = canvas.getContext('2d'); // Get the 2D drawing context

const clearButton = document.getElementById('clearButton'); // Get the clear button element

clearButton.addEventListener('click', () => { // Listen for click event on clear button
  ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
});</code></pre>

        <p>In this example, we get the canvas element and its 2D drawing context, as well as the clear button element.
          We
          then add an event listener to the clear button that calls the clearRect method on the canvas context to clear
          the
          canvas when the button is clicked.</p>

        <p>So let's draw some shapes on the canvas: </p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)
drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>And now we can clear the canvas!</p>

        <h2>Drawing with the Mouse</h2>

        <p>So far, we've used JavaScript to draw shapes on the canvas and an event listener to add button functionalty
          to clear the canvas, but that's not really how we draw, is it?</p>

        <p>A good drawing application has to respond to the mouse. We need to be able to draw lines and shapes by
          clicking and dragging the mouse.</p>

        <p>We need a listener that responds to the canvas being clicked, and another listener that responds to mouse
          movement, but only when the mouse button is pressed down.</p>

        <p>Mouse events in JavaScript allow us to create interactive and dynamic web applications. By capturing mouse
          events, we can respond to user actions such as clicks, movements, and drags. This is particularly useful when
          working with the HTML5 canvas element, as it enables us to create drawing applications, games, and other
          interactive graphics.</p>

        <p>JavaScript provides several mouse events that we can listen for:</p>

        <ul>
          <li><b>mousedown</b> - Triggered when the mouse button is pressed down.</li>
          <li><b>mouseup</b> - Triggered when the mouse button is released.</li>
          <li><b>mousemove</b> - Triggered when the mouse is moved.</li>
          <li><b>click</b> - Triggered when the mouse button is clicked (pressed and released).</li>
          <li><b>dblclick</b> - Triggered when the mouse button is double-clicked.</li>
        </ul>

        <h2>Using Mouse Events with HTML Canvas</h2>

        <p>To use mouse events with the HTML5 canvas, we need to add event listeners to the canvas element. These event
          listeners will call functions that handle the events and perform actions such as drawing on the canvas.</p>

        <h3>Determining Mouse Position</h3>

        <p>To determine the mouse position within the canvas, we need to account for the canvas's position relative to
          the viewport. This can be done using the <b>getBoundingClientRect()</b> method, which returns the size of an
          element and its position relative to the viewport.</p>

        <p>Here is an example of how to determine the mouse position within the canvas:</p>

        <pre><code class="javascript">function getMousePos(canvas, event) {
const rect = canvas.getBoundingClientRect(); // Get the size and position of the canvas
  return {
    x: event.clientX - rect.left, // X coordinate of mouse relative to canvas
    y: event.clientY - rect.top // Y coordinate of mouse relative to canvas
  };
}

// Example usage
canvas.addEventListener('mousemove', (event) => { // Listen for mousemove event
  const mousePos = getMousePos(canvas, event); // Get the mouse position
  console.log('Mouse position: ' + mousePos.x + ',' + mousePos.y); // Log the mouse position
});</code></pre>

        <p>In this example:</p>

        <ul>
          <li>The <b>getMousePos</b> function takes the canvas element and the mouse event as arguments.</li>
          <li>It uses <b>getBoundingClientRect()</b> to get the position of the canvas relative to the viewport.</li>
          <li>It calculates the mouse position by subtracting the canvas's top-left corner coordinates from the mouse's
            <b>clientX</b> and <b>clientY</b> coordinates. The <b>clientX</b> and <b>clientY</b> properties of the mouse
            event provide the horizontal and vertical coordinates of the mouse pointer, respectively, relative to the
            viewport (the visible area of the browser window).
          </li>
          <li>The mouse position is logged to the console whenever the mouse is moved over the canvas.</li>
        </ul>

        <h3>Example: Drawing on Canvas with Mouse</h3>

        <p>Now let's try using the mouse to draw.</p>

        <h4>HTML</h4>
        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;</code></pre>

        <h4>JavaScript</h4>
        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function startPosition(e) {
  painting = true;
  draw(e);
}

function endPosition() {
  painting = false;
  ctx.beginPath(); // Begin a new path to avoid connecting lines
}

function draw(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.lineWidth = 5; // Set the line width
  ctx.strokeStyle = 'black'; // Set the line color

  ctx.lineTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Draw a line to the current mouse position
  ctx.stroke(); // Stroke the line
  ctx.beginPath(); // Begin a new path
  ctx.moveTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Move to the new starting point
}

canvas.addEventListener('mousedown', startPosition); // Listen for mousedown event
canvas.addEventListener('mouseup', endPosition); // Listen for mouseup event
canvas.addEventListener('mousemove', draw);</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define three functions: <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b>.</li>
          <li>The <b>startPosition</b> function is called when the mouse button is pressed down. It sets the
            <b>painting</b>
            flag to true and calls the <b>draw</b> function.
          </li>
          <li>The <b>endPosition</b> function is called when the mouse button is released. It sets the <b>painting</b>
            flag to
            false and begins a new path to avoid connecting lines.</li>
          <li>The <b>draw</b> function is called when the mouse is moved. It checks if the user is drawing, sets the
            line
            width and color, draws a line to the current mouse position, and moves to the new starting point.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b> functions
            respectively.</li>
        </ul>

        <p>In another example we can use our mouse to draw circles on the canvas: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function drawCircle(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.beginPath(); // Start a new path
  ctx.arc(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop, 10, 0, Math.PI * 2); // Draw a circle
  ctx.fill(); // Fill the circle
}

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
  painting = true; // Set the painting flag to true
  drawCircle(e); // Draw a circle
});

canvas.addEventListener('mouseup', () => { // Listen for mouseup event
  painting = false; // Set the painting flag to false
});

canvas.addEventListener('mousemove', drawCircle); // Listen for mousemove event</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define a <b>drawCircle</b> function that draws a circle at the current mouse position.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>drawCircle</b> function
            when the mouse button is pressed down, released, and moved respectively.</li>
        </ul>

        <p>We can simplify the code a bit by using boundingClientRect which gives the size of an element and its
          position
          relative to the viewport:</p>

        <pre><code class="javascript
">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

  ctx.beginPath(); // Start a new path
  ctx.arc(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top, 10, 0, Math.PI * 2); // Draw a circle
  ctx.fill(); // Fill the circle
  </code></pre>

        <p>Although this works, we can achieve the same result a bit simpler with offsetX and offsetY:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
  ctx.beginPath(); // Start a new path
  ctx.arc(e.offsetX, e.offsetY, 10, 0, Math.PI * 2); // Draw a circle
  ctx.fill(); // Fill the circle
});</code></pre>

        <p>This works because offsetX and offsetY give the position of the mouse <b>relative to the target element</b>, in this
          case the canvas.</p>

        <p>So to rewrite our draw function:</p>

        <pre><code class="javascript">function draw(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.lineWidth = 5; // Set the line width
  ctx.strokeStyle = 'black'; // Set the line color

  ctx.lineTo(e.offsetX, e.offsetY); // Draw a line to the current mouse position
  ctx.stroke(); // Stroke the line
  ctx.beginPath(); // Begin a new path
  ctx.moveTo(e.offsetX, e.offsetY); // Move to the new starting point
}</code></pre>

        <p>And that's it! We can now draw on the canvas with our mouse.</p>

        <p>By combining mouse events with the canvas API, we can create interactive drawing applications that respond to
          user
          input.</p>

        <hr />

        <h2>Exercise: Drawing More Figures in HTML5 Canvas</h2>

        <p>How can we draw the following graphics on a 400 x 400 canvas?</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/Exercise%202.png?raw=true"
          alt="Drawing Exercise">


      </article>
      <br />
    </section>

    <hr />



    <section class="main-section" id="Additional_Events">
      <br />
      <header><b>Additional Events</b></header>
      <article>
        <p>In addition to our mouse down events, there are also a few others that may come in handy (for both 2D and 3D
          graphics)</p>

        <h2>Keyboard Events</h2>

        <p>Keyboard events are triggered when a key is pressed or released on the keyboard. They allow us to respond to
          user input and create interactive web applications that respond to key presses.</p>

        <p>There are several keyboard events that we can listen for:</p>

        <ul>
          <li><b>keydown</b> - Triggered when a key is pressed down.</li>
          <li><b>keyup</b> - Triggered when a key is released.</li>
          <li><b>keypress</b> - Triggered when a key is pressed down and released.</li>
        </ul>

        <p>Keyboard events provide information about the key that was pressed, such as the key code and the key value.
          This information can be used to perform specific actions based on the key that was pressed.</p>

        <p>Here is an example of how to listen for keyboard events:</p>

        <pre><code class="javascript">document.addEventListener('keydown', (event) => { // Listen for keydown event
  console.log('Key pressed: ' + event.key); // Log the key that was pressed

  if (event.key == 'ArrowUp') {
    console.log('Up arrow key pressed'); // Log a message if the up arrow key was pressed
  }

  if (event.key == 'ArrowDown') {
    console.log('Down arrow key pressed'); // Log a message if the down arrow key was pressed
  }

  // Add more key press conditions as needed
});</code></pre>

        <p>Each key on the keyboard has a unique key code and key value. The key code is a numerical value that
          represents
          the key, while the key value is a string that represents the key.</p>

        <p>For example, the key code for the up arrow key is 38, and the key value is 'ArrowUp'.</p>

        <p>Standard key codes are as follows: </p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/keycodes.png?raw=true"
          alt="Key Codes">


        <p>Keyboard events can be used to create keyboard shortcuts, control game characters, and more.</p>

        <p>For example, let's write some code to create a rectangle that we will be able to move with the WASD and
          directional arrow keys: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let x = 100; // Initial x position of the rectangle
let y = 100; // Initial y position of the rectangle
let speed = 5; // Speed of the rectangle

function drawRectangle() {
    ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
    ctx.fillStyle = 'blue'; // Set rectangle color
    ctx.fillRect(x, y, 50, 50); // Draw the rectangle
}

document.addEventListener('keydown', (event) => { // Listen for keydown event
    switch (event.key) {
        case 'ArrowUp':
        case 'w':
            y -= speed; // Move the rectangle up
            break;
        case 'ArrowDown':
        case 's':
            y += speed; // Move the rectangle down
            break;
        case 'ArrowLeft':
        case 'a':
            x -= speed; // Move the rectangle left
            break;
        case 'ArrowRight':
        case 'd':
            x += speed; // Move the rectangle right
            break;
    }

    drawRectangle(); // Redraw the rectangle
});

// Initial call to draw the rectangle when the page loads
drawRectangle();</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define the initial x and y positions of the rectangle, as well as the speed at which it will move.</li>
          <li>We define a <b>drawRectangle</b> function that clears the canvas and draws a rectangle at the current
            position.</li>
          <li>We add an event listener for the <b>keydown</b> event that moves the rectangle based on the key that was
            pressed.
          </li>
          <li>We use a <b>switch</b> statement to check which key was pressed and update the x and y positions of the
            rectangle
            accordingly.</li>
          <li>We call the <b>drawRectangle</b> function to redraw the rectangle after it has been moved.</li>
        </ul>


        <h2>Touch Events</h2>

        <p>In the early days of touch-enabled devices, touch events were often interpreted and essentially "translated"
          into mouse events for compatibility with existing web applications, meaning that a touch interaction would
          trigger a corresponding mouse event like a click or hover, although this approach had limitations when dealing
          with multi-touch gesture.</p>

        <p>With the widespread adoption of touchscreen devices, such as a smartphone or tablet, HTML5 brings to the
          table, among many other things, a set of touch-based interaction events. </p>

        <p>Mouse-based events such as hover, mouse in, mouse out etc. aren't able to adequately capture the range of
          interactions possible via touchscreen, so touch events are a welcome and necessary addition to the web
          developer's toolbox.</p>

        <p>They allow us to create web applications that respond to touch gestures, such as tapping, swiping, and
          pinching.</p>

        <p>Use cases for the touch events API include gesture recognition, multi-touch, drag and drop, and any other
          touch-based interfaces.</p>

        <h2>The Touch Events API</h2>

        <p>We'll get some of the technical details of the API out of the way first, before moving on to some real
          examples. The API is
          defined in terms of Touches, TouchEvents, and TouchLists.</p>

        <p>Each Touch describes a touch point, and has the following attributes:</p>

        <ul>
          <li><b>clientX</b> - The x-coordinate of the touch point relative to the viewport.</li>
          <li><b>clientY</b> - The y-coordinate of the touch point relative to the viewport.</li>
          <li><b>screenX</b> - The x-coordinate of the touch point relative to the screen.</li>
          <li><b>screenY</b> - The y-coordinate of the touch point relative to the screen.</li>
          <li><b>pageX</b> - The x-coordinate of the touch point relative to the document.</li>
          <li><b>pageY</b> - The y-coordinate of the touch point relative to the document.</li>
          <li><b>target</b> - The element that the touch point started in.</li>
          <li><b>identifier</b> - A unique identifier for the touch point.</li>
        </ul>

        <p>There are several touch events that we can listen for:</p>

        <ul>
          <li><b>touchstart</b> - Triggered when a touch point is placed on the touch surface.</li>
          <li><b>touchmove</b> - Triggered when a touch point is moved along the touch surface.</li>
          <li><b>touchend</b> - Triggered when a touch point is removed from the touch surface.</li>
          <li><b>touchcancel</b> - Triggered when a touch point is disrupted in some way.</li>
        </ul>

        <p>Touch events provide information about the touch points, such as the touch coordinates and the touch
          identifier. This information can be used to perform specific actions based on the touch gestures.</p>

        <p>Here is an example of how to listen for touch events:</p>

        <pre><code class="javascript">document.addEventListener('touchstart', (event) => { // Listen for touchstart event
  console.log('Touch started at: ' + event.touches[0].clientX + ',' + event.touches[0].clientY); // Log the touch
  coordinates
});</code></pre>

        <p>Let's go one step further and display the touch position information visually, by displaying a dot on the
          canvas at the point where it was touched.</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('touchstart', (event) => { // Listen for touchstart event
  const touch = event.touches[0]; // Get the first touch point
  const x = touch.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
  const y = touch.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the touch point
  ctx.fill(); // Fill the circle
});</code></pre>

        <p>In this example:</p>
        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We add an event listener for the <b>touchstart</b> event to the canvas element.</li>
          <li>We get the first touch point from the <b>touches</b> property of the event.</li>
          <li>We calculate the x and y coordinates of the touch point relative to the canvas.</li>
          <li>We draw a circle at the touch point using the <b>arc</b> method and fill it using the <b>fill</b> method.
          </li>
        </ul>

        <h2>Browser Support and Fallbacks</h2>

        <p>Touch events are widely supported among mobile devices.</p>

        <p>However, unless specifically targeting touch devices, a fallback should be implemented when touchevents are
          not supported. In these cases, the traditional click etc. events can be bound to, but as discussed below, care
          is needed when deciding which events to support instead of the touch events.</p>

        <h2>Touch and Mouse Events</h2>

        <p>Since touch events may not be supported on a user's device - indeed, the user may not even be accessing your
          app on a touchscreen device - this contingency should be planned for. </p>

        <p>You may want to enable your app to support particular mouse events instead. Care should be taken here as
          there is not a one-to-one correspondance between mouse events and touch events, and the behaviour differences
          can be subtle.</p>


        <p>So for our code we should also enable mouse events:</p>

        <pre><code class="javascript">canvas.addEventListener('mousedown', (event) => { // Listen for mousedown event
const x = event.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
const y = event.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

ctx.beginPath(); // Start a new path
ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the mouse point
ctx.fill(); // Fill the circle
});</code></pre>

        <h2>Best Practices</h2>

        <p>Care should also be taken implementing touch events that the events don't interfere with typical browser
          behaviours such as scrolling and zooming - thus there is an argument for disabling these default browser
          behaviours if you are making use of touch events.</p>

        <p>It is also important to remember that touch events are not the same as mouse events.</p>

        <p>For example, a touchstart event is not the same as a mousedown event. The former is triggered when a touch
          point is placed on the touch surface, while the latter is triggered when a mouse button is pressed down. </p>

        <p>Therefore, it is important to consider the differences between touch and mouse events when designing
          touch-based interfaces.</p>

        <p>Finally, it is important to test touch events on a variety of devices to ensure that they work as expected
          and provide a good user experience.</p>

        <p>In addition, there are also scroll events, resize events, and more. These can all be used to create more
          interactive and dynamic web applications.</p>

      </article>
      <br />
    </section>

    <hr />
    <section class="main-section" id="Transforms">
      <br />

      <header><b>Transforms</b></header>
      <article>
        <p>It is possible to transform
          coordinates from one coordinate system to another. Let's look at how geometric transformations can be used to
          place graphics
          objects into a coordinate system.</p>

        <h2>Viewing and Modeling</h2>

        <p>In a typical application, we have a rectangle made of pixels, with its natural pixel coordinates,
          where an image will be displayed. This rectangle will be called the <b>viewport</b>.</p>

        <p>We also have
          a set of geometric objects that are defined in a possibly different coordinate system, generally
          one that uses real-number coordinates rather than integers. These objects make up the “scene”
          or "world" that we want to view, and the coordinates that we use to define the scene are called
          <b>world coordinates</b>.
        </p>

        <p>For 2D graphics, the world lies in a plane. It's not possible to show a picture of the entire
          infinite plane. We need to pick some rectangular area in the plane to display in the image.
          Let's call that rectangular area the <b>window</b>, or view window.</p>

        <p>A coordinate transform is used
          to map the window to the viewport.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/window%20and%20viewport.png?raw=true"
          alt="Coordinate Transformation">

        <p>In this illustration, T represents the coordinate transformation. T is a function that takes world
          coordinates (x,y) in some window and maps them to pixel coordinates T(x,y) in the viewport</p>

        <p>In this example, as you can
          check,</p>

        <text class="center">T(x,y) = ( 800*(x+4)/8, 600*(3-y)/6 )</text>

        <p>Look at the rectangle with corners at (-1,2) and (3,-1) in the window. When this rectangle is
          displayed in the viewport, it is displayed as the rectangle with corners T(-1,2) and T(3,-1). In
          this example, T(-1,2) = (300,100) and T(3,-1) = (700,400).</p>

        <p>We use coordinate transformations in this way because it allows us to choose a world
          coordinate system that is natural for describing the scene that we want to display, and it is easier to do
          that than to work directly with viewport coordinates. Along the same lines,
          suppose that we want to define some complex object, and suppose that there will be several
          copies of that object in our scene. Or maybe we are making an animation, and we would like the
          object to have different positions in different frames.</p>

        <p>We would like to choose some convenient
          coordinate system and use it to define the object once and for all.</p>

        <p>The coordinates that we
          use to define an object are called <b>object coordinates</b> for the object. </p>

        <p>When we want to place
          the object into a scene, we need to transform the object coordinates that we used to define the
          object into the world coordinate system that we are using for the scene. The transformation that
          we need is called a <b>modeling transformation</b>. </p>

        <p>This picture illustrates an object defined in
          its own object coordinate system and then mapped by three different modeling transformations
          into the world coordinate system:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/modeling%20transformation.png?raw=true"
          alt="Modeling Transformation">

        <p>Remember that in order to view the scene, there will be another transformation that maps the
          object from a view window in world coordinates into the viewport.</p>

        <p>Now, keep in mind that the choice of a view window tells which part of the scene is shown
          in the image. Moving, resizing, or even rotating the window will give a different view of the
          scene. Suppose we make several images of the same car:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/modeling%20transformation%202.png?raw=true"
          alt="Modeling Transformation 2">

        <p>What happened between making the top image in this illustration and making the image on
          the bottom left?</p>

        <p>In fact, there are two possibilities: Either the car was moved to the right, or
          the view window that defines the scene was moved to the left.</p>

        <p>This is important, so be sure
          you understand it. (Try it with your cell phone camera. Aim it at some objects, take a step
          to the left, and notice what happens to the objects in the camera's viewfinder: They move to the right in the
          picture!)</p>

        <p>Similarly, what happens between the top picture and the middle
          picture on the bottom? Either the car rotated counterclockwise, or the window was rotated
          clockwise. (Again, try it with a camera—you might want to take two actual photos so that you
          can compare them.)</p>

        <p>Finally, the change from the top picture to the one on the bottom right
          could happen because the car got smaller or because the window got larger. (On your camera,
          a bigger window means that you are seeing a larger field of view, and you can get that by
          applying a zoom to the camera or by backing up away from the objects that you are viewing.)</p>

        <p>There is an important general idea here. When we modify the view window, we change
          the coordinate system that is applied to the viewport. But in fact, this is the same as leaving
          that coordinate system in place and moving the objects in the scene instead. Except that to
          get the same effect in the final image, you have to apply the opposite transformation to the
          objects (for example, moving the window to the left is equivalent to moving the objects to the
          right).</p>

        <p>So, there is no essential distinction between transforming the window and transforming
          the object. Mathematically, you specify a geometric primitive by giving coordinates in some
          natural coordinate system, and the computer applies a sequence of transformations to those
          coordinates to produce, in the end, the coordinates that are used to actually draw the primitive
          in the image.</p>

        <p>You will think of some of those transformations as modeling transforms and some
          as coordinate transforms, but to the computer, it's all the same.</p>

        <p>We will return to this idea several times later throughout this class, but in any case, you can see that
          geometric transforms are a central concept in computer graphics. Let's look at some basic types
          of transformation in more detail.</p>

        <p>The transforms we will use in 2D graphics can be written in
          the form:</p>

        <text class="center"><b>x1 = a*x + b*y + e</b></text>

        <text class="center"><b>y1 = c*x + d*y + f</b></text>

        <p>where (x,y) represents the coordinates of some point before the transformation is applied, and
          (x1,y1 ) are the transformed coordinates.</p>

        <p>The transform is defined by the six constants a, b, c,
          d, e, and f. Note that this can be written as a function T, where</p>

        <text class="center">T(x,y) = (a*x + b*y + c, d*x + e*y + f)</text>

        <p>A transformation of this form is called an <b>affine transform</b>.</p>
        <p>An affine transform has the
          property that, when it is applied to two parallel lines, the transformed lines will also be parallel.
          Also, if you follow one affine transform by another affine transform, the result is again an affine
          transform.</p>

        <p>There are four basic types of affine transforms that are commonly used in computer graphics:</p>

        <ul>
          <li><b>Translation</b> - Moves an object by a specified distance along the x and y axes.</li>
          <li><b>Rotation</b> - Rotates an object by a specified angle around a specified point.</li>
          <li><b>Scaling</b> - Increases or decreases the size of an object by a specified factor along the x and y
            axes.</li>
          <li><b>Shearing</b> - Skews an object by a specified angle along the x or y axis.</li>
        </ul>

        <h2>Translation</h2>

        <p>A translation transform simply moves every point by a certain amount horizontally and a
          certain amount vertically.</p>

        <p>If (x,y) is the original point and (x1,y1) is the transformed point,
          then the formula for a translation is:</p>

        <text class="center">x1 = x + e</text>

        <text class="center">y1 = y + f</text>

        <p>where e is the number of units by which the point is moved horizontally and f is the amount by
          which it is moved vertically. (Thus for a translation, a = d = 1, and b = c = 0 in the general
          formula for an affine transform.)</p>

        <p>A 2D graphics system will typically have a function such as:</p>

        <text class="center"><b>translate(e, f)</b></text>

        <p>to apply a translate transformation. The translation would apply to everything that is drawn
          <b>after</b> the command is given. That is, for all subsequent drawing operations, e would be added
          to the x-coordinate and f would be added to the y-coordinate.
        </p>

        <p>Let's look at an example: Suppose that you draw an "F" using coordinates in which the "F" is centered at
          (0,0).</p>

        <p>If you say translate(4,2) before drawing the "F", then every point of the "F" will be moved
          horizontally by 4 units and vertically by 2 units before the coordinates are actually used, so
          that after the translation, the "F" will be centered at (4,2):</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/translation.png?raw=true"
          alt="Translation Transformation">

        <p>The light gray "F" in this picture shows what would be drawn without the translation; the dark
          red "F" shows the same "F" drawn after applying a translation by (4,2).</p>

        <p>The top arrow shows
          that the upper left corner of the "F" has been moved over 4 units and up 2 units. Every point
          in the "F" is subjected to the same displacement.</p>

        <p>Note that in these examples, we are assuming
          that the y-coordinate increases from bottom to top. That is, the y-axis points up.</p>

        <p>Remember that when you give the command translate(e,f), the translation applies to <b>all</b> the
          drawing that you do after that, not just to the next shape that you draw.</p>

        <p>If you apply another
          transformation after the translation, the second transform will not replace the translation.
          It will be combined with the translation, so that subsequent drawing will be affected by the
          combined transformation.</p>

        <p>For example, if you combine translate(4,2) with translate(-1,5), the
          result is the same as a single translation, translate(3,7). This is an important point, and there
          will be a lot more to say about it later.
        </p>

        <p>Also remember that you don't compute coordinate transformations yourself. You just
          specify the original coordinates for the object (that is, the object coordinates), and you specify
          the transform or transforms that are to be applied. The computer takes care of applying the
          transformation to the coordinates.</p>

        <p>You don't even need to know the exact equations that are used
          for the transformation; <b>you just need to understand what it does geometrically</b>.</p>

        <p>For example, here is how you might use the translate function in HTML5 Canvas:</p>

        <p>First, draw one rectangle in position (10,10), then set translate() to (70,70) (This will be the new start
          point). Then draw another rectangle in position (10,10). Notice that the second rectangle now starts in
          position (80,80):</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(10, 10, 100, 50);

ctx.translate(70, 70);

ctx.fillStyle = "blue";
ctx.fillRect(10, 10, 100, 50);</code></pre>

        <h2>Rotation</h2>

        <p>A rotation transform, for our purposes here, rotates each point about the origin, (0,0). Every
          point is rotated through the same angle, called the angle of rotation.</p>

        <p>For this purpose, angles can be measured either in degrees or in radians.</p>

        <p>A rotation with a positive angle rotates objects in the direction from the positive x-axis towards
          the positive y-axis.</p>

        <p>This is counterclockwise in a coordinate system (cartesian) where the y-axis points up,
          as it does in my examples here, but it is clockwise in the usual pixel coordinates, where the
          y-axis points down rather than up.</p>

        <p>Although it is not obvious, when rotation through an angle of r radians about the origin is applied to the
          point (x,y), then the resulting point (x1,y1 ) is
          given by: </p>

        <text class="center">x1 = cos(r) * x - sin(r) * y</text>
        <text class="center">y1 = sin(r) * x + cos(r) * y</text>

        <p>That is, in the general formula for an affine transform, e = f = 0, a = d = cos(r), b = -sin(r),
          and c = sin(r).</p>

        <p>Here is a picture that illustrates a rotation about the origin by the angle
          negative 135 degrees:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rotation.png?raw=true"
          alt="Rotation Transformation">


        <p>Again, the light gray "F" is the original shape, and the dark red "F" is the shape that results
          if you apply the rotation. The arrow shows how the upper left corner of the original “F” has
          been moved.</p>

        <p>A 2D graphics API would typically have a command rotate(r) to apply a rotation. The
          command is used before drawing the objects to which the rotation applies.</p>

        <p>For example, HTML5 Canvas
          has a rotate method that takes an angle in radians as a parameter. The rotation is applied to all
          subsequent drawing operations.</p>

        <p>Let's rotate a rectangle in canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(50, 10, 100, 50);

ctx.rotate((Math.PI/180)*20); // Rotate 20 degrees

ctx.strokeStyle = "blue";
ctx.strokeRect(70, 30, 100, 50);</code></pre>

        <p>Remember that the rotation is applied to all subsequent drawing operations, so we need to
          translate to the center of the rectangle to rotate it around its center.</p>

        <h2>Scaling</h2>

        <p>A scaling transform can be used to make objects bigger or smaller.</p>

        <p>Mathematically, a scaling
          transform simply multiplies each x-coordinate by a given amount and each y-coordinate by a
          given amount.</p>

        <p>That is, if a point (x,y) is scaled by a factor of a in the x direction and by a
          factor of d in the y direction, then the resulting point (x1,y1 ) is given by:</p>

        <text class="center">x1 = a*x</text>
        <text class="center">y1 = d*y</text>

        <p>If you apply this transform to a shape that is centered at the origin, it will stretch the shape
          by a factor of a horizontally and d vertically.</p>

        <p>Here is an example, in which the original light
          gray "F" is scaled by a factor of 3 horizontally and 2 vertically to give the final dark red "F":</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/scaling.png?raw=true"
          alt="Scaling Transformation">


        <p>The common case where the horizontal and vertical scaling factors are the same is called
          <b>uniform scaling</b>. Uniform scaling stretches or shrinks a shape without distorting it.
        </p>

        <p>When scaling is applied to a shape that is not centered at (0,0), then in addition to being
          stretched or shrunk, the shape will be moved away from 0 or towards 0. In fact, the true
          description of a scaling operation is that it pushes every point away from (0,0) or pulls every
          point towards (0,0). If you want to scale about a point other than (0,0), you can use a sequence
          of three transforms, similar to what was done in the case of rotation.</p>

        <p>A 2D graphics API can provide a function scale(a,d) for applying scaling transformations.
          As usual, the transform applies to all x and y coordinates in subsequent drawing operations.</p>

        <p>One unit on the canvas is one pixel. If we set the scaling factor to 2, one unit becomes two pixels, and
          shapes will be drawn twice as large. If we set a scaling factor to 0.5, one unit becomes 0.5 pixels, and
          shapes will be drawn at half size.</p>

        <p>Note that negative scaling factors are allowed and will result in reflecting the shape as well
          as possibly stretching or shrinking it. For example, scale(1,-1) will reflect objects vertically,
          through the x -axis.</p>

        <p>As with the other transformations, a scaling transform is applied to all subsequent drawing
          operations. In HTML5 Canvas, you can use the scale method to apply a scaling transform:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.scale(2, 2);

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>It is a fact that every affine transform can be created by combining translations,
          rotations about the origin, and scalings about the origin. </p>

        <p>Also note that a transform that is made from translations and rotations, with no scaling,
          will preserve length and angles in the objects to which it is applied. It will also preserve aspect
          ratios of rectangles.</p>

        <p>Transforms with this property are called <b>"Euclidean"</b>. If you also allow
          uniform scaling, the resulting transformation will preserve angles and aspect ratio, but not
          lengths.
        </p>

        <h2>Shearing</h2>

        <p>We will look at one more type of basic transform, a <b>shearing transform</b>.</p>

        <p>Although shears
          can in fact be built up out of rotations and scalings if necessary, it is not really obvious how
          to do so.</p>

        <p>A shear will "tilt" objects. A horizontal shear will tilt things towards the left (for
          negative shear) or right (for positive shear). A vertical shear tilts them up or down. </p>

        <p>Here is an
          example of horizontal shear:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shearing.png?raw=true"
          alt="Shearing Transformation">


        <p>A horizontal shear does not move the x-axis. Every other horizontal line is moved to the left or
          to the right by an amount that is proportional to the y-value along that line.</p>

        <p>When a horizontal shear is applied to a point (x,y), the resulting point (x1,y1) is given by:
        </p>

        <text class="center">x1 = x + b * y</text>
        <text class="center">y1 = y</text>

        <p>for some constant shearing factor b. Similarly, a vertical shear with shearing factor c is given
          by the equations:</p>

        <text class="center">x1 = x</text>
        <text class="center">y1 = c * x + y</text>

        <p>Shear is occasionally called "skew", but skew is usually specified as an angle rather than as a
          shear factor.</p>















        <h2>Combining Transformations</h2>

        <p>As we just saw, we are now in a position to see what can happen when you combine two transformations.
          Suppose that before drawing some object, you say:</p>

        <text class="center"><b>translate(4,0)</b></text>
        <text class="center"><b>rotate(90)</b></text>

        <p>Assume that angles are measured in degrees.</p>

        <p>The translation will then apply to all subsequent
          drawing. But, because of the rotation command, the things that you draw after the translation
          are <b>rotated</b> objects.</p>

        <p>That is, the translation applies to objects that have <b>already</b> been rotated.</p>

        <p>An example is shown on the left in the illustration below, where the light gray "F" is the original
          shape, and red "F" shows the result of applying the two transforms to the original.</p>

        <p>The original "F" was first rotated through a 90 degree angle, and then moved 4 units to the right.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/combining%20transforms.png?raw=true"
          alt="Combining Transformations">


        <p>Note that transforms are applied to objects in the reverse of the order in which they are given
          in the code (because the first transform in the code is applied to an object that has already
          been affected by the second transform). And note that the order in which the transforms are
          applied is important.</p>

        <p>If we reverse the order in which the two transforms are applied in this
          example, by saying: </p>

        <text class="center"><b>rotate(90)</b></text>
        <text class="center"><b>translate(4,0)</b></text>

        <p>then the result is as shown on the right in the above illustration. In that picture, the original
          "F" is first moved 4 units to the right and the resulting shape is then rotated through an angle
          of 90 degrees about the origin to give the shape that actually appears on the screen.</p>

        <p>For another example of applying several transformations, suppose that we want to rotate
          a shape through an angle r about a point (p,q) instead of about the point (0,0).</p>

        <p>We can do
          this by first moving the point (p,q) to the origin, using translate(-p,-q). Then we can do a
          standard rotation about the origin by calling rotate(r). Finally, we can move the origin back
          to the point (p,q) by applying translate(p,q).</p>

        <p>Keeping in mind that we have to write the code
          for the transformations in the reverse order, we need to say: </p>

        <text class="center"><b>translate(p,q)</b></text>
        <text class="center"><b>rotate(r)</b></text>
        <text class="center"><b>translate(-p,-q)</b></text>

        <p>before drawing the shape. (In fact, some graphics APIs let us accomplish this transform with a
          single command such as rotate(r,p,q). This would apply a rotation through the angle r about
          the point (p,q).)
        </p>

        <p>In HTML5 Canvas, we have a method called transform().</p>

        <p>The transform() method replaces the current transformation matrix with the matrix described by the
          arguments of this method. The transform() method multiplies the current transformation matrix with the
          matrix described by the arguments of this method. This is useful for applying multiple transformations to
          the same shape.</p>

        <p>The transform method takes the following parameters: </p>

        <ul>
          <li><b>a</b> - Horizontal scaling. 1 is no scaling.</li>
          <li><b>b</b> - Horizontal skewing.</li>
          <li><b>c</b> - Vertical skewing.</li>
          <li><b>d</b> - Vertical scaling. 1 is no scaling.</li>
          <li><b>e</b> - Horizontal moving.</li>
          <li><b>f</b> - Vertical moving.</li>
        </ul>
        <p>Here is an example of how to use the transform() method in HTML5 Canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.transform(2, 0, 0, 2, 40, 40); // Scale by 2 and move 40 units
ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>We can scale, rotate, and translate using a mixture of these methods as well:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.translate(200, 40); // Move 200 units to the right and 40 units down
ctx.rotate((Math.PI/180)*45); // Rotate 45 degrees
ctx.scale(2, 2); // Scale by 2

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>


        <p>Note that we will talk again about transformations in the context of 3D graphics,
          where they
          become even more important.</p>


      </article>
      <br />
    </section>


    <!-- <section class="main-section" id="Linear_Algebra">
      <br />
      <header><b>Linear Algebra</b></header>
      <article>
        <p>Linear algebra is a branch of mathematics that deals with vectors and matrices. It is a fundamental tool in
          computer graphics, as it allows us to perform transformations on objects in 2D and 3D space.</p>

        <h2>Vectors</h2>

        <p>A vector is a mathematical object that has both magnitude and direction. It is often represented as an arrow
          in space, with the length of the arrow representing the magnitude of the vector and the direction of the arrow
          representing the direction of the vector.</p>

        <p>Vectors can be represented in different ways, such as:</p>

        <ul>
          <li>As a list of numbers, such as [x, y, z] for a 3D vector.</li>
          <li>As a column matrix, such as [x y z].</li>
          <li>As a row matrix, such as [x y z].</li>
        </ul>

        <p>Vectors can be added, subtracted, multiplied by a scalar, and normalized. They can also be used to represent
          points in space, directions, velocities, and forces.</p>

        <h2>Matrices</h2>

        <p>A matrix is a two-dimensional array of numbers arranged in rows and columns. It is often used to represent
          linear
          transformations, such as rotations, translations, and scaling.</p>

        <p>Matrices can be multiplied together to combine transformations, and they can be inverted to undo
          transformations. They can also be used to solve systems of linear equations and find eigenvectors and
          eigenvalues.</p>

        <h2>Transformations</h2>

        <p>Transformations are operations that change the position, orientation, or size of an object in space. They can
          be represented as matrices that act on vectors to produce new vectors.</p>

        <p>There are several types of transformations that are commonly used in computer graphics:</p>

        <ul>
          <li>Translation - Moves an object from one position to another.</li>
          <li>Rotation - Rotates an object around a point or axis.</li>
          <li>Scaling - Changes the size of an object.</li>
          <li>Shearing - Skews an object along one axis.</li>
          <li>Reflection - Flips an object across a line or plane.</li>
        </ul>

        <p>These transformations can be combined to create more complex transformations, such as scaling and rotating an
          object, or translating and reflecting an object.</p>

        <p>The transforms that are used in computer graphics can be represented as matrices, and the
          points on which they operate are represented as vectors.</p>

        <p>Recall that a <b>matrix</b>, from the point
          of view of a computer scientist, is a <b>two-dimensional array of numbers</b>, while a <b>vector</b> is a
          <b>one-dimensional array</b>.</p>

        <p>Matrix and vector math
          is built into GPUs. You won't need to know a great deal about linear algebra for this class,
          but a few basic ideas are essential.
        </p>

        <p>The vectors that we need are lists of two, three, or four numbers. They are often written
          as (x,y), (x,y,z ), and (x,y,z,w).</p>

        <p>A matrix with N rows and M columns is called an "N-by-M
          matrix". For the most part, the matrices that we need are N-by-N matrices, where N is 2, 3,
          or 4. That is, they have 2, 3, or 4 rows and columns, and the number of rows is equal to the
          number of columns.</p>

        <p>If A and B are two N-by-N matrices, then they can be multiplied to give a product matrix
          C = AB.</p>

        <p>If A is an N-by-N matrix, and v is a vector of length N, then v can be multiplied
          by A to give another vector w = Av</p>

        <p>The function that takes v to Av is a transformation; it
          transforms any given vector of size N into another vector of size N.</p>

        <p>A transformation of this
          form is called a <b>linear transformation</b>.</p>

        <p>Now, suppose that A and B are N-by-N matrices and v is a vector of length N. Then, we can
          form two different products: A(Bv) and (AB)v</p>

        <p> It is a central fact that these two operations
          have the same effect. That is, we can multiply v by B and then multiply the result by A, or
          we can multiply the matrices A and B to get the matrix product AB and then multiply v by
          AB. The result is the same.
        </p>

        <p><b>Rotation and scaling, as it turns out, are linear transformations</b>. That is, the operation of
          rotating (x,y) through an angle d about the origin can be done by multiplying (x,y) by a 2-by-2
          matrix. Let's call that matrix Rd.</p>

        <p>Similarly, scaling by a factor a in the horizontal direction
          and b in the vertical direction can be given as a matrix Sa,b.</p>

        <p>If we want to apply a scaling
          followed by a rotation to the point v = (x,y), <b>we can compute either Rd(Sa,bv) or (RdSa,b)v</b>.</p>

        <p>So what? Well, suppose that we want to apply the same two operations, scale then rotate, to
          thousands of points, as we typically do when transforming objects for computer graphics.</p>

        <p>The point is that we could compute the product matrix RdSa,b once and for all, and then apply the
          combined transform to each point with a single multiplication.</p>

        <p>This means that if a program
          says:</p>

        <pre><code class="javascript">rotate(d)
scale(a,b)
.
. // draw a complex object
. </code></pre>

        <p>the computer doesn't have to keep track of two separate operations.</p>

        <p>It combines the operations
          into a single matrix and just keeps track of that. Even if you apply, say, 50 transformations to
          the object, the computer can just combine them all into one matrix. By using matrix algebra,
          multiple transformations can be handled as efficiently as a single transformation!</p>

        <p>This is really nice, but there is a gaping problem: <b>Translation is not a linear
            transformation</b>.</p>

        <p>To bring translation into this framework, we do something that looks a
          little strange at first: Instead of representing a point in 2D as a pair of numbers (x,y), we
          represent it as the triple of numbers (x,y,1).</p>

        <p>That is, we add a one as the third coordinate. It
          then turns out that we can then represent rotation, scaling, and translation—and hence any
          affine transformation—on 2D space as multiplication by a 3-by-3 matrix.</p>

        <p>The matrices that we
          need have a bottom row containing (0,0,1). Multiplying (x,y,1) by such a matrix gives a new
          vector (x1,y1,1). We ignore the extra coordinate and consider this to be a transformation of
          (x,y) into (x1,y1 ). </p>

        ADD IMAGE HERE

        <p>You can compare multiplication by these matrices to the formulas given above for translation,
          scaling, and rotation.</p>

        <p>But when doing graphics programming, you won't need to do the
          multiplication yourself. For now, the important idea that you should take away from this
          discussion is that a sequence of transformations can be combined into a single transformation.</p>

        <p>The computer only needs to keep track of a single matrix, which we can call the "current
          matrix" or "current transformation". To implement transform commands such as translate(a,b)
          or rotate(d), the computer simply multiplies the current matrix by the matrix that represents
          the transform.</p>





















      </article>
      <br />
    </section>-->






    <hr />
    <section class="main-section" id="Reference">
      <br />
      <header><b>Reference</b></header>
      <article>
        <p>Documentation on this page is taken from the following:</p>
        <ul>
          <li>
            Eck, D. J. (2023). Introduction to computer graphics (Version 1.4). Hobart and William Smith Colleges.
            Retrieved from https://math.hws.edu/graphicsbook/
          </li>
          <li>
            <a href="https://www.tutorialspoint.com/computer_graphics/index.htm" target="_blank">Tutorials Point</a>
          </li>
          <li>
            <a href="https://learn.leighcotnoir.com/artspeak/elements-color/hue-value-saturation/"
              target="_blank">Learn. (Leigh Cotnoir's art and design courses)</a>
          </li>
          <br />

        </ul>
      </article>
    </section>
  </main>
  <script>
    var dropdown = document.getElementsByClassName("dropdown-btn");
    var i;

    for (i = 0; i < dropdown.length; i++) {
      dropdown[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var dropdownContent = this.nextElementSibling;
        if (dropdownContent.style.display === "block") {
          dropdownContent.style.display = "none";
        } else {
          dropdownContent.style.display = "block";
        }
      });
    }
  </script>
  <script>
    // Function to close the dropdown menu
    function closeDropdown() {
      var dropdown = document.querySelector(".dropdown-container");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 2 dropdown menu
    function closeDropdown2() {
      var dropdown = document.querySelector(".container2");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 3 dropdown menu
    function closeDropdown3() {
      var dropdown = document.querySelector(".container3");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown5() {
      var dropdown = document.querySelector(".container5");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown7() {
      var dropdown = document.querySelector(".container7");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 9 dropdown menu
    function closeDropdown9() {
      var dropdown = document.querySelector(".container9");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 4 and chapter 6 dropdown menu
    function closeDropdown46() {
      var dropdown = document.querySelector(".container46");
      dropdown.style.display = "none";
    }

    // Function to close the chapter 8 dropdown menu
    function closeDropdown8() {
      var dropdown = document.querySelector(".container8");
      dropdown.style.display = "none";
    }

    // Function to close the Number Systems dropdown menu
    function closeDropdownNS() {
      var dropdown = document.querySelector(".containerNS");
      dropdown.style.display = "none";
    }
  </script>
  <script>
    // Check for saved dark mode preference
    if (localStorage.getItem('darkMode') === 'enabled') {
      document.body.classList.add('dark-mode');
      document.querySelector('.change').textContent = 'ON';
    }

    // Dark mode toggle functionality
    document.querySelector('.mode').addEventListener('click', function () {
      document.body.classList.toggle('dark-mode');

      // Get the change element
      const changeModeText = document.querySelector('.change');

      // Update the text based on dark mode status
      if (document.body.classList.contains('dark-mode')) {
        changeModeText.textContent = 'ON'; // Change the text to 'ON'
        localStorage.setItem('darkMode', 'enabled'); // Save preference
      } else {
        changeModeText.textContent = 'OFF'; // Change the text to 'OFF'
        localStorage.setItem('darkMode', 'disabled'); // Save preference
      }
    });

  </script>
  <script>
    function toggleMode() {
      const lightModeLink = document.getElementById('light-mode');
      const darkModeLink = document.getElementById('dark-mode');
      const modeText = document.querySelector('.change');

      if (lightModeLink.disabled) {
        lightModeLink.disabled = false;
        darkModeLink.disabled = true;
        modeText.textContent = "OFF"; // Update the mode text to show Dark mode is off
      } else {
        lightModeLink.disabled = true;
        darkModeLink.disabled = false;
        modeText.textContent = "ON"; // Update the mode text to show Dark mode is on
      }
    }

    // Call highlight.js to apply syntax highlighting
    document.addEventListener('DOMContentLoaded', () => {
      hljs.highlightAll();
    });
  </script>
</body>

</html>
