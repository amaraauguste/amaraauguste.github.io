<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <!-- Default CSS style for dark mode -->
  <link id="dark-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/base16/bright.css" />

  <!-- CSS style for light mode -->
  <link id="light-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/felipec.css" disabled />

  <link rel="stylesheet" href="../3620style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

  <script>
    hljs.highlightAll();
  </script>
  <title>CISC 3620</title>
</head>

<body>
  <nav id="navbar">
    <header class="header-link">
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" style="color: inherit; text-decoration: none;"
        class="header-text">CISC 3620</a>
    </header>
    <div class="mode">
      Dark mode:
      <span class="change" onclick="toggleMode()">OFF</span>
    </div>
    <hr />

    <ul>
      <!--Chapter 1 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">Introduction</a>
        <div class="dropdown-container">
          <a href="#What_is_Computer_Graphics?" onclick="closeDropdown()">What is Computer Graphics?</a>
          <a href="#Types_of_Computer_Graphics" onclick="closeDropdown()">Types of Computer Graphics</a>
          <a href="#Applications_of_Computer_Graphics" onclick="closeDropdown()">Applications of Computer Graphics</a>
          <a href="#The_Computer_Graphics_System" onclick="closeDropdown()">The Computer Graphics System</a>
        </div>
      </li>
      <!--Chapter 2 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">2D Computer Graphics</a>
        <div class="dropdown-container container2">
          <a href="#Types_of_2D_Graphics" onclick="closeDropdown2()">Types of 2D Graphics</a>
          <a href="#Pixels_and_Coordinate_Systems" onclick="closeDropdown2()">Pixels and Coordinate Systems</a>
          <a href="#Color_Models" onclick="closeDropdown2()">Color Models</a>
          <a href="#Shapes" onclick="closeDropdown2()">Shapes</a>
          <a href="#JavaScript" onclick="closeDropdown2()">Basic JavaScript</a>
          <a href="#Intro_to_HTML5_Canvas" onclick="closeDropdown2()">Intro to HTML5 Canvas</a>
          <a href="#Polygons_and_Curves" onclick="closeDropdown2()">Polygons and Curves</a>
          <a href="#Mouse_Events" onclick="closeDropdown2()">Mouse Events</a>
          <a href="#Additional_Events" onclick="closeDropdown2()">Additional Events</a>
          <a href="#Transforms" onclick="closeDropdown2()">Transforms</a>
        </div>
      </li>

      <li>
        <a class="dropdown-btn" style="font-size: 20px">3D Computer Graphics</a>
        <div class="dropdown-container container2">
          <a href="#Linear_Algebra" onclick="closeDropdown3()">Linear Algebra</a>
          <a href="#Intro_to_3D_Graphics" onclick="closeDropdown3()">Intro to 3D Graphics</a>
          <a href="#Graphic_APIs" onclick="closeDropdown3()">Graphic APIs</a>
          <a href="#Intro_to_Three.js" onclick="closeDropdown3()">Intro to Three.js</a>
          <a href="#Lights_and_Interactivity" onclick="closeDropdown3()">Lights and Interactivity</a>
          <a href="#Shadows" onclick="closeDropdown3()">Shadows</a>
          <a href="#Textures" onclick="closeDropdown3()">Textures</a>
          <a href="#A_More_Complex_Scene" onclick="closeDropdown3()">A More Complex Scene</a>
          <a href="#Custom_Geometry" onclick="closeDropdown3()">Custom Geometry</a>
          <a href="#Loading_Models" onclick="closeDropdown3()">Loading Models</a>
          <a href="#Global_Lighting_Models" onclick="closeDropdown3()">Global Lighting Models</a>
          <a href="#Particle_Effects" onclick="closeDropdown3()">Particle Effects</a>
          <a href="#Adding_Physics" onclick="closeDropdown3()">Adding Physics</a>
          <a href="#Cameras_and_Projections" onclick="closeDropdown3()">Cameras and Projections</a>
          <a href="#Shaders" onclick="closeDropdown3()">Shaders</a>
          <!--
          <a href="#Cameras_and_Projections" onclick="closeDropdown3()">Cameras and Projections</a>-->
          <!--<a href="#Linear_Algebra" onclick="closeDropdown2()">Linear Algebra</a>
          <a href="#Variable_Scope" onclick="closeDropdown2()">Variable Scope</a>
          <a href="#Global_Variables" onclick="closeDropdown2()">Global Variables</a>
          <a href="#Operators" onclick="closeDropdown2()">Operators</a>
          <a href="#Floating-Point_Numbers" onclick="closeDropdown2()">Floating-Point Numbers</a>
          <a href="#Math_Methods" onclick="closeDropdown2()">Math Methods</a>
          <a href="#Error_Types" onclick="closeDropdown2()">Error Types</a>-->
        </div>
      </li>

      <!--Chapter 3 Dropdown-->
      <!--<li>
        <a class="dropdown-btn" href="#Input_and_Output" style="font-size: 20px">Input and Output</a>
        <div class="dropdown-container container3">
          <a href="#System_Class" onclick="closeDropdown3()">System Class</a>
          <a href="#Data_Types" onclick="closeDropdown3()">Data Types</a>
          <a href="#Reading_Input" onclick="closeDropdown3()">Reading Input</a>
          <a href="#Literals_and_Constants" onclick="closeDropdown3()">Literals and Constants</a>
          <a href="#Putting_it_all_Together" onclick="closeDropdown3()">Putting it all Together</a>
          <a href="#Program_Structure" onclick="closeDropdown3()">Program Structure</a>
          <a href="#Using_Files" onclick="closeDropdown3()">Using Files</a>
        </div>
      </li>-->

      <li><a href="#Reference" style="font-size: 20px">Reference</a></li>
      <!-- add more links here -->
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" class="previous backbutton"
        style="font-size: 20px;">&laquo; Back</a>
      <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
    </ul>
  </nav>

  <main id="main-doc">
    <!-- WEEK 1 NOTES -->
    <!-- WEEK 1 DAY 1-->


    <section class="main-section" id="What_is_Computer_Graphics?">
      <br />
      <header><b>What is Computer Graphics?</b></header>
      <article>
        <p>The term "Computer Graphics" is concerned with all aspects of producing pictures or images using a computer.
        </p>
        <p>It encompasses the <b>creation</b>, <b>manipulation</b>, and <b>representation of images and animations</b>
          on computers.</p>
      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Types_of_Computer_Graphics">
      <br />
      <header><b>Types of Computer Graphics</b></header>
      <article>
        <p>Computer graphics can be broadly classified into two types: two-dimensional (2D) and three-dimensional (3D)
          graphics.</p>
        <p><b>2D Graphics:</b> are digital images that are computer-based. </p>
        <p>They include 2D geometric models, such as image compositions,
          pixel art, digital art, photographs, and text.</p>
        <p>2D graphics or computer generated images are used everyday on traditional printing and drawing. </p>
        <p><b>3D Graphics:</b> are graphics that use 3D representation of geometric data.</p>
        <p>This geometric data is then manipulated by computers via 3D computer graphics software in order to customize
          their display,
          movements, and appearance.</p>
        <p>3D computer graphics are often referred to as 3d models. A 3d model is a mathematical representation of
          geometric data that is contained in a data file. 3D models, can be used for real-time 3D viewing in
          animations, videos,
          movies, training, simulations, architectural visualizations or for display as 2D rendered images (2D renders)
        </p>
      </article>
      <br />
    </section>
    <section class="main-section" id="Applications_of_Computer_Graphics">
      <br />
      <header><b>Applications of Computer Graphics</b></header>
      <article>
        <p>The development of computer graphics has been driven both by the needs of the user
          community and by advances in hardware and software. The applications of computer
          graphics are many and varied; we can, however, divide them into <b>four</b> major areas:</p>
        <ol>
          <li>Display of information</li>
          <li>Design</li>
          <li>Simulation and animation</li>
          <li>User interfaces</li>
        </ol>
        <p>Although many applications span two or more of these areas, the development of the
          field was based on separate work in each.</p>
        <h2>1. Display of Information:</h2>
        <p>One of the most common uses of computer graphics is to display information in a
          pictorial or graphical form. This includes the generation of charts, graphs, and maps,
          as well as the visualization of scientific data. For example, medical imaging techniques
          such as MRI and CT scans use computer graphics to create detailed images of the human
          body.</p>
        <h2>2. Design:</h2>
        <p>Computer graphics is widely used in design and modeling applications, such as
          computer-aided design (CAD) for engineering and architectural design. It allows
          designers to create and manipulate 3D models of objects and structures, visualize
          designs from different angles, and simulate how they will look and function in the real
          world.</p>
        <h2>3. Simulation and Animation:</h2>
        <p>Computer graphics is also used to create realistic simulations and animations for
          various purposes, including entertainment, training, and scientific visualization. This
          includes the creation of 3D animations for movies and video games, as well as
          simulations for training pilots, surgeons, and other professionals.</p>
        <h2>4. User Interfaces:</h2>
        <p>Computer graphics plays a crucial role in the design of user interfaces for
          software applications. It allows developers to create visually appealing and intuitive
          interfaces that enhance the user experience. This includes the design of icons, buttons,
          menus, and other graphical elements that users interact with.</p>


      </article>
      <br />
    </section>
    <section class="main-section" id="The_Computer_Graphics_System">
      <br />
      <header><b>The Computer Graphics System</b></header>
      <article>
        <p>A computer graphics system is a computer system; as such, it must have all the
          components of a general-purpose computer system. There are six major elements in our system:</p>
        <ol>
          <li>Input devices</li>
          <li>Central Processing Unit</li>
          <li>Graphics Processing Unit</li>
          <li>Memory</li>
          <li>Frame buffer</li>
          <li>Output devices</li>
        </ol>

        <p>These components are shown in the figure below:</p>

        <img style="width: 100%; height: 100%" src="images/the%20computer%20graphics%20system.png?raw=true"
          alt="The computer graphics system">

        <p>This model is general enough to include workstations and personal computers,
          interactive game systems, mobile phones, GPS systems, and sophisticated imagegeneration systems.
          Although most of the components are present in a standard computer, it is the way each element is specialized
          for
          computer graphics that characterizes this diagram as a portrait of a graphics system.</p>

        <h2>Input Devices</h2>

        <p>Input devices are used to capture data (and images) from the real world and convert them into a form that can
          be processed by the computer.</p>

        <p>Most graphics systems provide a keyboard and at least one other input device. The
          most common input devices are the mouse, the joystick, and the data tablet. Each
          provides positional information to the system, and each usually is equipped with one
          or more buttons to provide signals to the processor. Often called pointing devices,
          these devices allow a user to indicate a particular location on the display.</p>

        <p>Modern systems, such as game consoles, provide a much richer set of input
          devices, with new devices appearing almost weekly. In addition, there are devices
          which provide three- (and more) dimensional input. Consequently, we want to provide a
          flexible model for incorporating the input from such devices into our graphics
          programs</p>

        <p>We can think about input devices in two distinct ways. The obvious one is to look
          at them as <b>physical devices</b>, such as a keyboard or a mouse, and to discuss how they
          work. Certainly, we need to know something about the physical properties of our input devices,
          so such a discussion is necessary if we are to obtain a full understanding
          of input. However, from the perspective of an application programmer, we should not
          need to know the details of a particular physical device to write an application program.</p>

        <p>Rather, we prefer to treat input devices as <b>logical devices</b> whose properties are
          specified in terms of what they do from the perspective of the application program. A
          logical device is characterized by its high-level interface with the user program rather
          than by its physical characteristics. </p>

        <p>Logical devices are familiar to all writers of highlevel programs.
          For example, data input and output in Java are done through classes
          such as System.out for output, PrintWriter for writing to files, and Scanner for input, whose methods use the
          standard Java data types. When we output a string using System.out.println or PrintWriter.println, the
          physical device
          on which the output appears could be a printer, a terminal, or a disk file. This output could even be the
          input to another
          program. The details of the format required by the destination device are of minor
          concern to the writer of the application program.</p>

        <p>In computer graphics, the use of logical devices is slightly more complex because
          the forms that input can take are more varied than the strings of bits or characters
          to which we are usually restricted in nongraphical applications. For example, we can
          use the mouse—a physical device—either to select a location on the screen of our
          CRT or to indicate which item in a menu we wish to select. In the first case, an x, y
          pair (in some coordinate system) is returned to the user program; in the second, the
          application program may receive an integer as the identifier of an entry in the menu.
          The separation of physical from logical devices allows us to use the same physical
          devices in multiple markedly different logical ways. It also allows the same program
          to work, without modification, if the mouse is replaced by another physical device,
          such as a data tablet or trackball.</p>

        <h3>Physical Input Devices</h3>

        <p>From the physical perspective, each input device has properties that make it more
          suitable for certain tasks than for others. We take the view used in most of the workstation
          literature that there are <b>two</b> primary types of physical devices: <b>pointing devices</b>
          and <b>keyboard devices</b></p>

        <p>The pointing device allows the user to indicate a position on
          the screen and almost always incorporates one or more buttons to allow the user to
          send signals or interrupts to the computer.</p>

        <p>The keyboard device is almost always a physical keyboard but can be generalized to include any device that
          returns character
          codes. We use the American Standard Code for Information Interchange (ASCII) in
          our examples. ASCII assigns a single unsigned byte to each character. Nothing we do
          restricts us to this particular choice, other than that ASCII is the prevailing code used.
          Note, however, that other codes, especially those used for Internet applications, use
          multiple bytes for each character, thus allowing for a much richer set of supported
          characters.</p>

        <p>The mouse and trackball are similar in use and often
          in construction as well. A typical mechanical mouse when turned over looks like a
          trackball. In both devices, the motion of the ball is converted to signals sent back to
          the computer by pairs of encoders inside the device that are turned by the motion of
          the ball. The encoders measure motion in two orthogonal directions</p>

        <p>There are many variants of these devices. Some use optical detectors rather than
          mechanical detectors to measure motion. Small trackballs are popular with portable
          computers because they can be incorporated directly into the keyboard. There are
          also various pressure-sensitive devices used in keyboards that perform similar functions
          to the mouse and trackball but that do not move; their encoders measure the
          pressure exerted on a small knob that often is located between two keys in the middle
          of the keyboard</p>

        <p>We can view the output of the mouse or trackball as two independent values
          provided by the device. These values can be considered as positions and converted—
          either within the graphics system or by the user program—to a two-dimensional
          location in a convenient coordinate system. If it is configured in this manner, we can
          use the device to position a marker (cursor) automatically on the display; however,
          we rarely use these devices in this direct manner.</p>

        <p>It is not necessary that the output of the mouse or trackball encoders be interpreted as a position.
          Instead, either the device driver or a user program can interpret
          the information from the encoder as two independent velocities. The computer can
          then integrate these values to obtain a two-dimensional position.</p>

        <p>Thus, as a mouse moves across a surface, the integrals of the velocities yield x, y
          values that can be converted to indicate the position for a cursor on the screen, as shown below:</p>

        <img style="width: 100%; height: 100%" src="images/cursor%20positioning.png?raw=true" alt="cursor positioning">

        <p>By interpreting the distance traveled by the ball as a velocity, we can use the device
          as a variable-sensitivity input device. Small deviations from rest cause slow or small
          changes; large deviations cause rapid large changes.</p>

        <p>With either device, if the ball does not rotate, then there is no change in the integrals and a cursor
          tracking the position of the mouse will not move. </p>

        <p>In this mode, these devices are <b>relative-positioning</b> devices because changes in the position of
          the ball yield a position in the user program; the absolute location of the ball (or the mouse) is not used by
          the application
          program.</p>

        <p>Relative positioning, as provided by a mouse or trackball, is <b>not always desirable</b>.</p>

        <p>In particular, these devices are not suitable for an operation such as tracing a diagram.
          If, while the user is attempting to follow a curve on the screen with a mouse, she
          lifts and moves the mouse, the absolute position on the curve being traced is lost.
        </p>

        <p><b>Data tablets</b> provide <b>absolute positioning</b>. A typical data tablet has rows
          and columns of wires embedded under its surface. The position of the stylus is
          determined through electromagnetic interactions between signals traveling through
          the wires and sensors in the stylus. Touch-sensitive transparent screens that can be
          placed over the face of a CRT have many of the same properties as the data tablet.
          Small, rectangular, pressure-sensitive touchpads are embedded in the keyboards of
          many portable computers. These touchpads can be configured as either relative- or
          absolute-positioning devices.</p>

        <h3>Logical Devices</h3>

        <p>Two major characteristics describe the logical behavior
          of an input device: (1) the measurements that the device returns to the user program
          and (2) the time when the device returns those measurements.</p>

        <p>The logical <b>string</b> device in Java is similar to using character input through Scanner or
          BufferedReader.
          A physical keyboard will return a string of characters to an application program; the same string might be
          provided from a file, or the user may see a virtual keyboard displayed on the output and use a pointing device
          to generate the
          string of characters. Logically, all three methods are examples of a string device, and application code for
          using such input can be
          the same regardless of which physical device is used.</p>

        <p>The physical pointing device can be used in a variety of logical ways. As a <b>locator</b>
          it can provide a position to the application in either a device-independent coordinate
          system, such as world coordinates, as in OpenGL, or in screen coordinates, which the
          application can then transform to another coordinate system. A logical <b>pick</b> device
          returns the identifier of an object on the display to the application program. It is
          usually implemented with the same physical device as a locator but has a separate
          software interface to the user program.</p>

        <p>A <b>widget</b> is a graphical interactive device, provided by either the window system
          or a toolkit. Typical widgets include menus, scrollbars, and graphical buttons. Most
          widgets are implemented as special types of windows. Widgets can be used to provide
          additional types of logical devices. For example, a menu provides one of a number of
          <b>choices</b> as may a row of graphical buttons. A logical <b>valuator</b> provides analog input
          to the user program, usually through a widget such as a slidebar, although the same
          logical input could be provided by a user typing numbers into a physical keyboard.
        </p>

        <h2>The CPU and The GPU</h2>
        <p>In a simple system, there may be only one processor, the central processing unit
          (CPU) of the system, which must do both the normal processing and the graphical processing.
          The main graphical function of the processor is to take specifications of graphical primitives (such as lines,
          circles, and polygons)
          generated by application programs and to assign values to the pixels in the frame buffer that best represent
          these entities.</p>

        <p> For example, a triangle is specified by its three vertices, but to display
          its outline by the three line segments connecting the vertices, the graphics system
          must generate a set of pixels that appear as line segments to the viewer.
          The conversion of geometric entities to pixel colors and locations in the frame buffer is known
          as <b>rasterization</b>, or <b>scan conversion</b>.</p>

        <p> In early graphics systems, the frame buffer was
          part of the standard memory that could be directly addressed by the CPU. Today,
          virtually all graphics systems are characterized by special-purpose <b>graphics processing
            units (GPUs)</b>, custom-tailored to carry out specific graphics functions. The GPU can
          be either on the mother board of the system or on a graphics card. The frame buffer
          is accessed through the graphics processing unit and usually is on the same circuit
          board as the GPU.</p>

        <p>GPUs have evolved to where they are as complex or even more complex than
          CPUs. They are characterized by both special-purpose modules geared toward graphical operations
          and a high degree of parallelism—recent GPUs contain over 100 processing units, each of which is user
          programmable. GPUs are so powerful that they can often be used as mini supercomputers for general purpose
          computing.</p>


        <h2>Output Devices</h2>
        <p>Until recently, the dominant type of display (or monitor) was the <b>cathode-ray tube
            (CRT)</b>. A simplified picture of a CRT is shown below:</p>

        <img style="width: 100%; height: 100%" src="images/the%20cathode%20ray%20tube.png?raw=true"
          alt="the cathode-ray tube">

        <p>When electrons strike the phosphor coating on the tube, light is emitted. The direction of the beam is
          controlled
          by two pairs of deflection plates. The output of the computer is converted, by digitalto-analog converters,
          to voltages across the x and y deflection plates. Light appears
          on the surface of the CRT when a sufficiently intense beam of electrons is directed at
          the phosphor.</p>

        <p>If the voltages steering the beam change at a constant rate, the beam will trace
          a straight line, visible to a viewer. Such a device is known as the <b>random-scan</b>,
          <b>calligraphic</b>, or <b>vector</b> CRT, because the beam can be moved directly from any
          position to any other position. If intensity of the beam is turned off, the beam can
          be moved to a new position without changing any visible display. This configuration
          was the basis of early graphics systems that predated the present raster technology
        </p>

        <p>A typical CRT will emit light for only a short time—usually, a few milliseconds—
          after the phosphor is excited by the electron beam. For a human to see a steady,
          flicker-free image on most CRT displays, the same path must be retraced, or <b>refreshed</b>,
          by the beam at a sufficiently high rate, the <b>refresh rate</b>. In older systems,
          the refresh rate is determined by the frequency of the power system, 60 cycles per
          second or 60 Hertz (Hz) in the United States and 50 Hz in much of the rest of the world.
          Modern displays are no longer coupled to these low frequencies and operate at rates
          up to about 85 Hz.</p>

        <p>In a raster system, the graphics system takes pixels from the frame buffer and
          displays them as points on the surface of the display in one of two fundamental
          ways.</p>

        <p>In a <b>noninterlaced</b> system, the pixels are displayed row by row, or scan line
          by scan line, at the refresh rate.</p>

        <p>In an <b>interlaced</b> display, odd rows and even rows are refreshed alternately.
          Interlaced displays are used in commercial television. In an interlaced display operating at 60 Hz,
          the screen is redrawn in its entirety only 30 times per second, although the visual system is tricked
          into thinking the refresh rate is 60 Hz rather than 30 Hz. Viewers located near the screen, however, can tell
          the difference between the interlaced and noninterlaced displays. Noninterlaced displays
          are becoming more widespread, even though these displays process pixels at twice the
          rate of the interlaced display</p>

        <p>Color CRTs have three different colored phosphors (red, green, and blue), arranged in small groups.
          One common style arranges the phosphors in triangular groups called <b>triads</b>, each triad consisting of
          three phosphors,
          one of each primary. Most color CRTs have three electron beams, corresponding to the three types of phosphors.
        </p>
        <p>In the shadow-mask CRT, a metal screen with small holes—the
          shadow mask—ensures that an electron beam excites only phosphors of the proper
          color:</p>

        <img style="width: 100%; height: 100%" src="images/shadowmask%20CRT.png?raw=true" alt="Shadowmask CRT">

        <p>Although CRTs are still common display devices, they are rapidly being replaced
          by flat-screen technologies. Flat-panel monitors are inherently raster based. Although
          there are multiple technologies available, including light-emitting diodes (LEDs),
          liquid-crystal displays (LCDs), and plasma panels, all use a two-dimensional grid
          to address individual light-emitting elements.</p>

        <p>The following shows a generic flat-panel monitor:</p>

        <img style="width: 100%; height: 100%" src="images/flat%20panel%20display.png?raw=true"
          alt="Flat panel display">

        <p>The two outside plates each contain parallel grids of wires that are oriented
          perpendicular to each other. By sending electrical signals to the proper wire in each
          grid, the electrical field at a location, determined by the intersection of two wires, can
          be made strong enough to control the corresponding element in the middle plate.
          The middle plate in an LED panel contains light-emitting diodes that can be turned
          on and off by the electrical signals sent to the grid. In an LCD display, the electrical
          field controls the polarization of the liquid crystals in the middle panel, thus turning
          on and off the light passing through the panel. A plasma panel uses the voltages on the
          grids to energize gases embedded between the glass panels holding the grids. The
          energized gas becomes a glowing plasma.</p>

        <p>Most projection systems are also raster devices. These systems use a variety of
          technologies, including CRTs and digital light projection (DLP). From a user perspective,
          they act as standard monitors with similar resolutions and precisions. Hard-copy
          devices, such as printers and plotters, are also raster based but cannot be refreshed.</p>


        <h2></h2>

      </article>
      <br />
    </section>

    <section class="main-section" id="Types_of_2D_Graphics">
      <br />
      <header><b>Types of 2D Graphics</b></header>
      <article>

        <p>There are two kinds of 2D computer graphics: raster graphics and vector graphics.</p>

        <h2>1. Raster Graphics</h2>

        <p><b>Raster graphics</b>, also known as <b>bitmap graphics</b>, are images that are made up of a grid of
          <b>pixels</b>.
        </p>

        <p>The pixels are small enough that they are not easy to see individually. In fact, for many very
          high-resolution displays, they
          become essentially invisible. Each pixel in the grid has a specific color value, and together they form the
          complete image.</p>

        <p>Modern screens typically use <b>24-bit color</b>, where each color is defined by three 8-bit numbers
          representing the levels of red, green, and blue. These three primary colors combine to create any color
          displayed on the
          screen. Such systems are known as <b>true-color</b>, <b>RGB-color</b>, or <b>full-color systems</b> because
          each
          pixel's color is determined by the combination of red, green, and blue values.</p>

        <p> Other formats are possible, such as <b>grayscale</b>, where each pixel is some shade of gray and the pixel
          color is given by one number that specifies the level of gray on a black-to-white scale. Typically, 256 shades
          of gray are used.</p>

        <p>Early computer screens used <b>indexed color</b>, where only a small set of colors, usually 16 or
          256, could be displayed. For an indexed color display, there is a numbered list of possible colors,
          and the color of a pixel is specified by an integer giving the position of the color in the list.</p>

        <p>In any case, the color values for all the pixels on the screen are stored in a large block of
          memory known as a <b>frame buffer</b>. Changing the image on the screen requires changing color
          values that are stored in the frame buffer. The screen is redrawn many times per second, so
          that almost immediately after the color values are changed in the frame buffer, the colors of
          the pixels on the screen will be changed to match, and the displayed image will change.</p>

        <p>In a very simple system, the frame buffer holds only the colored pixels that are
          displayed on the screen. In most systems, the frame buffer holds far more information,
          such as depth information needed for creating images from three-dimensional
          data. In these systems, the frame buffer comprises multiple buffers, one or more of
          which are color buffers that hold the colored pixels that are displayed. For now, we
          can use the terms frame buffer and color buffer synonymously without confusion.</p>

        <p>A computer screen used in this way is the basic model of <b>raster graphics</b>. The term
          "raster" technically refers to the mechanism used on older vacuum tube computer monitors:
          An electron beam would move along the rows of pixels, making them glow. The beam was
          moved across the screen by powerful magnets that would deflect the path of the electrons. The
          stronger the beam, the brighter the glow of the pixel, so the brightness of the pixels could be
          controlled by modulating the intensity of the electron beam. The color values stored in the
          frame buffer were used to determine the intensity of the electron beam. (For a color screen,
          each pixel had a red dot, a green dot, and a blue dot, which were separately illuminated by the
          beam.)</p>

        <p>Virtually all modern graphics systems are raster based. The image we see on the output device is an array—the
          <b>raster</b>—of
          picture elements, or pixels, produced by the graphics system.
        </p>

        <p>Raster graphics are best suited for representing complex images with many colors and gradients, such as
          photographs and detailed
          illustrations.</p>

        <h2>2. Vector Graphics</h2>

        <p>Although images on the computer screen are represented using pixels, specifying individual
          pixel colors is not always the best way to create an image. Another way is to specify the basic
          geometric objects that it contains, shapes such as lines, circles, triangles, and rectangles. This
          is the idea that defines <b>vector graphics</b>: Represent an image as a list of the geometric shapes
          that it contains.</p>

        <p>To make things more interesting, the shapes can have attributes, such as
          the thickness of a line or the color that fills a rectangle. Of course, not every image can be
          composed from simple geometric shapes. This approach certainly wouldn't work for a picture
          of a beautiful sunset (or for most any other photographic image). However, it works well for
          many types of images, such as architectural blueprints and scientific illustrations.</p>

        <p>In fact, early in the history of computing, vector graphics was even used directly on computer
          screens. When the first graphical computer displays were developed, raster displays were too
          slow and expensive to be practical. Fortunately, it was possible to use vacuum tube technology
          in another way: The electron beam could be made to directly draw a line on the screen, simply
          by sweeping the beam along that line. A vector graphics display would store a display list
          of lines that should appear on the screen. Since a point on the screen would glow only very
          briefly after being illuminated by the electron beam, the graphics display would go through the
          display list over and over, continually redrawing all the lines on the list. To change the image,
          it would only be necessary to change the contents of the display list. Of course, if the display
          list became too long, the image would start to flicker because a line would have a chance to
          visibly fade before its next turn to be redrawn</p>

        <p>But here is the point: For an image that can be specified as a reasonably small number of
          geometric shapes, the amount of information needed to represent the image is much smaller
          using a vector representation than using a raster representation. Consider an image made up
          of one thousand line segments. For a vector representation of the image, We only need to store
          the coordinates of two thousand points, the endpoints of the lines. <b>This would take up only a
            few kilobytes of memory. To store the image in a frame buffer for a raster display would require
            much more memory.</b> Similarly, a vector display could draw the lines on the screen more quickly
          than a raster display could copy the same image from the frame buffer to the screen. (As soon
          as raster displays became fast and inexpensive, however, they quickly displaced vector displays
          because of their ability to display all types of images reasonably well.)
        </p>

        <p>Unlike raster graphics, vector graphics are resolution-independent, meaning that they can be scaled to any
          size without losing quality.
          This is because instead of pixels, vector graphics use points, lines, and curves to represent elements.
          This allows for scalable graphics that can be resized without loss of quality. </p>

        <p>Vector graphics are best suited for representing simple images with solid colors and sharp edges, such as
          logos and icons, and
          widely used in graphic design, architectural design, and illustration industries.</p>

        <p>In summary, raster graphics are made up of pixels and are best suited for complex images with many colors,
          while vector graphics
          are made up of lines and curves and are best suited for simple images with solid colors.</p>

        <h2>So What's The Difference?</h2>

        <p>The divide between raster graphics and vector graphics persists in several areas of computer
          graphics.</p>

        <p> For example, it can be seen in a division between two categories of programs that
          can be used to create images: <b>painting programs</b> and <b>drawing programs</b></p>

        <h2>Painting Programs</h2>

        <p>In a painting program, the image is represented as a grid of pixels, and the user creates an image by
          assigning colors to pixels. This might be done by using a "drawing tool" that acts like a painter's brush,
          or even by tools that draw geometric shapes such as lines or rectangles. But the point in a
          painting program is to color the individual pixels, and it is only the pixel colors that are saved.
          To make this clearer, suppose that We use a painting program to draw a house, then draw a
          tree in front of the house. If We then erase the tree, We'll only reveal a blank background, not
          a house. In fact, the image never really contained a "house" at all—only individually colored
          pixels that the viewer might perceive as making up a picture of a house</p>

        <h2>Drawing Programs</h2>

        <p>In a drawing program, the user creates an image by adding geometric shapes, and the image
          is represented as a list of those shapes. If We place a house shape (or collection of shapes making
          up a house) in the image, and We then place a tree shape on top of the house, the house is
          still there, since it is stored in the list of shapes that the image contains. If We delete the tree,
          the house will still be in the image, just as it was before We added the tree. Furthermore, We
          should be able to select one of the shapes in the image and move it or change its size, so drawing
          programs offer a rich set of editing operations that are not possible in painting programs. (The
          reverse, however, is also true.)</p>

        <p>A practical program for image creation and editing might <b>combine elements of painting and
            drawing</b>, although one or the other is usually dominant.</p>

        <p>For example, a drawing program might allow the user to include a raster-type image, treating it as one shape.
          A painting program
          might let the user create “layers,” which are separate images that can be layered one on top of
          another to create the final image. The layers can then be manipulated much like the shapes in
          a drawing program (so that We could keep both our house and our tree in separate layers,
          even if in the image of the house is in back of the tree).</p>

        <p>Two well-known graphics programs are <b>Adobe Photoshop</b> and <b>Adobe Illustrator</b>. Photoshop
          is in the category of painting programs, while Illustrator is more of a drawing program. In
          the world of free software, the GNU image-processing program, Gimp, is a good alternative to
          Photoshop, while Inkscape is a reasonably capable free drawing program</p>

        <h2>File Formats</h2>

        <p>The divide between raster and vector graphics also appears in the field of graphics file
          formats. There are many ways to represent an image as data stored in a file. If the original
          image is to be recovered from the bits stored in the file, the representation must follow some
          exact, known specification.</p>
        <p>Such a specification is called a <b>graphics file format</b>.</p>
        <p>Some popular graphics file formats include GIF, PNG, JPEG, WebP, and SVG. Most images used on the
          Web are GIF, PNG, or JPEG, but most browsers also have support for SVG images and for
          the newer WebP format</p>

        <img class="center" style="width: 65%; height: 65%; top: 50%; left: 50%;"
          src="images/raster%20vs%20vector.png?raw=true" alt="raster vs vector">

        <p><b>GIF, PNG, JPEG, and WebP are raster graphics formats; an image is specified
            by storing a color value for each pixel.</b></p>

        <p><b>JPEG (Joint Photographic Experts Group)</b> allows <b>up to 16 million colors</b> and is <b>best for
            images with many
            colors or color gradations</b>, especially photographs. JPEG is a <b>"lossy" format, meaning each time the
            image is
            saved and compressed, some image information is lost, degrading quality</b>. JPEG images allow for various
          levels of compression.</p>
        <p>Low compression means high image quality, but large file size.
          High compression means lower image quality, but smaller file size.</p>

        <p><b>GIF (Graphics Interchange Format)</b> is a <b>"lossless"</b> format, meaning <b>image quality is not
            degraded through
            compression</b>. However, GIFs are <b>limited to a 256-color palette</b>, making them suitable for
          <b>simpler graphics with
            fewer colors</b>. GIFs also <b>support transparent backgrounds and simple animations</b>.
        </p>

        <p><b>PNG (Portable Network Graphics)</b> combines features of <b>both JPEG and GIF</b>. PNG <b>supports
            millions of colors and
            transparent backgrounds</b>. It uses <b>lossless compression, ensuring no quality loss</b>. However, PNGs
          may not be
          supported by older web browsers.</p>

        <p>WebP is a modern format that supports both lossless and lossy compression, providing a balance between image
          quality and file size.</p>

        <p>The amount of data necessary to represent a raster image can be quite large. However, the data usually
          contains a lot of redundancy and can be compressed to reduce its size. GIF and PNG use lossless compression,
          meaning the original image can be perfectly recovered. JPEG uses lossy compression, which allows for greater
          reduction in file size but at the cost of some image quality. WebP supports both types of compression.</p>


        <p><b>SVG, on the other hand, is fundamentally a vector graphics format (although SVG images
            can include raster images).</b> SVG is actually an XML-based language for describing twodimensional vector
          graphics images.</p>
        <p>"SVG" stands for "Scalable Vector Graphics" and the term "scalable" indicates one of the advantages of vector
          graphics: There is no loss of quality when the size of the image is increased. A line between two points can
          be
          represented at any scale, and it is still the same perfect geometric line. If We try to greatly increase the
          size of
          a raster image, on the other hand, We will find that We don't have enough color values for
          all the pixels in the new image; each pixel from the original image will be expanded to cover a
          rectangle of pixels in the scaled image, and We will get multi-pixel blocks of uniform color. The
          scalable nature of SVG images make them a good choice for web browsers and for graphical
          elements on our computer's desktop. And indeed, some desktop environments are now using
          SVG images for their desktop icons.
        </p>

        <p>A digital image, no matter what its format, is specified using a coordinate system. A
          coordinate system sets up a correspondence between numbers and geometric points. In two
          dimensions, each point is assigned a pair of numbers, which are called the coordinates of the
          point. The two coordinates of a point are often called its x -coordinate and y-coordinate,
          although the names "x" and "y" are arbitrary.</p>

        <p>A raster image is a two-dimensional grid of pixels arranged into rows and columns. As
          such, it has a natural coordinate system in which each pixel corresponds to a pair of integers
          giving the number of the row and the number of the column that contain the pixel. (Even in
          this simple case, there is some disagreement as to whether the rows should be numbered from
          top-to-bottom or from bottom-to-top.)</p>

        <p>For a vector image, it is natural to use real-number coordinates. The coordinate system for
          an image is arbitrary to some degree; that is, the same image can be specified using different
          coordinate systems.</p>

      </article>
      <br />
    </section>

    <section class="main-section" id="Pixels_and_Coordinate_Systems">
      <br />
      <header><b>Pixels and Coordinate Systems</b></header>
      <article>

        <p>As previously mentioned, most images viewed online are raster-based. Raster images are created with
          pixel-based
          software or captured with a camera or scanner. They are more common in general such as jpg, gif, png, and are
          widely
          used on the web.</p>

        <p>To create these two-dimensional images, each point in the image is assigned a color.</p>

        <p> A point in 2D can be identified by a pair of numerical coordinates. Colors can also be specified
          numerically. </p>

        <p>However, the assignment of numbers to points or colors is somewhat arbitrary.
          So we need to spend some time studying coordinate systems, which associate numbers to
          points, and color models, which associate numbers to colors.</p>

        <p>A digital image is made up of rows and columns of pixels. A pixel in such an image can be
          specified by saying which column and which row contains it. In terms of coordinates, a pixel
          can be identified by a pair of integers giving the column number and the row number.</p>

        <p> For example, the pixel with coordinates (3,5) would lie in column number 3 and row number 5.</p>

        <p>Conventionally, columns are numbered from left to right, starting with zero. Most graphics
          systems (like HTML Canvas), number rows from top to bottom starting from zero. </p>

        <p>Some, including OpenGL, number the rows from bottom to top instead.</p>

        <img style="width: 100%; height: 100%" src="images/pixel%20grids.png?raw=true" alt="Pixel grids">

        <p>Note in particular that the pixel that is identified by a pair of coordinates (x,y) depends on the
          choice of coordinate system. We always need to know what coordinate system is in use before
          We know what point We are talking about.</p>

        <p>Row and column numbers identify a pixel, not a point. A pixel contains many points;
          mathematically, it contains an infinite number of points. The goal of computer graphics is not
          really to color pixels—it is to create and manipulate images. In some ideal sense, an image
          should be defined by specifying a color for each point, not just for each pixel. Pixels are an
          approximation. If we imagine that there is a true, ideal image that we want to display, then
          any image that we display by coloring pixels is an approximation. This has many implications.</p>

        <p>Suppose, for example, that we want to draw a line segment. A mathematical line has no
          thickness and would be invisible. So we really want to draw a thick line segment, with some
          specified width.</p>
        <p>Let's say that the line should be one pixel wide.</p>
        <p>The problem is that, <b>unless
            the line is horizontal or vertical, we can't actually draw the line by coloring pixels</b>. A diagonal
          geometric line will cover some pixels only partially. It is not possible to make part of a pixel
          black and part of it white. When We try to draw a line with black and white pixels only,
          the result is a jagged staircase effect.</p>

        <p>This effect is an example of something called <b>"aliasing"</b>.</p>
        <p>Aliasing can also be seen in the outlines of characters drawn on the screen and in diagonal or
          curved boundaries between any two regions of different color. (The term aliasing likely comes
          from the fact that ideal images are naturally described in real-number coordinates. When We
          try to represent the image using pixels, many real-number coordinates will map to the same
          integer pixel coordinates; they can all be considered as different names or "aliases" for the same
          pixel.)
        </p>

        <h3>Anti-Aliasing</h3>

        <p>Anti-aliasing is a fundamental technique employed in graphics production that allows for smoother and more
          realistic images.
          This technology is used to reduce the jagged edges or "jaggies" that are commonly seen in computer-generated
          images,
          allowing them to appear as they would in real life.</p>
        <p>It was presented by the Architecture Machine Group team, which later became known as the Media Lab,
          a laboratory engaged in research and development in the field of technology, science, art, design, and
          medicine,
          in 1972 at the Massachusetts Institute of Technology.</p>
        <p>The idea is that when a pixel is only partially covered by a shape, the color of the pixel should be
          a mixture of the color of the shape and the color of the background. When drawing a black line
          on a white background, the color of a partially covered pixel would be gray, with the shade of
          gray depending on the fraction of the pixel that is covered by the line. (In practice, calculating
          this area exactly for each pixel would be too difficult, so some approximate method is used.)
        </p>
        <p>At its core, <b>anti-aliasing (also known as AA) is a method of manipulating pixels so that they appear
            smoother
            than they actually are</b>.
          To achieve this effect, the software or hardware being used will <b>sample adjacent pixels and create an
            average
            color value
            between them</b>. This helps the image appear more natural and realistic since it blends together sharp
          pixel
          lines into one
          continuous line instead of several distinct pixelated lines.</p>

        <p>So why does the "jagged" effect occur? Modern monitors and screens of mobile devices consist of quadrangular
          elements - pixels.
          This means that, in fact, only horizontal or vertical lines can be displayed in straight lines with clear
          boundaries.
          Angled curves are displayed as "steps". For example, the line in the picture below appears straight, but as
          We zoom in,
          it becomes clear that it is not.</p>

        <p>Here, for example, is a geometric line, shown on the left, along with two approximations of that
          line made by coloring pixels. The lines are greatly magnified so that We can see the individual
          pixels. The line on the right is drawn using anti-aliasing, while the one in the middle is not:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/antialiasing%201.png?raw=true"
          alt="Antialiasing 1">

        <p>Note that anti-aliasing does not give a perfect image, but it can reduce the "jaggies" that are
          caused by aliasing (at least when it is viewed on a normal scale).</p>

        <p>Anyone who has played older games is familiar with the distinctive pixelated and blocky aesthetic.
          "Jaggedness" occurs due to the lack of smooth transitions between colors, and anti-aliasing helps to mitigate
          this issue.</p>

        <p>Jagged edges, or aliasing, occur when real-world objects with smooth, continuous curves are rasterized using
          pixels.
          This problem arises from <b>undersampling</b>, which happens when the sampling frequency is lower than the
          <a
            href="https://www.gatan.com/nyquist-frequency#:~:text=The%20Nyquist%2DShannon%20sampling%20theorem,shown%20in%20the%20figures%20below.">Nyquist
            Sampling Frequency</a>,
          leading to a loss of information about the image.
        </p>

        <p>Anti-aliasing works by sampling multiple points within and around each pixel, then calculating an average
          color value.
          This process effectively blurs the edges of objects, creating the illusion of smoother lines and reducing
          visible pixelation.</p>

        <p>While anti-aliasing improves image quality, it also increases the load on the processor and graphics card,
          as they need to render additional shades and expend more power resources.</p>

        <p>One way to reduce jagged edges is to increase the resolution, as higher resolution images have smaller
          pixels,
          making the blocky appearance less noticeable. However, resolution alone is not always sufficient,
          and software developers use various anti-aliasing techniques to further improve image quality.</p>

        <h3>Methods of Anti-Aliasing (AA)</h3>

        <p>There are essentially four methods of Anti-Aliasing:</p>
        <ol>
          <li>High-Resolution Display</li>
          <li>Post-Filtering (Supersampling)</li>
          <li>Pre-Filtering (Area Sampling)</li>
          <li>Pixel Phasing</li>
        </ol>

        <h4>High-Resolution Display</h4>

        <p>Using a high-resolution display is one of the simplest methods of anti-aliasing.
          By increasing the resolution, more pixels can be used to represent the image, reducing the appearance of
          jagged edges.
          However, this method is limited by the physical resolution of the display and may not be practical for all
          applications.</p>

        <h4>Post-Filtering (Supersampling)</h4>

        <p>Post-filtering, also known as supersampling, involves treating the screen as if it has a finer grid,
          effectively reducing the pixel size.
          The average intensity of each pixel is calculated from the intensities of subpixels, and the image is
          displayed at the screen resolution.
          This method is called post-filtering because it is done after generating the rasterized image.</p>

        <h4>Pre-Filtering (Area Sampling)</h4>

        <p>Pre-filtering, or area sampling, calculates pixel intensities based on the areas of overlap between each
          pixel and the objects to be displayed.
          The final pixel color is an average of the colors of the overlapping areas. This method is called
          pre-filtering because it is done before generating the rasterized image.</p>

        <h4>Pixel Phasing</h4>

        <p>Pixel phasing involves shifting pixel positions to approximate the positions near object geometry.
          Some systems allow the size of individual pixels to be adjusted to distribute intensities, which helps in
          pixel phasing.</p>

        <h3>Types of Anti-Aliasing (AA)</h3>

        <p>Generally all anti-aliasing methods can be classified into two classifications:</p>
        <ol>
          <li>Spatial Anti-Aliasing</li>
          <li>Post Process Anti-Aliasing</li>
        </ol>

        <h4>1. Spatial Anti-Aliasing</h4>

        <p>Spatial anti-aliasing techniques work by sampling multiple points within each pixel and averaging the colors
          to reduce jagged edges.</p>

        <h4>Supersampling Anti-Aliasing (SSAA)</h4>

        <p><b>Supersampling Anti-Aliasing (SSAA)</b>, also called full-scene anti-aliasing (FSAA), works by <b>rendering
            the
            image at a higher resolution and then downsampling it to the display resolution</b>.
          This method reduces jagged edges by averaging colors near the edges.</p>
        <p> In this approach, a 512x512 image is first computed at higher resolution, such as 2048x2048, for example.
          It is then reduced through averaging or filtering to produce a 512x512 image.</p>
        <p>While effective, SSAA is <b>computationally
            intensive and can heavily load the GPU</b>.</p>

        <h4>Multi-Sample Anti-Aliasing (MSAA)</h4>

        <p><b>Multi-Sample Anti-Aliasing (MSAA)</b> improves performance compared to SSAA by <b>sampling multiple points
            within
            each pixel only at the edges of polygons</b>.</p>
        <p> Images are computed for 4 (or 8) subpixel sample points, followed by averaging. It is slow, since the frame
          rate is
          reduced by a factor of 4 (or 8). It works well for horizontal and vertical triangle edges.
          For other edge angles, the gaps between subpixels can cause narrow face breakups.</p>
        <p>This method <b>reduces the computational load while still providing good anti-aliasing quality</b>.</p>

        <h4>Coverage Sampling Anti-Aliasing (CSAA)</h4>

        <p><b>Coverage Sampling Anti-Aliasing (CSAA)</b> is an Nvidia-specific technique that improves upon MSAA by
          increasing
          the number of coverage samples
          without significantly increasing the number of color/depth samples. This method <b>provides better edge
            quality
            with less performance impact</b>.</p>

        <h4>2. Post-Processing Anti-Aliasing</h4>

        <p>Post-processing anti-aliasing techniques are applied after the image has been rendered to smooth out jagged
          edges.</p>

        <h4>Fast Approximate Anti-Aliasing (FXAA)</h4>

        <p>Fast Approximate Anti-Aliasing (FXAA) is a post-processing technique, created by Timothy Lottes at Nvidia,
          that smooths edges by <b>analyzing the final image and blending colors at the edges</b>.</p>
        <p>This is the <b>cheapest and simplest smoothing algorithm</b>.</p>
        <p>In layman's terms, FXAA is applied to our final rendered image and works based on pixel data, not geometry.
          GPU's are particularly fast at executing these shader algorithms in parallel, thus it's very quick to render.
        </p>
        <p>FXAA is less computationally intensive than SSAA and MSAA, making it <b>suitable for real-time applications
            like
            video games</b>.</p>

        <h4>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</h4>

        <p><b>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</b> is a logical development of the FXAA algorithm.
          This post effect is used in post-processing the final image that combines edge
          detection and blending to reduce aliasing.
          SMAA provides <b>high-quality anti-aliasing with a lower performance cost compared to SSAA and MSAA</b>.</p>

        <h4>Temporal Anti-Aliasing (TAA) </h4>

        <p>Temporal anti-aliasing techniques use information from previous frames to reduce aliasing in the current
          frame.</p>

        <p><b>Temporal Anti-Aliasing (TAA)</b> reduces aliasing by <b>using information from previous frames to smooth
            edges in
            the current frame.</b>
          TAA is effective at reducing flickering and shimmering in moving images, but it <b>can introduce ghosting
            artifacts</b> (visual distortions that appear in images due to a variety of factors, including movement,
          refraction, and sampling errors)
          if not implemented correctly.</p>






        <p>There are other issues involved in mapping real-number coordinates to pixels.</p>
        <p>For example,
          which point in a pixel should correspond to integer-valued coordinates such as (3,5)? The center
          of the pixel? One of the corners of the pixel? In general, we think of the numbers as referring
          to the top-left corner of the pixel.</p>
        <p>Another way of thinking about this is to say that integer coordinates refer to the lines between pixels,
          rather than to the pixels themselves. But that still doesn't determine exactly which pixels are affected when
          a geometric shape is drawn.</p>

        <p>For example, here are two lines drawn using HTML canvas graphics, shown greatly magnified. The
          lines were specified to be colored black with a one-pixel line width:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/antialiasing%202.png?raw=true"
          alt="Antialiasing 2">

        <p>The top line was drawn from the point (100,100) to the point (120,100).</p>

        <p>In canvas graphics, integer coordinates correspond to the lines between pixels, but when a one-pixel line is
          drawn,
          it extends one-half pixel on either side of the infinitely thin geometric line.</p>

        <p>So for the top line, the line as it is drawn lies half in one row of pixels and half in another row. The
          graphics
          system, which uses anti-aliasing, rendered the line by coloring both rows of pixels gray.</p>

        <p>The bottom line was drawn from the point (100.5,100.5) to (120.5,100.5). In this case, the line lies
          exactly along one line of pixels, which gets colored black. The gray pixels at the ends of the
          bottom line have to do with the fact that the line only extends halfway into the pixels at its
          endpoints. Other graphics systems might render the same lines differently</p>

        <p>All this is complicated further by the fact that pixels aren't what they used to be. Pixels
          today are smaller!</p>

        <h3>Understanding Resolution</h3>

        <p>The <b>resolution</b> of a display device can be measured in terms of the number
          of pixels per inch on the display, a quantity referred to as <b>PPI (pixels per inch)</b> or sometimes
          <b>DPI (dots per inch)</b>.
        </p>

        <h4>PPI vs DPI</h4>

        <p>While PPI (Pixels Per Inch) and DPI (Dots Per Inch) are often used interchangeably, they refer to different
          concepts and are used in different contexts.</p>

        <h4>Pixels Per Inch (PPI)</h4>

        <p>PPI is a measure of the <b>pixel density of a digital display</b>, such as a computer monitor, smartphone
          screen, or
          television. It indicates the number of pixels present in one inch of the display. <b>Higher PPI values mean
            more
            pixels are packed into each inch, resulting in sharper and more detailed images</b>.</p>

        <p>For example, a display with a resolution of 1920x1080 pixels and a diagonal size of 15.6 inches has a PPI of
          approximately 141. This means there are 141 pixels in each inch of the display.</p>

        <h4>Dots Per Inch (DPI)</h4>

        <p>DPI is a measure of the <b>resolution of a printed image</b>, indicating the number of individual dots of ink
          or
          toner that a printer can produce within one inch. <b>Higher DPI values result in finer detail and smoother
            gradients in printed images</b>.</p>

        <p>For example, a printer with a resolution of 300 DPI can produce 300 dots of ink per inch, resulting in
          high-quality prints suitable for photographs and detailed graphics.</p>

        <p>Both measures are important for ensuring high-quality visuals, but they apply to different
          mediums.</p>

        <p>Early screens tended to have resolutions of somewhere close to 72 PPI.
          At that resolution, and at a typical viewing distance, individual pixels are clearly visible. For a
          while, it seemed like most displays had about 100 pixels per inch, but high resolution displays
          today can have 200, 300 or even 400 pixels per inch. At the highest resolutions, individual
          pixels can no longer be distinguished.
        </p>

        <p>The fact that pixels come in such a range of sizes is a problem if we use coordinate systems
          based on pixels. An image created assuming that there are 100 pixels per inch will look tiny on a
          400 PPI display. A one-pixel-wide line looks good at 100 PPI, but at 400 PPI, a one-pixel-wide
          line is probably too thin</p>

        <p>In fact, in many graphics systems, "pixel" doesn't really refer to the size of a physical
          pixel. Instead, it is just <b>another unit of measure</b>, which is set by the system to be something
          appropriate. (On a desktop system, a pixel is usually about one one-hundredth of an inch. On
          a smart phone, which is usually viewed from a closer distance, the value might be closer to
          1/160 inch. Furthermore, the meaning of a pixel as a unit of measure can change when, for
          example, the user applies a magnification to a web page.)
        </p>

        <p>Pixels cause problems that have not been completely solved. Fortunately, they are less of a
          problem for vector graphics.</p>
        <p>For vector graphics, pixels only become an issue during rasterization, the step in which a vector image is
          converted into pixels for display. The vector image itself can be created using any convenient coordinate
          system. It represents an idealized, resolution-independent image.</p>
        <p>A rasterized image is an approximation of that ideal image, but how to do the approximation can be left to
          the display
          hardware.
        </p>

        <h2>Real-number Coordinate Systems</h2>

        <p>When doing 2D graphics, We are given a rectangle in which We want to draw some graphics
          primitives. Primitives are specified using some coordinate system on the rectangle. It should
          be possible to select a coordinate system that is appropriate for the application. For example, if
          the rectangle represents a floor plan for a 15 foot by 12 foot room, then We might want to use
          a coordinate system in which the unit of measure is one foot and the coordinates range from 0
          to 15 in the horizontal direction and 0 to 12 in the vertical direction. The unit of measure in
          this case is feet rather than pixels, and one foot can correspond to many pixels in the image.
          The coordinates for a pixel will, in general, be real numbers rather than integers. In fact, it's
          better to forget about pixels and just think about points in the image. A point will have a pair
          of coordinates given by real numbers.</p>

        <p>To specify the coordinate system on a rectangle, We just have to specify the horizontal
          coordinates for the left and right edges of the rectangle and the vertical coordinates for the top
          and bottom. Let's call these values left, right, top, and bottom. Often, they are thought of as
          xmin, xmax, ymin, and ymax, but there is no reason to assume that, for example, top is less
          than bottom. We might want a coordinate system in which the vertical coordinate increases
          from bottom to top instead of from top to bottom. In that case, top will correspond to the
          maximum y-value instead of the minimum value.</p>

        <p>To allow programmers to specify the coordinate system that they would like to use, it would
          be good to have a subroutine such as:</p>

        <p class="center"><b>setCoordinateSystem(left,right,bottom,top)</b></p>

        <p>The graphics system would then be responsible for automatically transforming the coordinates
          from the specified coordinate system into pixel coordinates. Such a subroutine might not be
          available, so it's useful to see how the transformation is done by hand. Let's consider the general
          case. Given coordinates for a point in one coordinate system, we want to find the coordinates
          for the same point in a second coordinate system. (Remember that a coordinate system is just
          a way of assigning numbers to points. It's the points that are real!)</p>

        <p>Suppose that the horizontal and vertical limits are oldLeft, oldRight, oldTop, and oldBottom for the first
          coordinate system,
          and are newLeft, newRight, newTop, and newBottom for the second. Suppose that a point
          has coordinates (oldX,oldY ) in the first coordinate system. We want to find the coordinates
          (newX,newY ) of the point in the second coordinate system</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="images/coordinates.png?raw=true" alt="coordinates">

        <p>Formulas for newX and newY are then given by: </p>

        <p class="center"><b>newX = newLeft + ((oldX - oldLeft) / (oldRight - oldLeft)) * (newRight - newLeft)</b></p>
        <p class="center"><b>newY = newTop + ((oldY - oldTop) / (oldBottom - oldTop)) * (newBottom - newTop)</b></p>

        <p>The logic here is that oldX is located at a certain fraction of the distance from oldLeft to
          oldRight. That fraction is given by:</p>

        <p class="center"><b>(oldX - oldLeft) / (oldRight - oldLeft)</b></p>

        <p>The formula for newX just says that newX should lie at the same fraction of the distance from
          newLeft to newRight. We can also check the formulas by testing that they work when oldX is
          equal to oldLeft or to oldRight, and when oldY is equal to oldBottom or to oldTop.</p>

        <p>As an example, suppose that we want to transform some real-number coordinate system
          with limits left, right, top, and bottom into pixel coordinates that range from 0 at left to 800 at
          the right and from 0 at the top 600 at the bottom. In that case, newLeft and newTop are zero,
          and the formulas become simply:</p>

        <p class="center"><b>newX = ((oldX - left) / (right - left)) * 800</b></p>
        <p class="center"><b>newY = ((oldY - top) / (bottom - top)) * 600</b></p>

        <p>Of course, this gives newX and newY as real numbers, and they will have to be rounded
          or truncated to integer values if we need integer coordinates for pixels. The reverse
          transformation—going from pixel coordinates to real number coordinates—is also useful.</p>

        <p>For example, if the image is displayed on a computer screen, and We want to react to mouse clicks
          on the image, We will probably get the mouse coordinates in terms of integer pixel coordinates,
          but We will want to transform those pixel coordinates into our own chosen coordinate system.</p>

        <p>In practice, though, We won't usually have to do the transformations Werself, since most
          graphics APIs provide some higher level way to specify transforms.</p>

        <h2>Aspect Ratio</h2>

        <p>The <b>aspect ratio</b> of a rectangle is the ratio of its width to its height. For example an aspect
          ratio of 2:1 means that a rectangle is twice as wide as it is tall, and an aspect ratio of 4:3 means
          that the width is 4/3 times the height. Although aspect ratios are often written in the form
          width:height, I will use the term to refer to the fraction width/height. A square has aspect ratio
          equal to 1. A rectangle with aspect ratio 5/4 and height 600 has a width equal to 600*(5/4),
          or 750.</p>

        <p>A coordinate system also has an aspect ratio. If the horizontal and vertical limits for the
          coordinate system are left, right, bottom, and top, as above, then the aspect ratio is the absolute
          value of:</p>

        <p class="center"><b>(right - left) / (top - bottom)</b></p>

        <p>If the coordinate system is used on a rectangle with the same aspect ratio, then when viewed in
          that rectangle, one unit in the horizontal direction will have the same apparent length as a unit
          in the vertical direction. If the aspect ratios don't match, then there will be some distortion.</p>

        <p>For example, the shape defined by the equation x^2 + y^2 = 9 should be a circle, but that will
          only be true if the aspect ratio of the (x,y) coordinate system matches the aspect ratio of the
          drawing area.</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="images/aspect%20ratio%201.png?raw=true" alt="aspect ratio 1">

        <p>It is not always a bad thing to use different units of length in the vertical and horizontal
          directions. However, suppose that We want to use coordinates with limits left, right, bottom,
          and top, and that We do want to preserve the aspect ratio.</p>

        <p>In that case, depending on the shape of the display rectangle, We might have to adjust the values either of
          left and right or
          of bottom and top to make the aspect ratios match:</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="images/aspect%20ratio%202.png?raw=true" alt="aspect ratio 2">

      </article>
      <br />
    </section>

    <section class="main-section" id="Color_Models">
      <br />
      <header><b>Color Models</b></header>
      <article>

        <p>We are talking about the most basic foundations of computer graphics. One of those is
          coordinate systems. The other is color.</p>

        <p>Red, Yellow, and Blue — Primary colors. Or at least, that's what we have been told since kindergarten, isn't
          it?
          But there is more to it.</p>

        <p>The colors on a computer screen are produced as combinations of <b>red, green, and blue light</b>.</p>

        <p>Now the question is — if RYB is the primary color set then why do computers use RGB instead?</p>

        <p>Going deep into the line, we first need to understand the color theory.</p>
        <p>There are two different theories: </p>
        <ol>
          <li>Additive</li>
          <li>Subtractive</li>
        </ol>

        <h2>Additive</h2>

        <p>Different colors are produced by varying the intensity of each type of light. A color can be
          specified by three numbers giving the intensity of red, green, and blue in the color. Intensity
          can be specified as a number in the range zero, for minimum intensity, to one, for maximum
          intensity.</p>

        <p>The additive is the case of the projection of one or more colored lights (wavelengths). These are the colors
          when mixed produce more light.</p>

        <p>This method of specifying color is called the <b>RGB color model</b>, where RGB stands
          for Red/Green/Blue.</p>


        <p>The red, green, and blue values for a color are called the color components of
          that color in the RGB color model and when mixed produces lighter colors, resulting in white light at the end.
          That's how our computer, TV, and other light-emitting screen works.</p>

        <p>Each parameter (red, green, and blue) defines the intensity of the color with a value <b>between 0 and
            255</b>.</p>

        <p>This means that there are 256 x 256 x 256 = 16777216 possible colors!</p>

        <p>For example, rgb(255, 0, 0) is displayed as red, because red is set to its highest value (255), and the other
          two (green and blue) are set to 0.</p>

        <p>Another example, rgb(0, 255, 0) is displayed as green, because green is set to its highest value (255), and
          the other two (red and blue) are set to 0.</p>

        <p>To display black, set all color parameters to 0, like this: rgb(0, 0, 0).</p>

        <p>To display white, set all color parameters to 255, like this: rgb(255, 255, 255).</p>

        <h3>Shades of Gray</h3>

        <p>Shades of gray are often defined using equal values for all three parameters:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/shades%20of%20gray.png?raw=true"
          alt="shades of gray">

        <p>Light is made up of waves with a variety of wavelengths. A pure color is one for which
          all the light has the same wavelength, but in general, a color can contain many wavelengths—
          mathematically, an infinite number. How then can we represent all colors by combining just
          red, green, and blue light? In fact, we can't quite do that.</p>

        <p>We might have heard that combinations of the three basic, or "primary" colors are sufficient
          to represent all colors, because the human eye has three kinds of color sensors that detect red,
          green, and blue light. However, that is only an approximation. The eye does contain three
          kinds of color sensors. The sensors are called "cone cells."</p>

        <p>However, cone cells do not respond exclusively to red, green, and blue light. Each kind of cone cell
          responds, to a varying degree,
          to wavelengths of light in a wide range. A given mix of wavelengths will stimulate each type
          of cell to a certain degree, and the intensity of stimulation determines the color that we see. A
          different mixture of wavelengths that stimulates each type of cone cell to the same extent will
          be perceived as the same color.</p>

        <p>So a perceived color can, in fact, be specified by three numbers
          giving the intensity of stimulation of the three types of cone cell. However, it is not possible
          to produce all possible patterns of stimulation by combining just three basic colors, no matter how those
          colors are chosen.
          This is just a fact about the way our eyes actually work; it might have been different. </p>

        <p>Three basic colors can produce a reasonably large fraction of the set of
          perceivable colors, but there are colors that We can see in the world that We will never see on
          our computer screen. (This whole discussion only applies to people who actually have three
          kinds of cone cell. Color blindness, where someone is missing one or more kinds of cone cell, is
          surprisingly common.)
        </p>

        <p>The range of colors that can be produced by a device such as a computer screen is called
          the <b>color gamut</b> of that device. Different computer screens can have different color gamuts,
          and the same RGB values can produce somewhat different colors on different screens. The color
          gamut of a color printer is noticeably different—and probably smaller—than the color gamut
          of a screen, which explains why a printed image probably doesn't look exactly the same as it
          did on the screen.
        </p>

        <h2>Subtractive</h2>

        <p>When we mix paints or inks, subtractive mixing results. Paints or inks are non-emissive objects here. They
          reflect when light falls on them.
          Molecules of paint absorb some of the wavelengths of light and reflect rest. That's how we see such objects.
        </p>

        <p>Printers, by the way, make colors differently from the way a screen does it.
          Whereas a screen combines light to make a color, a printer combines inks or dyes. Because of
          this difference, colors meant for printers are often expressed using a different set of basic colors.</p>

        <p>The primary colors of the subtractive mix are CMYK — Cyan, Magenta, Yellow, and K which stands for black ( To
          distinguish it from B for Blue.
          Just a convention.)</p>

        <p>When the CMY (not K) gets mixed, it produces brownish color — a bit muddy. To get the more blackish color,
          the additional K for black is used.
          CMYK — the model used by printers & publishing houses.</p>

        <img class="center" style="width: 75%; height: 75%; top: 50%; left: 50%;"
          src="images/additive%20and%20subtractive.png?raw=true" alt="additive and subtractive color">

        <p>In any case, the most common color model for computer graphics is RGB. RGB colors are
          most often represented using 8 bits per color component, a total of 24 bits to represent a color.
          This representation is sometimes called "24-bit color."" An 8-bit number can represent 28, or
          256, different values, which we can take to be the positive integers from 0 to 255. A color is
          then specified as a triple of integers (r,g,b) in that range.</p>

        <p>This representation works well because 256 shades of red, green, and blue are about as many
          as the eye can distinguish. In applications where images are processed by computing with color
          components, it is common to use additional bits per color component to avoid visual effects
          that might occur due to rounding errors in the computations. Such applications might use a
          16-bit integer or even a 32-bit floating point value for each color component. On the other
          hand, sometimes fewer bits are used.</p>

        <p> For example, one common color scheme uses 5 bits for
          the red and blue components and 6 bits for the green component, for a total of 16 bits for a
          color. (Green gets an extra bit because the eye is more sensitive to green light than to red or
          blue.) This “16-bit color” saves memory compared to 24-bit color and was more common when
          memory was more expensive.
        </p>

        <p>There are many other color models besides RGB. RGB is sometimes criticized as being
          unintuitive. For example, it's not obvious to most people that yellow is made of a combination
          of red and green.</p>

        <h2>Hues, Saturation, and Values (Lightness)</h2>

        <p> The closely related color models <b>HSV</b> and <b>HSL</b> describe the same set of
          colors as RGB, but attempt to do it in a more intuitive way. (HSV is sometimes called HSB,
          with the "B" standing for "brightness" HSV and HSB are exactly the same model.)
        </p>

        <p>The "H" in these models stands for <b>"hue", a basic spectral color</b>. As H increases, the color
          changes from red to yellow to green to cyan to blue to magenta, and then back to red. The
          value of H is often taken to range from 0 to 360, since the colors can be thought of as arranged
          around a circle with red at both 0 and 360 degrees.</p>

        <p>The "S" in HSV and HSL stands for <b>"saturation"</b> and is taken to <b>range from 0 to 1</b>. A
          saturation of 0 gives a shade of gray (the shade depending on the value of V or L). A saturation
          of 1 gives a "pure color" and decreasing the saturation is like adding more gray to the color.</p>

        <p>"V" stands for <b>"value"</b> and "L" stands for <b>"lightness"</b>. They determine how bright or dark the
          color is. The main difference is that in the HSV model, the pure spectral colors occur for V=1,
          while in HSL, they occur for L=0.5.
        </p>

        <img class="center" style="width: 50%; height: 50%; top: 50%; left: 50%;" src="images/HSV.png?raw=true"
          alt="HSV explained">

        <p>Let's look at some colors in the HSV color model. The illustration below shows colors with
          a full range of H-values, for S and V equal to 1 and to 0.5. Note that for S=V=1, We get
          bright, pure colors. S=0.5 gives paler, less saturated colors. V=0.5 gives darker colors.
        </p>

        <img style="width: 100%; height: 100%" src="images/HSV%20color%20model.png?raw=true" alt="HSV color model">

        <p> In the simple scale diagrams below, the first model indicates amount of black, white, or grey pigment added
          to the hue.
          The second model illustrates the same scale but explains the phenomenon based on light [spectral] properties.
        </p>

        <img class="center" style="width: 75%; height: 75%" src="images/pigment%20and%20light%20scale.png?raw=true"
          alt="pigment and light scale">

        <p>Regardless of the two Additive and Subtractive color models, all color is a result of how our eyes physically
          process light waves.
          So let's start with the light Additive model to see how it filters into the Subtractive model and to see how
          hues,
          values and saturation interact to produce unique colors.</p>

        <h3>Hues</h3>

        <p>The three primary hues in light are red, green, and blue. Thus, that is why televisions, computer monitors,
          and other full-range,
          electronic color visual displays use a triad of red, green, and blue phosphors to produce all electronically
          communicated color.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/hues%201.png?raw=true" alt="hues 1">

        <p>As we mentioned before, in light, all three of these wavelengths added together at full strength produces
          pure white light.
          The absence of all three of these colors produces complete darkness, or black.</p>

        <h3>Mixing Adjacent Primaries = Secondary Hues</h3>

        <h4>Making Cyan, Magenta, and Yellow</h4>

        <p>Although additive and subtractive color models are considered their own unique entities for screen vs. print
          purposes,
          the hues CMY do not exist in a vacuum.</p>

        <p>They are produced as secondary colors when RGB light hues are mixed, as follows:</p>

        <ul>
          <li>Green + Red light → Yellow</li>
          <li>Red + Blue light → Magenta</li>
          <li>Blue + Green light → Cyan</li>
        </ul>

        <img class="center" style="width: 50%; height: 50%" src="images/hues%202.png?raw=true" alt="hues 2">

        <h3>Overview of Hues</h3>

        <p>The colors on the outermost perimeter of the color circle are the "hues", which are colors in their purest
          form. This process can continue
          filling in colors around the wheel. The next level colors, the tertiary colors, are those colors between the
          secondary and primary colors.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/hues%203.png?raw=true" alt="hues 3">

        <h3>Saturation</h3>

        <p>Saturation is also referred to as "intensity" and "chroma". It refers to the dominance of hue in the color.
          On the outer edge of the hue wheel are the 'pure' hues.
          As We move into the center of the wheel, the hue we are using to describe the color dominates less and less.
          When We reach the center of the wheel, no hue dominates. These colors directly on the central axis are
          considered <b>desaturated</b>.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/saturation%201.png?raw=true" alt="saturation 1">

        <p>Naturally, the opposite of the image above is to saturate color.</p>
        <p>The first example below describes the general direction color must
          move on the color circle to become more saturated (towards the outside). The second example depicts how a
          single color looks completely
          saturated, having no other hues present in the color.</p>

        <img class="center" style="width: 100%; height: 100%" src="images/saturation%2002.png?raw=true"
          alt="saturation 2">


        <h3>Value</h3>

        <p>Now let's add "value" to the HSV scale. Value is the dimension of lightness/darkness. In terms of a spectral
          definition of color,
          value describes the overall intensity or strength of the light. If hue can be thought of as a dimension going
          around a wheel,
          then value is a linear axis running through the middle of the wheel, as seen below:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/value%201.png?raw=true" alt="value 1">

        <p>To better visualize even more, look at the example below showing a full color range for a single hue:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/value%202.png?raw=true" alt="value 2">

        <p>Now, if We imagine that each hue was also represented as a slice like the one above, we would have a solid,
          upside-down cone of colors.
          The example above can be considered a slice of the cone. Notice how the right-most edge of this cone slice
          shows the greatest amount of the
          dominant red hue (least amount of other competing hues), and how as We go down vertically, it gets darker in
          "value".</p>
        <p>Also notice that as we travel from right to left in the cone, the hue becomes less dominant and eventually
          becomes completely desaturated
          along the vertical center of the cone. This vertical center axis of complete desaturation is referred to as
          <b>grayscale</b>.
        </p>
        <p>See how this slice below translates into some isolated color swatches:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/value%203.png?raw=true" alt="value 3">


        <p>
          Often, a fourth component is added to color models. The fourth component is called <b>alpha</b>,
          and color models that use it are referred to by names such as <b>RGBA</b> and <b>HSLA</b>. Alpha is not a
          color as such. It is usually used to represent transparency.</p>

        <p>A color with maximal alpha value is
          fully opaque; that is, it is not at all transparent. A color with alpha equal to zero is completely
          transparent and therefore invisible. Intermediate values give translucent, or partly transparent, colors.
        </p>

        <p>Transparency determines what happens when We draw with one color (the foreground
          color) on top of another color (the background color). If the foreground color is fully opaque,
          it simply replaces the background color. If the foreground color is partly transparent, then it
          is blended with the background color.</p>

        <p>Assuming that the alpha component ranges from 0 to 1,
          the color that We get can be computed as: </p>

        <p class="center"><b>new_color = (alpha)*(foreground_color) + (1 - alpha)*(background_color)</b></p>

        <p>This computation is done separately for the red, blue, and green color components. This is
          called <b>alpha blending</b>. The effect is like viewing the background through colored glass; the
          color of the glass adds a tint to the background color. This type of blending is not the only
          possible use of the alpha component, but it is the most common.</p>

        <p>An RGBA color model with 8 bits per component uses a total of 32 bits to represent a color.
          This is a convenient number because integer values are often represented using 32-bit values. A
          32-bit integer value can be interpreted as a 32-bit RGBA color.</p>

        <p>How the color components are arranged within a 32-bit integer is somewhat arbitrary.</p>

        <p>The most common layout is to store the alpha component in the eight high-order bits,
          followed by red, green, and blue. (This should probably be called ARGB color.) However, other layouts are also
          in use.</p>

      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="Shapes">
      <br />
      <header><b>Shapes</b></header>
      <article>

        <p>We have been talking about low-level graphics concepts like pixels and coordinates, but
          fortunately we don't usually have to work on the lowest levels. Most graphics systems let us
          work with higher-level shapes, such as triangles and circles, rather than individual pixels.</p>

        <p>In a graphics API, there will be certain basic shapes that can be drawn with one command,
          whereas more complex shapes will require multiple commands. Exactly what qualifies as a
          basic shape varies from one API to another.</p>

        <p>For example, the HTML5 canvas API provides commands to draw rectangles, circles, and
          lines, but not triangles. To draw a triangle, We have to draw three lines.</p>

        <p>By "line", we really mean line segment, that is a straight line segment connecting two given
          points in the plane. <b>A simple one-pixel-wide line segment, without anti-aliasing, is the most
            basic shape</b>. It can be drawn by coloring pixels that lie along the infinitely thin geometric line
          segment.</p>

        <p>An algorithm for drawing the line has to decide exactly which pixels to color. One of
          the first computer graphics algorithms, Bresenham's algorithm for line drawing, implements
          a very efficient procedure for doing so.</p>

        <p>In any case, lines are typically more complicated. Anti-aliasing is one
          complication. Line width is another. A wide line might actually be drawn as a rectangle.</p>

        <p>Lines can have other attributes, or properties, that affect their appearance. One question
          is, what should happen at the end of a wide line?</p>

        <p>Appearance might be improved by adding
          a rounded "cap" on the ends of the line. A square cap—that is, extending the line by half of
          the line width—might also make sense.</p>

        <p>Another question is, when two lines meet as part of a
          larger shape, how should the lines be joined? And many graphics systems support lines that
          are patterns of dashes and dots.</p>

        <p>This illustration shows some of the possibilities:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/types%20of%20lines.png?raw=true"
          alt="Types of Lines">

        <p>On the left are three wide lines with no cap, a round cap, and a square cap. The geometric line
          segment is shown as a dotted line. (The no-cap style is called “butt.”) To the right are four
          lines with different patterns of dots and dashes. In the middle are three different styles of line
          joins: mitered, rounded, and beveled.</p>

        <p>The basic rectangular shape has sides that are vertical and horizontal. (A tilted rectangle
          generally has to be made by applying a rotation.) Such a rectangle can be specified with two
          points, (x1,y1) and (x2,y2), that give the endpoints of one of the diagonals of the rectangle.
          Alternatively, the width and the height can be given, along with a single base point, (x,y). In
          that case, the width and height have to be positive, or the rectangle is empty. The base point
          (x,y) will be the upper left corner of the rectangle if y increases from top to bottom, and it will
          be the lower left corner of the rectangle if y increases from bottom to top.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/types%20of%20rectangles.png?raw=true"
          alt="Types of Rectangles">

        <p>Suppose that We are given points (x1,y1) and (x2,y2), and that We want to draw the rectangle
          that they determine. And suppose that the only rectangle-drawing command that We have
          available is one that requires a point (x,y), a width, and a height. For that command, x must
          be the smaller of x1 and x2, and the width can be computed as the absolute value of x1 minus
          x2. And similarly for y and the height.</p>

        <p>In pseudocode,</p>

        <img class="center" style="width: 75%; height: 75%" src="images/rectangle%20pseudocode.png?raw=true"
          alt="Rectangle Pseudocode">

        <p>A common variation on rectangles is to allow rounded corners. For a “round rect,” the
          corners are replaced by elliptical arcs. The degree of rounding can be specified by giving the
          horizontal radius and vertical radius of the ellipse.</p>

        <p> Here are some examples of round rects. For
          the shape at the right, the two radii of the ellipse are shown:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/rounded%20rectangles.png?raw=true"
          alt="Rounded Rectangles">

        <p>Our final basic shape is the oval. (An oval is also called an ellipse.) An oval is a closed curve
          that has two radii. For a basic oval, we assume that the radii are vertical and horizontal. An
          oval with this property can be specified by giving the rectangle that just contains it. Or it can
          be specified by giving its center point and the lengths of its vertical radius and its horizontal
          radius. </p>

        <p>In this illustration, the oval on the left is shown with its containing rectangle and with
          its center point and radii:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/types%20of%20ovals.png?raw=true"
          alt="Types of Ovals">

        <p>The oval on the right is a circle. A circle is just an oval in which the two radii have the same
          length.</p>

        <p>If ovals are not available as basic shapes, they can be approximated by drawing a large
          number of line segments. The number of lines that is needed for a good approximation depends
          on the size of the oval. It's useful to know how to do this. Suppose that an oval has center
          point (x,y), horizontal radius r1, and vertical radius r2. Mathematically, the points on the oval
          are given by:</p>

        <text class="center"><b>( x + r1*cos(angle), y + r2*sin(angle) )</b></text>

        <p>where angle takes on values from 0 to 360 if angles are measured in degrees or from 0 to 2π if
          they are measured in radians. Here sin and cos are the standard sine and cosine functions. To
          get an approximation for an oval, we can use this formula to generate some number of points
          and then connect those points with line segments.</p>

        <p>In pseudocode, assuming that angles are
          measured in radians and that pi represents the mathematical constant π,</p>

        <img class="center" style="width: 100%; height: 100%" src="images/oval%20pseudocode.png?raw=true"
          alt="Oval Pseudocode">

        <p>For a circle, of course, We would just have r1 = r2. This is the first time we have used the
          sine and cosine functions, but it won't be the last. These functions play an important role in
          computer graphics because of their association with circles, circular motion, and rotation. We
          will meet them again when we talk about transforms later.</p>

        <h2>Stroke and Fill</h2>

        <p>There are two ways to make a shape visible in a drawing.</p>

        <p>We can <b>stroke</b> it. Or, if it is a closed
          shape such as a rectangle or an oval, We can <b>fill</b> it.</p>

        <p>Stroking a line is like dragging a pen along
          the line. Stroking a rectangle or oval is like dragging a pen along its boundary.</p>

        <p>Filling a shape
          means coloring all the points that are contained inside that shape.</p>

        <p>It's possible to both stroke
          and fill the same shape; in that case, the interior of the shape and the outline of the shape can
          have a different appearance.
        </p>

        <p>When a shape intersects itself, like the two shapes in the illustration below, it's not entirely
          clear what should count as the interior of the shape. In fact, there are at least two different
          rules for filling such a shape.</p>

        <p> In fact, there are at least two different
          rules for filling such a shape. Both are based on something called the <b>winding number</b>. The
          winding number of a shape about a point is, roughly, <b>how many times the shape winds around
            the point in the positive direction</b>, which we'll take here to be counterclockwise.</p>

        <p>Winding number
          can be negative when the winding is in the opposite direction.</p>

        <p>In the illustration, the shapes on
          the left are traced in the direction shown, and the winding number about each region is shown
          as a number inside the region.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/understanding%20winding%20number.png?raw=true"
          alt="Understanding Winding Number">

        <p>The shapes are also shown filled using the two fill rules.</p>

        <p>For the shapes in the center, the fill
          rule is to color any region that has a non-zero winding number.</p>

        <p>For the shapes shown on the
          right, the rule is to color any region whose winding number is odd; regions with even winding
          number are not filled.
        </p>

        <p>There is still the question of what a shape should be filled with. Of course, it can be filled
          with a color, but other types of fill are possible, including <b>patterns</b> and <b>gradients</b>.</p>

        <p> A pattern
          is an image, usually a small image. When used to fill a shape, a pattern can be repeated
          horizontally and vertically as necessary to cover the entire shape.</p>

        <p>A gradient is similar in that
          it is a way for color to vary from point to point, but instead of taking the colors from an image,
          they are computed. There are a lot of variations to the basic idea, but there is always a line
          segment along which the color varies. The color is specified at the endpoints of the line segment,
          and possibly at additional points; between those points, the color is interpolated. The color
          can also be extrapolated to other points on the line that contains the line segment but lying
          outside the line segment; this can be done either by repeating the pattern from the line segment
          or by simply extending the color from the nearest endpoint.</p>

        <p>For a <b>linear gradient</b>, the color
          is constant along lines perpendicular to the basic line segment, so we get lines of solid color
          going in that direction.</p>

        <p>In a radial gradient, the color is constant along circles centered at
          one of the endpoints of the line segment.</p>

        <p>And that doesn't exhaust the possibilities. To give
          we an idea what patterns and gradients can look like, here is a shape, filled with two gradients
          and two patterns:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/patterns%20and%20gradients.png?raw=true"
          alt="Patterns and Gradients">

        <p>The first shape is filled with a simple linear gradient defined by just two colors, while the second
          shape uses a radial gradient.</p>

        <p>Patterns and gradients are not necessarily restricted to filling shapes. Stroking a shape is,
          after all, the same as filling a band of pixels along the boundary of the shape, and that can be
          done with a gradient or a pattern, instead of with a solid color.</p>

        <p>Finally, a string of text can be considered to be a shape for the purpose
          of drawing it. The boundary of the shape is the outline of the characters. The text is drawn
          by filling that shape. </p>

        <p>In some graphics systems, it is also possible to stroke the outline of the
          shape that defines the text.</p>

        <p>In the following illustration, the string "Graphics" is shown, on
          top, filled with a pattern and, below that, filled with a gradient and stroked with solid black:</p>


        <img class="center" style="width: 75%; height: 75%" src="images/stroke%20and%20fill%20text.png?raw=true"
          alt="Stroke and Fill Text">







      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="JavaScript">
      <br />
      <header><b>JavaScript</b></header>
      <article>
        <p>JavaScript is a dynamic programming language that's used for web development, in web applications, for game
          development, and lots more. It allows we to implement dynamic features on web pages that cannot be done with
          only HTML and CSS.</p>

        <p>Many browsers use JavaScript as a scripting language for doing dynamic things on the web. Any time we see a
          click-to-show dropdown menu, extra content added to a page, and dynamically changing element colors on a page,
          to name a few features, we're seeing the effects of JavaScript.</p>

        <h2>How JavaScript Makes Things Dynamic</h2>
        <p>HTML defines the structure of our web document and the content therein. CSS declares various styles for the
          contents provided on the web document.</p>

        <p>HTML and CSS are often called markup languages rather than programming languages, because they, at their
          core, provide markups for documents with very little dynamism.</p>

        <p>JavaScript, on the other hand, is a dynamic programming language that supports Math calculations, allows we
          to dynamically add HTML contents to the DOM, creates dynamic style declarations, fetches contents from another
          website, and lots more.</p>

        <p>Here's a basic breakdown of JavaScript fundamentals:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/javascript%20cheat%20sheet.png?raw=true"
          alt="JavaScript Cheat Sheet" />


      </article>
      <br />
    </section>

    <hr />




    <section class="main-section" id="Intro_to_HTML5_Canvas">
      <br />
      <header><b>HTML5 Canvas</b></header>
      <article>
        <p>HTML5 (Hypertext Markup Language 5) is a markup language used for structuring and presenting hypertext
          documents on
          the World Wide Web. It was the <b>fifth and final major HTML version</b> that is now a retired World Wide Web
          Consortium (W3C)
          recommendation.
          The current specification is known as the <b>HTML Living Standard</b>.</p>

        <p>Canvas is a new element in HTML5, which provides APIs used to draw raster graphics on a web application.
          The presence of the Canvas API for HTML5, strengthens the HTML5 platform by providing two drawing contexts: 2D
          and 3D.
          These capabilities are supported on most modern operating systems and browsers.</p>

        <p>However, we will begin with 2D graphics.</p>

        <p>The <b>HTML5 canvas element</b> is used to draw graphics, on the fly, via JavaScript. The canvas element is
          only a container
          for graphics. We must use a <b>script</b> to actually draw the graphics.</p>

        <p>Canvas has several methods for drawing paths, boxes, circles, text, and adding images.</p>

        <p>HTML Canvas can:</p>

        <ul>
          <li><b>draw colorful text</b>, with or without animation</li>
          <li><b>draw graphics</b> using great features for graphical data presentation with an imagery of graphs and
            charts
          </li>
          <li><b>be animated</b> - everything is possible: from simple bouncing balls to complex animations</li>
          <li><b>be interactive</b> - canvas can respond to JavaScript events to any user action (key clicks, mouse
            clicks,
            button clicks, finger movement)</li>
          <li><b>be used in games</b> - canvas' methods for animations, offer a lot of possibilities for HTML gaming
            applications</li>
        </ul>

        <p>Here is an example of a canvas element:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;&lt;/canvas&gt;</code></pre>

        <ul>
          <li>The <b>id attribute is required</b> (so it can be referred to by JavaScript)</li>
          <li>The width and height attribute defines the size of the canvas (the default size of the canvas is 300px
            (width) x 150px (height))</li>
          <li>The canvas element requires the closing tag</li>
          <p>Unlike the &lt;img&gt; element, The &lt;canvas&gt; element requires the closing tag &lt;/canvas&gt;. Any
            content between the
            opening and closing tags is fallback content that will display only if the browser doesn't support the
            &lt;canvas&gt; element.</p>
          <p>For example:</p>
          <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;The browser doesn't support the canvas element&lt;/canvas&gt;</code></pre>
          <p>However, nowadays, most modern web browsers support the &lt;canvas&gt; element.</p>
          <li>We can have multiple &lt;canvas&gt; elements on one HTML page.</li>
          <li>By default, the &lt;canvas&gt; element has no border and no content.</li>
        </ul>

        <p>Dimensions of canvas element can either be set statically in HTML, or dynamically using JavaScript, or a
          combination of both.</p>

        <p>To add a border, use a style attribute:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <p>Canvas consists of a drawable region defined in HTML code with height and width attributes.
          JavaScript code may access the area through a full set of drawing functions, allowing for dynamically
          generated graphics.</p>

        <p>The drawing on the canvas is done with JavaScript.</p>

        <p>The canvas is initially blank. To display something, a script is needed to access the rendering context and
          draw on it.</p>

        <p>The following example draws a red rectangle on the canvas, from position (0,0) with a width of 150 and a
          height of 75:</p>

        <h3>Step 1: Find the Canvas Element</h3>

        <p>Initially, the canvas is blank. To draw something, We need to access the rendering context and use it to
          draw on the canvas.</p>

        <p>First, we need to find the &lt;canvas&gt; element.</p>

        <p>We access a &lt;canvas&gt; element with the HTML DOM method getElementById():</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");</code></pre>

        <p>The getElementById() method of the Document interface returns an Element object representing the element
          whose id property matches the specified string.</p>

        <p>To set the dimensions dynamically with JavaScript, we can access the width and height as follows: </p>

        <pre><code class="javascript">//To set width and height of current viewport
canvas.width = window.innerWidth; //or to set a specific width i.e 200
canvas.height = window.innerHeight; //or to set a specific height i.e 300</code></pre>

        <h3>Step 2: Create a Drawing Object</h3>

        <p>Secondly, we need a drawing object for the canvas.</p>

        <p>The getContext() method returns an object with tools (properties and methods) for drawing:</p>

        <pre><code class="javascript">const ctx = canvas.getContext("2d");</code></pre>

        <h3>Step 3: Draw on the Canvas</h3>

        <p>Finally, we can draw on the canvas.</p>

        <p>Set the fill-color to red with the fillStyle property:</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)";</code></pre>

        <p>The fillStyle property can be a color, a gradient, or a pattern. The default fillStyle is black.</p>

        <p>The fillRect(x, y, width, height) method draws the rectangle, filled with the fill style color, on the
          canvas:</p>

        <pre><code class="javascript">ctx.fillRect(0, 0, 150, 75);</code></pre>

        <h2>Canvas Fill and Stroke</h2>

        <p>To define fill-color and outline-color for shapes/objects in canvas, we use the following properties:</p>
        <ul>
          <li>fillStyle - Defines the color, gradient, or pattern used to fill shapes</li>
          <li>strokeStyle - Defines the color, gradient, or pattern used for strokes</li>
        </ul>

        <h3>The fillStyle Property</h3>

        <p>The fillStyle property defines the fill-color of the object.</p>
        <p>The fillStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
          
//How can we set the fill-color to blue? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>The strokeStyle Property</h3>

        <p>The strokeStyle property defines the color of the outline.</p>

        <p>The strokeStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
                      
//How can we set the stroke-color to yellow? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>Combining fillStyle and strokeStyle</h3>

        <p>It is perfectly legal to combine the previous two rectangles: </p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// the filled rectangle
ctx.fillStyle = "rgb(0 0 255)";
ctx.fillRect(10,10, 100,100);

// the outline rectangle
ctx.strokeStyle = "rgb(255 255 0)";
ctx.strokeRect(10,10, 100,100);</code></pre>

        <h3>Gradients</h3>

        <p>Gradients let us display smooth transitions between two or more specified colors.</p>

        <p>Gradients can be used to fill rectangles, circles, lines, text, etc.</p>

        <p>There are two methods used for creating gradients:</p>

        <ul>
          <li>createLinearGradient() - creates a linear gradient</li>
          <li>createRadialGradient() - creates a radial/circular gradient</li>
        </ul>

        <h3>Linear Gradient</h3>

        <p>The createLinearGradient() method is used to define a linear gradient.</p>

        <p>A linear gradient changes color along a linear pattern (horizontally/vertically/diagonally).</p>

        <p>The createLinearGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting point</li>
          <li>y0 - The y-coordinate of the starting point</li>
          <li>x1 - The x-coordinate of the ending point</li>
          <li>y1 - The y-coordinate of the ending point</li>
        </ul>

        <p>The gradient object requires two or more color stops.</p>

        <p>The addColorStop() method specifies the color stops, and its position along the gradient. The positions can
          be anywhere between 0 and 1.</p>

        <p>To use the gradient, assign it to the fillStyle or strokeStyle property, then draw the shape (rectangle,
          circle, shape, or text).</p>

        <p>Here is an example of a linear gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create gradient
const grd = ctx.createLinearGradient(0, 0, 200, 0);
grd.addColorStop(0, "red");
grd.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd;
ctx.fillRect(10, 10, 150, 80);</code></pre>

        <h3>Radial Gradient</h3>

        <p>The createRadialGradient() method is used to create a radial/circular gradient.</p>

        <p>A radial gradient is defined by two circles, one smaller and one larger.</p>

        <p>The createRadialGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting circle</li>
          <li>y0 - The y-coordinate of the starting circle</li>
          <li>r0 - The radius of the starting circle</li>
          <li>x1 - The x-coordinate of the ending circle</li>
          <li>y1 - The y-coordinate of the ending circle</li>
          <li>r1 - The radius of the ending circle</li>
        </ul>

        <p>Here is an example of a radial gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");

const ctx = canvas.getContext("2d");

// Create gradient
const grd2 = ctx.createRadialGradient(85, 140, 0, 85, 140, 100);

// Add color stops
grd2.addColorStop(0, "red");
grd2.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd2;
ctx.fillRect(10, 100, 150, 80);</code></pre>

        <h3>Patterns</h3>

        <p>Patterns are used to fill shapes with images (instead of colors).</p>

        <p>There are two methods used for creating patterns:</p>

        <ul>
          <li>createPattern(image, type) - creates a pattern from an image</li>
          <li>createPattern(canvas, type) - creates a pattern from another canvas</li>
        </ul>

        <p>The createPattern() method has the following parameters:</p>

        <ul>
          <li>image - Specifies the image to use</li>
          <li>type - Repeat the pattern (repeat, repeat-x, repeat-y, no-repeat)</li>
        </ul>

        <p>Here is an example of a pattern:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create a pattern
const img = new Image();
img.src = "https://www.w3schools.com/tags/img_the_scream.jpg";
img.onload = function() {
  const pat = ctx.createPattern(img, "repeat");
  ctx.fillStyle = pat;
  ctx.fillRect(10, 10, 150, 80);
};</code></pre>

        <h3>The clearRect() Method</h3>

        <p>The clearRect() method is used to clear a rectangular area of the canvas. The cleared rectangle is
          transparent.</p>

        <p>The clearRect() method has the following parameters:</p>

        <ul>
          <li>x - The x-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>y - The y-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>width - The width of the rectangle to clear (in pixels)</li>
          <li>height - The height of the rectangle to clear (in pixels)</li>
        </ul>


        <p>Here we use fillRect() to draw a filled 150*100 pixels rectangle, starting in position (10,10). Then use
          clearRect() to clear a rectangular area in the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "pink";
ctx.fillRect(10,10,150,100);
            
ctx.clearRect(60,35,50,50);</code></pre>

        <p>And here is an example of clearing the entire canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas"); 
const ctx = canvas.getContext("2d");

// Clear the canvas
ctx.clearRect(0, 0, canvas.width, canvas.height);</code></pre>



        <p></p>

        <h2>Canvas Coordinates</h2>

        <p>As previously mentioned, The HTML canvas is a two-dimensional grid.</p>


        <p>It is important to understand the coordinate space of canvas, if We want elements to be positioned as
          desired. Top left of canvas represents (0,0) or origin coordinate.</p>

        <p>All the elements on canvas are placed with reference to this origin.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/coordinate%20grid%20space.png?raw=true"
          alt="The Grid or Coordinate Space 1">

        <p>1 point on grid is roughly equivalent to 1px.</p>

        <p>At the example above further elaborates: we have red border around our canvas and we have drawn a rectangle
          of 100px width and height with a stroke of blue.</p>

        <p>This can be achieved by the following code: </p>

        <h3>HTML</h3>

        <pre><code class="html">&lt;canvas id="myCanvas" width="320" height="320" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <h3>JavaScript</h3>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(0, 0, 100, 100); //draws a rectangle with a blue stroke, starting at (0,0) with a width and height of 100 pixels</code></pre>

        <p>Providing x and y coordinates would translate the element relative to the canvas' origin coordinates.</p>
        <p>As shown in the image below, our rectangle has moved 20 pixels to the right and bottom as we have provided
          the values of x and y as 20.</p>


        <img class="center" style="width: 50%; height: 50%" src="images/coordinate%20grid%20space%202.png?raw=true"
          alt="The Grid or Coordinate Space 2">

        <h3>Let's Look and Real Time Coordinates</h3>

        <p class="codepen" data-height="600" data-theme-id="dark" data-default-tab="result" data-slug-hash="emOoLbp"
          data-pen-title="Understanding Coordinates" data-user="amaraauguste"
          style="height: 300px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;">
          <span>See the Pen <a href="https://codepen.io/amaraauguste/pen/emOoLbp">
              Understanding Coordinates</a> by Amara (<a href="https://codepen.io/amaraauguste">@amaraauguste</a>)
            on <a href="https://codepen.io">CodePen</a>.</span>
        </p>
        <script async src="https://public.codepenassets.com/embed/index.js"></script>

        <h2>Shapes</h2>

        <p>It is easy to draw basic shapes like rectangle, triangle, square, circle, polygon or a just a simple line
          between two points. But by default Canvas provides a method only to draw rectangle.</p>

        <p>However, rest of shapes can be created by joining points using path API, and a combination of line and arc
          APIs.</p>

        <h3>Rectangle</h3>

        <p>The three most used methods for drawing rectangles in canvas are:</p>

        <ul>
          <li>The rect(x, y, width, height) method</li>
          <li>The fillRect(x, y, width, height) method</li>
          <li>The strokeRect(x, y, width, height) method</li>
        </ul>

        <p>The rect() method defines a rectangle. Note: the rect() method does not draw the rectangle (it just defines
          it). So, in addition, We have to use the stroke() method (or the fill() method) to actually draw it.</p>


        <p>fill and stroke are ink methods and each case it means to draw a rectangle with a filled color, or to draw a
          rectangular outline of a color. The default color is black.</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)"; //red color for fill
ctx.fillRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a red rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <p>Similarly, we can draw a rectangle with a stroke:</p>

        <pre><code class="javascript">ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a blue rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <h3>Circle</h3>

        <p>As we mentioned earlier there is no straight forward method to create a circle, but we can use a combination
          of path APIs and arc method to draw our circle. Let's understand a little more about path:</p>

        <p>"A path is list of points connected to form different shapes."</p>

        <p>This means a path can be formed between two given points on screen. It can be a straight line or curved arc
          or can be any shape or color.</p>

        <p>There are three steps to follow to create a shape using path:</p>

        <ol>
          <li>Invoke beingPath() method on context a create a new path. Once a path is created all future commands to
            draw are applied on
            this path.</li>
          <li>Next create a path using drawing methods, like lineTo, moveTo, arc, rect, etc.
            <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D#paths">Refer MDN</a> for
            list of all
            available methods that used with path.
          </li>
          <li>Once path has been created it needs to actually be rendered on canvas; we can do that using ink methods
            fill and stroke.</li>
        </ol>

        <p>Let's draw a circle:</p>

        <p>We have to use arc(x, y, radius, startAngle, endAngle) method on context to draw our circle.</p>

        <p> If we try to recollect basic geometry, to draw a circle using a protractor we need a radius, and a start &
          end angle. A semi-circle starts at angle 0 and ends at 180 degree or PI radians. So a full-circle extends
          further and just ends at 2*PI or 360 degrees.</p>

        <p>This exact concept can be used to draw a circle using arc method.</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, 2 * Math.PI);
ctx.stroke();</code></pre>

        <p>Here, we have drawn a circle with a center at (100, 75) and a radius of 50 pixels.</p>

        <h3>Draw a Half Circle</h3>

        <p>To draw a half circle, we change the endAngle to PI (not 2 * PI):</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, Math.PI);
ctx.fill();</code></pre>

        <h3>More About the Angles of an Arc</h3>

        <p>The following image shows some of the angles in an arc:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/angles%20of%20an%20arc.png?raw=true"
          alt="Angles of an Arc">

        <ul>
          <li>Center: arc(<text style="color: rgb(0, 223, 0)">100, 75</text>, 50, 0 * Math.PI, 1.5 * Math.PI)</li>
          <li>Start angle: arc(100, 75, 50, <text style="color: red">0</text>, 1.5 * Math.PI)</li>
          <li>End angle: arc(100, 75, 50, 0 * Math.PI, <text style="color: blue">1.5 * Math.PI</text>)</li>
          <li>Circle : arc(100, 75, 50, 0, 2 * Math.PI)</li>
        </ul>

        <h3>Line</h3>

        <p>To draw a line between two points we use moveTo(x, y)and lineTo(x, y)methods.</p>

        <p>If we consider two points A & B with x and y coordinates respectively,
          then moveTo acts as a position of A on canvas, while lineTo as position of point B.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line
ctx.stroke();</code></pre>

        <h3>The lineWidth Property</h3>

        <p>The lineWidth property defines the width of the line.</p>

        <p>It must be set before calling the stroke() method.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line 
ctx.lineWidth = 10; //width of line
ctx.stroke();</code></pre>


        <h3>Triangle</h3>

        <p>A triangle is simply three lines connected together.</p>

        <p>So, to draw a triangle we can use lineTo method to connect three points.</p>

        <p>We are going to use a special path method called as closePath() to complete our triangle. closePath basically
          adds a straight line from end coordinate to the start coordinate inside a path.</p>

        <p>If we assume a triangle is made of three points A, B & C, then we can draw our triangle like:</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(235, 114); // Point A
ctx.lineTo(135, 349); // Point B
ctx.lineTo(335, 349); // Point C
            
ctx.closePath(); // Join C & A
ctx.strokeStyle = "rgb(31 24 88)";
ctx.stroke();</code></pre>

        <h3>To Summarize Basic Shape Drawing</h3>

        <p>Apart from drawing specific rectangles and circles, drawing must be broken down into four distinct steps:</p>

        <ol>
          <li>ctx.beginPath(), to let the computer know We're beginning a new line/path</li>
          <li>ctx.moveTo(x, y), to move the 'cursor' to a specific point on the canvas without 'drawing' anything or
            recording any path</li>
          <li>ctx.lineTo(x, y), tells the computer to record a path from the current context position, in this case the
            point described by the ctx.moveTo(x, y) function—to the new coordinates provided</li>
          <li>ctx.stroke(), to then fill the described path. This is the step that actually 'draws' something onto the
            canvas</li>
        </ol>

        <p>In essence, We move the cursor to a starting position, tell the computer We're about to draw, record a path
          to a declared location, and then finally fill that path in.</p>

        <h2>Drawing Text</h2>

        <p>To draw text on the canvas, the most important property and methods are:</p>

        <ul>
          <li>font - defines the font properties for the text</li>
          <li>fillText(text, x, y) - fills a given text at the given (x, y) position</li>
          <li>strokeText(text, x, y) - strokes a given text at the given (x, y) position</li>
        </ul>

        <p>The font property defines the font to be used and the size of the font. The default value for this property
          is "10px sans serif".</p>

        <p>Both methods include an optional fourth parameter: maxwidth, which represents the maximum width of the
          text-string.</p>

        <p>Here is an example of drawing text on a canvas:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.font = "30px Arial";
ctx.fillText("Hello World", 10, 50);</code></pre>

        <p>The fillText() method draws filled text on the canvas. The default color of the text is black.</p>

        <p>The strokeText() method draws text on the canvas (no fill). The default color of the text is black.</p>


        <p><a href="https://codepen.io/amaraauguste/pen/KwPYByQ" target="_blank">Let's take a look at some example
            code</a></p>

        <hr />

        <h2>Exercise: Smiley Face</h2>

        <p>Knowing what we know now about drawing shapes in Canvas</p>

        <p>and given a canvas sized 350x350 ... </p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="350" height="350" style="border:1px solid rgb(0,0,0);"&gt;&lt;/canvas&gt;</code></pre>


        <p>How can we can create this smiley face in the center of the canvas?</p>



        <img class="center" style="width: 50%; height: 50%" src="images/smiley%20face%20canvas%20example.png?raw=true"
          alt="Smiley Face">


      </article>
      <br />
    </section>


    <hr />
    <section class="main-section" id="Polygons_and_Curves">
      <br />
      <header><b>Polygons and Curves</b></header>
      <article>
        <p>It is impossible for a graphics API to include every possible shape as a basic shape, but there is
          usually some way to create more complex shapes.</p>

        <p>For example, consider polygons. A <b>polygon</b>
          is a closed shape consisting of a sequence of line segments.</p>

        <p>Each line segment is joined to the
          next at its endpoint, and the last line segment connects back to the first. The endpoints are
          called the vertices of the polygon, and a polygon can be defined by listing its vertices.</p>

        <p>In a <b>regular polygon</b>, all the sides are the same length and all the angles between sides are
          equal. Squares and equilateral triangles are examples of regular polygons.</p>

        <p> A <b>convex polygon</b>
          has the property that whenever two points are inside or on the polygon, then the entire line
          segment between those points is also inside or on the polygon. Intuitively, a convex polygon
          has no "indentations" along its boundary. (Concavity can be a property of any shape, not just
          of polygons.)
        </p>

        <img class="center" style="width: 100%; height: 100%" src="images/convex%20polygons.png?raw=true"
          alt="Convex Polygons">


        <p>Sometimes, polygons are required to be "simple", meaning that the polygon has no selfintersections. That is,
          all the vertices are different, and a side can only intersect another
          side at its endpoints.</p>

        <p>And polygons are usually required to be "planar", meaning that all the
          vertices lie in the same plane. (Of course, in 2D graphics, everything lies in the same plane, so
          this is not an issue. However, it does become an issue in 3D.)
        </p>


        <p>How then should we draw polygons? That is, what capabilities would we like to have in a
          graphics API for drawing them</p>

        <p>One possibility is to have commands for stroking and for filling
          polygons, where the vertices of the polygon are given as an array of points or as an array of
          x-coordinates plus an array of y-coordinates.</p>

        <p>In fact, that is sometimes done; for example, the
          Java graphics API includes such commands. </p>

        <p>Another, more flexible, approach is to introduce
          the idea of a "path."</p>

        <p>Java, SVG, and the HTML canvas API all support this idea. A path is
          a general shape that can include both line segments and curved segments. Segments can, but
          don't have to be, connected to other segments at their endpoints. </p>

        <p>A path is created by giving a series of commands that tell, essentially, how a pen would be moved to draw the
          path.</p>

        <p>While
          a path is being created, there is a point that represents the pen’s current location. There will
          be a command for moving the pen without drawing, and commands for drawing various kinds
          of segments</p>

        <p>For drawing polygons, we need commands such as: </p>

        <ul>
          <li><b>beginPath()</b> — start a new, empty path</li>
          <li><b>moveTo(x,y)</b> — move the pen to the point (x,y), without adding a segment to the path;
            that is, without drawing anything</li>
          <li><b>lineTo(x,y)</b> — add a line segment to the path that starts at the current pen location
            and ends at the point (x,y), and move the pen to (x,y)</li>
          <li><b>closePath()</b> — add a line segment from the current pen location back to the starting
            point, unless the pen is already there, producing a closed path.</li>
        </ul>

        <p>(For closePath, I need to define “starting point.” A path can be made up of “subpaths” A
          subpath consists of a series of connected segments. A moveTo always starts a new subpath.
          A closePath ends the current segment and implicitly starts a new one. So “starting point”
          means the position of the pen after the most recent moveTo or closePath.)
        </p>

        <p>Suppose that we want a path that represents a five sided polygon (a pentagon!).</p>

        <p>First of all, let's consider the unit circle. This is a circle centred at (0, 0) with radius of 1 unit.</p>


        <img class="center" style="width: 50%; height: 50%" src="images/pentagon%201.png?raw=true" alt="Pentagon 1">

        <p>A regular polygon, such as a pentagon, can be drawn inside the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%" src="images/pentagon%202.png?raw=true" alt="Pentagon 2">

        <p>In order to draw the pentagon we need to be able to identify the 5 points on the unit circle and rotate and
          draw lines between them.</p>

        <p>This is where some understanding of trigonometry comes in useful.</p>

        <p>Let's consider some point, (a, b) on the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%" src="images/pentagon%203.png?raw=true" alt="Pentagon 3">

        <p>We know the radius (r), in this case it is 1 because it is the unit circle. However, it could be any length
          we choose.</p>

        <p>The point (a, b) can be written in terms of trigonometric ratios as follows:</p>

        <p>The x-ordinate is given by a = <b>r cos 𝛳</b></p>

        <p>The y-ordinate is given by b = <b>r sin 𝛳</b></p>

        <p>In Javascript we can identify the first point as:</p>

        <text class="center"><b>(x + radius * Math.cos(angle), y + radius * Math.sin(angle))</b></text>

        <p>Note that we add the (x, y) ordinate values since we will not necessarily be centering the circle at (0, 0).
        </p>

        <p>Since we are drawing a pentagon we know that the angle we will need to rotate through will be 360o / 5.
          However, all angles must be given in radians. So the angle will be 2*Pi / 5. In Javascript this is written as:
        </p>

        <text class="center"><b>angle = 2*Math.PI/numberOfSides</b></text>

        <p>We can now declare some variables:</p>

        <ul>
          <li>Since we are drawing a pentagon we set the number of sides to 5</li>
          <li>We define a radius for our circle. The pentagon will be drawn inside the circle, each vertex of the
            pentagon will be on the circumference of the circle.</li>
          <li>We set the x-ordinate of the centre of the circle</li>
          <li>We set the y-ordinate of the centre of the circle</li>
          <li>We calculate the size of the external angle of the pentagon. This is the angle we will need to rotate
            through after each line is drawn</li>
        </ul>

        <p>We can now begin the path and set up a loop to draw each line of the polygon:</p>

        <p>We move to the first point which is directly across from the centre of the circle (indicated in red below):
        </p>



        <img class="center" style="width: 50%; height: 50%" src="images/pentagon%204.png?raw=true" alt="Pentagon 4">

        <p>We set up a loop to draw each line and finally stroke the path when we are done.</p>

        <pre><code class="javascript">ctx.beginPath(); // start a new path
let numberOfSides = 5; // a pentagon
let radius=100; // radius of the circle
let x = 125; // center (x) of the circle
let y = 125; // center (y)  of the circle
let angle = 2*Math.PI/numberOfSides; // angle between sides
ctx.beginPath(); // start a new path
ctx.moveTo (x + radius*Math.cos(0), y + radius*Math.sin(0)); // first vertex      
for (let i = 1; i <= numberOfSides; i++) { // loop through each vertex
  ctx.lineTo (x + radius*Math.cos(i * angle), y + radius*Math.sin(i * angle)); // draw line to next vertex
}
ctx.stroke();</code></pre>

        <h2>Curves</h2>

        <p>As noted above, a path can contain other kinds of segments besides lines.</p>

        <p>For example,
          it might be possible to include an arc of a circle as a segment. </p>

        <p>Another type of curve is a
          <b>Bezier curve</b>. Bezier curves can be used to create very general curved shapes. They are fairly
          intuitive, so that they are often used in programs that allow users to design curves interactively.
        </p>

        <p>Mathematically, Bezier curves are defined by parametric polynomial equations, but you don’t
          need to understand what that means to use them.</p>

        <p>There are two kinds of Bezier curve in
          common use, cubic Bezier curves and quadratic Bezier curves; they are defined by cubic and
          quadratic polynomials respectively.</p>

        <p>When the general term "Bezier curve" is used, it usually
          refers to cubic Bezier curves.</p>

        <p>A cubic Bezier curve segment is defined by the two endpoints of the segment together with
          two control points. To understand how it works, it's best to think about how a pen would
          draw the curve segment.</p>

        <p>The pen starts at the first endpoint, headed in the direction of the
          first control point. The distance of the control point from the endpoint controls the speed of
          the pen as it starts drawing the curve. The second control point controls the direction and
          speed of the pen as it gets to the second endpoint of the curve. There is a unique cubic curve
          that satisfies these conditions.</p>

        <img class="center" style="width: 100%; height: 100%" src="images/curves.png?raw=true" alt="Bezier Curves">


        <p>The illustration above shows three cubic Bezier curve segments.</p>

        <p>The two curve segments on
          the right are connected at an endpoint to form a longer curve. The curves are drawn as thick
          black lines. The endpoints are shown as black dots and the control points as blue squares, with
          a thin red line connecting each control point to the corresponding endpoint. (Ordinarily, only
          the curve would be drawn, except in an interface that lets the user edit the curve by hand.)</p>

        <p>Note that at an endpoint, the curve segment is tangent to the line that connects the endpoint
          to the control point. Note also that there can be a sharp point or corner where two curve
          segments meet. However, one segment will merge smoothly into the next if control points are
          properly chosen.
        </p>

        <!--ADD EXAMPLE -->

        <p><b>Quadratic Bezier</b> curve segments are similar to the cubic version, but in the quadratic case,
          there is only one control point for the segment. The curve leaves the first endpoint heading
          in the direction of the control point, and it arrives at the second endpoint coming from the
          direction of the control point. The curve in this case will be an arc of a parabola.</p>

        <p>The three most used methods for drawing curves in canvas are:</p>

        <ul>
          <li>The arc() method (which we learned to use to draw circles)</li>
          <li>The quadraticCurveTo() method</li>
          <li>The bezierCurveTo() method</li>
        </ul>

        <h3>The quadraticCurveTo() Method</h3>

        <p>The quadraticCurveTo() method is used to define a quadratic Bezier curve.</p>

        <p>The quadraticCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx - The x-coordinate of the control point</li>
          <li>cpy - The y-coordinate of the control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a quadratic Bezier curve:</p>

        <pre><code class="javascript"
>const canvas = document.getElementById("myCanvas");  
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.quadraticCurveTo(60, 10, 100, 100);
ctx.stroke();</code></pre>

        <h3>The bezierCurveTo() Method</h3>

        <p>The bezierCurveTo() method is used to define a cubic Bezier curve.</p>

        <p>The bezierCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx1 - The x-coordinate of the first control point</li>
          <li>cpy1 - The y-coordinate of the first control point</li>
          <li>cpx2 - The x-coordinate of the second control point</li>
          <li>cpy2 - The y-coordinate of the second control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a cubic Bezier curve:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.bezierCurveTo(20, 10, 100, 10, 100, 100);
ctx.stroke();</code></pre>

        <p></p>




      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="Mouse_Events">
      <br />
      <header><b>Mouse Events</b></header>
      <article>

        <p>Okay, so we can draw a some shapes, lines, text, and curves. That's great and all, but how do we get from
          that to actually drawing on the
          screen with our mouse?</p>

        <p>Since our use of Canvas is beginning to get more complex, it might help to start using functions.</p>

        <h2>JavaScript Functions</h2>

        <p>Functions are a way to group together a set of instructions that perform a specific task. They allow us to
          reuse code, make our code more organized, and easier to read and maintain.</p>

        <p>Let's start by creating a function that will draw a line on the canvas. We'll call this function drawLine.
        </p>

        <pre><code class="javascript">function drawLine(x1, y1, x2, y2) {
  ctx.beginPath(); // Start a new path
  ctx.moveTo(x1, y1); // Move the pen to the starting point
  ctx.lineTo(x2, y2); // Draw a line to the ending point
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take four arguments: x1, y1, x2, and y2, which represent the starting and ending points
          of
          the line. We then use the canvas API to draw a line between these two points.</p>

        <p>Now that we have our drawLine function, we can use it to draw lines on the canvas. For example:</p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)</code></pre>

        <p>Now, let's create a function that will draw a circle on the canvas. We'll call this function drawCircle.</p>

        <pre><code class="javascript">function drawCircle(x, y, radius) {
  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, radius, 0, Math.PI * 2); // Draw a circle
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take three arguments: x, y, and radius, which represent the center of the circle and its
          radius. We then use the canvas API to draw a circle at the specified location.</p>

        <p>Now that we have our drawCircle function, we can use it to draw circles on the canvas. For example:</p>

        <pre><code class="javascript">drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>By creating functions like drawLine and drawCircle, we can easily draw shapes on the canvas and reuse our
          code
          to create more complex drawings.</p>

        <p>Now that we know the structure of a basic JavaScript function, let's talk about event listeners.</p>

        <h2>Event Listeners</h2>

        <p>Event listeners are a way to listen for and respond to events that occur in the browser. They allow us to
          create interactive web applications that respond to user actions such as clicks, key presses, and mouse
          movements.</p>

        <p>When an event occurs, the browser triggers the event listener, which calls a function that performs a
          specific
          action. This allows us to create dynamic and interactive web pages that respond to user input.</p>

        <p>There are many different types of events that can be listened for, such as:</p>

        <ul>
          <li>Click events - Triggered when an element is clicked</li>
          <li>Mouseover events - Triggered when the mouse pointer enters an element</li>
          <li>Keydown events - Triggered when a key is pressed down</li>
          <li>Submit events - Triggered when a form is submitted</li>
          <li>Scroll events - Triggered when the user scrolls the page</li>
        </ul>

        <p>Event listeners are added to elements in the DOM using the addEventListener method. This method takes two
          arguments: the name of the event to listen for, and the function to call when the event occurs.</p>

        <p>For example, to listen for a click event on a button element, we can use the following code:</p>

        <pre><code class="javascript">const button = document.getElementById('myButton'); // Get the button element   
button.addEventListener('click', () => { // Listen for click event
  console.log('Button clicked!'); // Log a message to the console
});</code></pre>

        <p>Let's add a button to clear the canvas: </p>

        <p>First, let's create a new canvas and a button:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;
&lt;button id="clearButton"&gt;Clear Canvas&lt;/button&gt;</code></pre>

        <p>Next, let's add an event listener to the button that clears the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas'); // Get the canvas element
const ctx = canvas.getContext('2d'); // Get the 2D drawing context

const clearButton = document.getElementById('clearButton'); // Get the clear button element

clearButton.addEventListener('click', () => { // Listen for click event on clear button
  ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
});</code></pre>

        <p>In this example, we get the canvas element and its 2D drawing context, as well as the clear button element.
          We
          then add an event listener to the clear button that calls the clearRect method on the canvas context to clear
          the
          canvas when the button is clicked.</p>

        <p>So let's draw some shapes on the canvas: </p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)
drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>And now we can clear the canvas!</p>

        <h2>Drawing with the Mouse</h2>

        <p>So far, we've used JavaScript to draw shapes on the canvas and an event listener to add button functionalty
          to clear the canvas, but that's not really how we draw, is it?</p>

        <p>A good drawing application has to respond to the mouse. We need to be able to draw lines and shapes by
          clicking and dragging the mouse.</p>

        <p>We need a listener that responds to the canvas being clicked, and another listener that responds to mouse
          movement, but only when the mouse button is pressed down.</p>

        <p>Mouse events in JavaScript allow us to create interactive and dynamic web applications. By capturing mouse
          events, we can respond to user actions such as clicks, movements, and drags. This is particularly useful when
          working with the HTML5 canvas element, as it enables us to create drawing applications, games, and other
          interactive graphics.</p>

        <p>JavaScript provides several mouse events that we can listen for:</p>

        <ul>
          <li><b>mousedown</b> - Triggered when the mouse button is pressed down.</li>
          <li><b>mouseup</b> - Triggered when the mouse button is released.</li>
          <li><b>mousemove</b> - Triggered when the mouse is moved.</li>
          <li><b>click</b> - Triggered when the mouse button is clicked (pressed and released).</li>
          <li><b>dblclick</b> - Triggered when the mouse button is double-clicked.</li>
        </ul>

        <h2>Using Mouse Events with HTML Canvas</h2>

        <p>To use mouse events with the HTML5 canvas, we need to add event listeners to the canvas element. These event
          listeners will call functions that handle the events and perform actions such as drawing on the canvas.</p>

        <h3>Determining Mouse Position</h3>

        <p>To determine the mouse position within the canvas, we need to account for the canvas's position relative to
          the viewport. This can be done using the <b>getBoundingClientRect()</b> method, which returns the size of an
          element and its position relative to the viewport.</p>

        <p>Here is an example of how to determine the mouse position within the canvas:</p>

        <pre><code class="javascript">function getMousePos(canvas, event) {
const rect = canvas.getBoundingClientRect(); // Get the size and position of the canvas
  return {
    x: event.clientX - rect.left, // X coordinate of mouse relative to canvas
    y: event.clientY - rect.top // Y coordinate of mouse relative to canvas
  };
}

// Example usage
canvas.addEventListener('mousemove', (event) => { // Listen for mousemove event
  const mousePos = getMousePos(canvas, event); // Get the mouse position
  console.log('Mouse position: ' + mousePos.x + ',' + mousePos.y); // Log the mouse position
});</code></pre>

        <p>In this example:</p>

        <ul>
          <li>The <b>getMousePos</b> function takes the canvas element and the mouse event as arguments.</li>
          <li>It uses <b>getBoundingClientRect()</b> to get the position of the canvas relative to the viewport.</li>
          <li>It calculates the mouse position by subtracting the canvas's top-left corner coordinates from the mouse's
            <b>clientX</b> and <b>clientY</b> coordinates. The <b>clientX</b> and <b>clientY</b> properties of the mouse
            event provide the horizontal and vertical coordinates of the mouse pointer, respectively, relative to the
            viewport (the visible area of the browser window).
          </li>
          <li>The mouse position is logged to the console whenever the mouse is moved over the canvas.</li>
        </ul>

        <h3>Example: Drawing on Canvas with Mouse</h3>

        <p>Now let's try using the mouse to draw.</p>

        <h4>HTML</h4>
        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;</code></pre>

        <h4>JavaScript</h4>
        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function startPosition(e) {
  painting = true;
  draw(e);
}

function endPosition() {
  painting = false;
  ctx.beginPath(); // Begin a new path to avoid connecting lines
}

function draw(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.lineWidth = 5; // Set the line width
  ctx.strokeStyle = 'black'; // Set the line color

  ctx.lineTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Draw a line to the current mouse position
  ctx.stroke(); // Stroke the line
  ctx.beginPath(); // Begin a new path
  ctx.moveTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Move to the new starting point
}

canvas.addEventListener('mousedown', startPosition); // Listen for mousedown event
canvas.addEventListener('mouseup', endPosition); // Listen for mouseup event
canvas.addEventListener('mousemove', draw);</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define three functions: <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b>.</li>
          <li>The <b>startPosition</b> function is called when the mouse button is pressed down. It sets the
            <b>painting</b>
            flag to true and calls the <b>draw</b> function.
          </li>
          <li>The <b>endPosition</b> function is called when the mouse button is released. It sets the <b>painting</b>
            flag to
            false and begins a new path to avoid connecting lines.</li>
          <li>The <b>draw</b> function is called when the mouse is moved. It checks if the user is drawing, sets the
            line
            width and color, draws a line to the current mouse position, and moves to the new starting point.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b> functions
            respectively.</li>
        </ul>

        <p>In another example we can use our mouse to draw circles on the canvas: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function drawCircle(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.beginPath(); // Start a new path
  ctx.arc(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop, 10, 0, Math.PI * 2); // Draw a circle
  ctx.fill(); // Fill the circle
}

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
  painting = true; // Set the painting flag to true
  drawCircle(e); // Draw a circle
});

canvas.addEventListener('mouseup', () => { // Listen for mouseup event
  painting = false; // Set the painting flag to false
});

canvas.addEventListener('mousemove', drawCircle); // Listen for mousemove event</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define a <b>drawCircle</b> function that draws a circle at the current mouse position.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>drawCircle</b> function
            when the mouse button is pressed down, released, and moved respectively.</li>
        </ul>

        <p>We can simplify the code a bit by using getBoundingClientRect which gives the size of an element and its
          position
          relative to the viewport:</p>

        <pre><code class="javascript
">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

ctx.beginPath(); // Start a new path
ctx.arc(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top, 10, 0, Math.PI * 2); // Draw a circle
ctx.fill(); // Fill the circle</code></pre>

        <p>Although this works, we can achieve the same result a bit simpler with offsetX and offsetY:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
ctx.beginPath(); // Start a new path
ctx.arc(e.offsetX, e.offsetY, 10, 0, Math.PI * 2); // Draw a circle
ctx.fill(); // Fill the circle
});</code></pre>

        <p>This works because offsetX and offsetY give the position of the mouse <b>relative to the target element</b>,
          in this
          case the canvas.</p>

        <p>So to rewrite our draw function:</p>

        <pre><code class="javascript">function draw(e) {
    if (!painting) return; // Exit the function if the user is not drawing

    ctx.lineWidth = 5; // Set the line width
    ctx.strokeStyle = 'black'; // Set the line color

    ctx.lineTo(e.offsetX, e.offsetY); // Draw a line to the current mouse position
    ctx.stroke(); // Stroke the line
    ctx.beginPath(); // Begin a new path
    ctx.moveTo(e.offsetX, e.offsetY); // Move to the new starting point
}</code></pre>

        <p>And that's it! We can now draw on the canvas with our mouse.</p>

        <p>By combining mouse events with the canvas API, we can create interactive drawing applications that respond to
          user
          input.</p>

        <hr />

        <h2>Exercise: Drawing More Figures in HTML5 Canvas</h2>

        <p>How can we draw the following graphics on a 400 x 400 canvas?</p>

        <img class="center" style="width: 50%; height: 50%" src="images/Exercise%202.png?raw=true"
          alt="Drawing Exercise">


      </article>
      <br />
    </section>

    <hr />



    <section class="main-section" id="Additional_Events">
      <br />
      <header><b>Additional Events</b></header>
      <article>
        <p>In addition to our mouse down events, there are also a few others that may come in handy (for both 2D and 3D
          graphics)</p>

        <h2>Keyboard Events</h2>

        <p>Keyboard events are triggered when a key is pressed or released on the keyboard. They allow us to respond to
          user input and create interactive web applications that respond to key presses.</p>

        <p>There are several keyboard events that we can listen for:</p>

        <ul>
          <li><b>keydown</b> - Triggered when a key is pressed down.</li>
          <li><b>keyup</b> - Triggered when a key is released.</li>
          <li><b>keypress</b> - Triggered when a key is pressed down and released.</li>
        </ul>

        <p>Keyboard events provide information about the key that was pressed, such as the key code and the key value.
          This information can be used to perform specific actions based on the key that was pressed.</p>

        <p>Here is an example of how to listen for keyboard events:</p>

        <pre><code class="javascript">document.addEventListener('keydown', (event) => { // Listen for keydown event
  console.log('Key pressed: ' + event.key); // Log the key that was pressed

  if (event.key == 'ArrowUp') {
    console.log('Up arrow key pressed'); // Log a message if the up arrow key was pressed
  }

  if (event.key == 'ArrowDown') {
    console.log('Down arrow key pressed'); // Log a message if the down arrow key was pressed
  }

  // Add more key press conditions as needed
});</code></pre>

        <p>Each key on the keyboard has a unique key code and key value. The key code is a numerical value that
          represents
          the key, while the key value is a string that represents the key.</p>

        <p>For example, the key code for the up arrow key is 38, and the key value is 'ArrowUp'.</p>

        <p>Standard key codes are as follows: </p>

        <img class="center" style="width: 100%; height: 100%" src="images/keycodes.png?raw=true" alt="Key Codes">


        <p>Keyboard events can be used to create keyboard shortcuts, control game characters, and more.</p>

        <p>For example, let's write some code to create a rectangle that we will be able to move with the WASD and
          directional arrow keys: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let x = 100; // Initial x position of the rectangle
let y = 100; // Initial y position of the rectangle
let speed = 5; // Speed of the rectangle

function drawRectangle() {
    ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
    ctx.fillStyle = 'blue'; // Set rectangle color
    ctx.fillRect(x, y, 50, 50); // Draw the rectangle
}

document.addEventListener('keydown', (event) => { // Listen for keydown event
    switch (event.key) {
        case 'ArrowUp':
        case 'w':
            y -= speed; // Move the rectangle up
            break;
        case 'ArrowDown':
        case 's':
            y += speed; // Move the rectangle down
            break;
        case 'ArrowLeft':
        case 'a':
            x -= speed; // Move the rectangle left
            break;
        case 'ArrowRight':
        case 'd':
            x += speed; // Move the rectangle right
            break;
    }

    drawRectangle(); // Redraw the rectangle
});

// Initial call to draw the rectangle when the page loads
drawRectangle();</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define the initial x and y positions of the rectangle, as well as the speed at which it will move.</li>
          <li>We define a <b>drawRectangle</b> function that clears the canvas and draws a rectangle at the current
            position.</li>
          <li>We add an event listener for the <b>keydown</b> event that moves the rectangle based on the key that was
            pressed.
          </li>
          <li>We use a <b>switch</b> statement to check which key was pressed and update the x and y positions of the
            rectangle
            accordingly.</li>
          <li>We call the <b>drawRectangle</b> function to redraw the rectangle after it has been moved.</li>
        </ul>


        <h2>Touch Events</h2>

        <p>In the early days of touch-enabled devices, touch events were often interpreted and essentially "translated"
          into mouse events for compatibility with existing web applications, meaning that a touch interaction would
          trigger a corresponding mouse event like a click or hover, although this approach had limitations when dealing
          with multi-touch gesture.</p>

        <p>With the widespread adoption of touchscreen devices, such as a smartphone or tablet, HTML5 brings to the
          table, among many other things, a set of touch-based interaction events. </p>

        <p>Mouse-based events such as hover, mouse in, mouse out etc. aren't able to adequately capture the range of
          interactions possible via touchscreen, so touch events are a welcome and necessary addition to the web
          developer's toolbox.</p>

        <p>They allow us to create web applications that respond to touch gestures, such as tapping, swiping, and
          pinching.</p>

        <p>Use cases for the touch events API include gesture recognition, multi-touch, drag and drop, and any other
          touch-based interfaces.</p>

        <h2>The Touch Events API</h2>

        <p>We'll get some of the technical details of the API out of the way first, before moving on to some real
          examples. The API is
          defined in terms of Touches, TouchEvents, and TouchLists.</p>

        <p>Each Touch describes a touch point, and has the following attributes:</p>

        <ul>
          <li><b>clientX</b> - The x-coordinate of the touch point relative to the viewport.</li>
          <li><b>clientY</b> - The y-coordinate of the touch point relative to the viewport.</li>
          <li><b>screenX</b> - The x-coordinate of the touch point relative to the screen.</li>
          <li><b>screenY</b> - The y-coordinate of the touch point relative to the screen.</li>
          <li><b>pageX</b> - The x-coordinate of the touch point relative to the document.</li>
          <li><b>pageY</b> - The y-coordinate of the touch point relative to the document.</li>
          <li><b>target</b> - The element that the touch point started in.</li>
          <li><b>identifier</b> - A unique identifier for the touch point.</li>
        </ul>

        <p>There are several touch events that we can listen for:</p>

        <ul>
          <li><b>touchstart</b> - Triggered when a touch point is placed on the touch surface.</li>
          <li><b>touchmove</b> - Triggered when a touch point is moved along the touch surface.</li>
          <li><b>touchend</b> - Triggered when a touch point is removed from the touch surface.</li>
          <li><b>touchcancel</b> - Triggered when a touch point is disrupted in some way.</li>
        </ul>

        <p>Touch events provide information about the touch points, such as the touch coordinates and the touch
          identifier. This information can be used to perform specific actions based on the touch gestures.</p>

        <p>Here is an example of how to listen for touch events:</p>

        <pre><code class="javascript">document.addEventListener('touchstart', (event) => { // Listen for touchstart event
  console.log('Touch started at: ' + event.touches[0].clientX + ',' + event.touches[0].clientY); // Log the touch
  coordinates
});</code></pre>

        <p>Let's go one step further and display the touch position information visually, by displaying a dot on the
          canvas at the point where it was touched.</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('touchstart', (event) => { // Listen for touchstart event
  const touch = event.touches[0]; // Get the first touch point
  const x = touch.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
  const y = touch.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the touch point
  ctx.fill(); // Fill the circle
});</code></pre>

        <p>In this example:</p>
        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We add an event listener for the <b>touchstart</b> event to the canvas element.</li>
          <li>We get the first touch point from the <b>touches</b> property of the event.</li>
          <li>We calculate the x and y coordinates of the touch point relative to the canvas.</li>
          <li>We draw a circle at the touch point using the <b>arc</b> method and fill it using the <b>fill</b> method.
          </li>
        </ul>

        <h2>Browser Support and Fallbacks</h2>

        <p>Touch events are widely supported among mobile devices.</p>

        <p>However, unless specifically targeting touch devices, a fallback should be implemented when touchevents are
          not supported. In these cases, the traditional click etc. events can be bound to, but as discussed below, care
          is needed when deciding which events to support instead of the touch events.</p>

        <h2>Touch and Mouse Events</h2>

        <p>Since touch events may not be supported on a user's device - indeed, the user may not even be accessing your
          app on a touchscreen device - this contingency should be planned for. </p>

        <p>You may want to enable your app to support particular mouse events instead. Care should be taken here as
          there is not a one-to-one correspondance between mouse events and touch events, and the behaviour differences
          can be subtle.</p>


        <p>So for our code we should also enable mouse events:</p>

        <pre><code class="javascript">canvas.addEventListener('mousedown', (event) => { // Listen for mousedown event
const x = event.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
const y = event.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

ctx.beginPath(); // Start a new path
ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the mouse point
ctx.fill(); // Fill the circle
});</code></pre>

        <h2>Best Practices</h2>

        <p>Care should also be taken implementing touch events that the events don't interfere with typical browser
          behaviours such as scrolling and zooming - thus there is an argument for disabling these default browser
          behaviours if you are making use of touch events.</p>

        <p>It is also important to remember that touch events are not the same as mouse events.</p>

        <p>For example, a touchstart event is not the same as a mousedown event. The former is triggered when a touch
          point is placed on the touch surface, while the latter is triggered when a mouse button is pressed down. </p>

        <p>Therefore, it is important to consider the differences between touch and mouse events when designing
          touch-based interfaces.</p>

        <p>Finally, it is important to test touch events on a variety of devices to ensure that they work as expected
          and provide a good user experience.</p>

        <p>In addition, there are also scroll events, resize events, and more. These can all be used to create more
          interactive and dynamic web applications.</p>

      </article>
      <br />
    </section>

    <hr />
    <section class="main-section" id="Transforms">
      <br />

      <header><b>Transforms</b></header>
      <article>
        <p>It is possible to transform
          coordinates from one coordinate system to another. Let's look at how geometric transformations can be used to
          place graphics
          objects into a coordinate system.</p>

        <h2>Viewing and Modeling</h2>

        <p>In a typical application, we have a rectangle made of pixels, with its natural pixel coordinates,
          where an image will be displayed. This rectangle will be called the <b>viewport</b>.</p>

        <p>We also have
          a set of geometric objects that are defined in a possibly different coordinate system, generally
          one that uses real-number coordinates rather than integers. These objects make up the “scene”
          or "world" that we want to view, and the coordinates that we use to define the scene are called
          <b>world coordinates</b>.
        </p>

        <p>For 2D graphics, the world lies in a plane. It's not possible to show a picture of the entire
          infinite plane. We need to pick some rectangular area in the plane to display in the image.
          Let's call that rectangular area the <b>window</b>, or view window.</p>

        <p>A coordinate transform is used
          to map the window to the viewport.</p>

        <img class="center" style="width: 100%; height: 100%" src="images/window%20and%20viewport.png?raw=true"
          alt="Coordinate Transformation">

        <p>In this illustration, T represents the coordinate transformation. T is a function that takes world
          coordinates (x,y) in some window and maps them to pixel coordinates T(x,y) in the viewport</p>

        <p>In this example, as you can
          check,</p>

        <text class="center">T(x,y) = ( 800*(x+4)/8, 600*(3-y)/6 )</text>

        <p>Look at the rectangle with corners at (-1,2) and (3,-1) in the window. When this rectangle is
          displayed in the viewport, it is displayed as the rectangle with corners T(-1,2) and T(3,-1). In
          this example, T(-1,2) = (300,100) and T(3,-1) = (700,400).</p>

        <p>We use coordinate transformations in this way because it allows us to choose a world
          coordinate system that is natural for describing the scene that we want to display, and it is easier to do
          that than to work directly with viewport coordinates. Along the same lines,
          suppose that we want to define some complex object, and suppose that there will be several
          copies of that object in our scene. Or maybe we are making an animation, and we would like the
          object to have different positions in different frames.</p>

        <p>We would like to choose some convenient
          coordinate system and use it to define the object once and for all.</p>

        <p>The coordinates that we
          use to define an object are called <b>object coordinates</b> for the object. </p>

        <p>When we want to place
          the object into a scene, we need to transform the object coordinates that we used to define the
          object into the world coordinate system that we are using for the scene. The transformation that
          we need is called a <b>modeling transformation</b>. </p>

        <p>This picture illustrates an object defined in
          its own object coordinate system and then mapped by three different modeling transformations
          into the world coordinate system:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/modeling%20transformation.png?raw=true"
          alt="Modeling Transformation">

        <p>Remember that in order to view the scene, there will be another transformation that maps the
          object from a view window in world coordinates into the viewport.</p>

        <p>Now, keep in mind that the choice of a view window tells which part of the scene is shown
          in the image. Moving, resizing, or even rotating the window will give a different view of the
          scene. Suppose we make several images of the same car:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/modeling%20transformation%202.png?raw=true"
          alt="Modeling Transformation 2">

        <p>What happened between making the top image in this illustration and making the image on
          the bottom left?</p>

        <p>In fact, there are two possibilities: Either the car was moved to the right, or
          the view window that defines the scene was moved to the left.</p>

        <p>This is important, so be sure
          you understand it. (Try it with your cell phone camera. Aim it at some objects, take a step
          to the left, and notice what happens to the objects in the camera's viewfinder: They move to the right in the
          picture!)</p>

        <p>Similarly, what happens between the top picture and the middle
          picture on the bottom? Either the car rotated counterclockwise, or the window was rotated
          clockwise. (Again, try it with a camera—you might want to take two actual photos so that you
          can compare them.)</p>

        <p>Finally, the change from the top picture to the one on the bottom right
          could happen because the car got smaller or because the window got larger. (On your camera,
          a bigger window means that you are seeing a larger field of view, and you can get that by
          applying a zoom to the camera or by backing up away from the objects that you are viewing.)</p>

        <p>There is an important general idea here. When we modify the view window, we change
          the coordinate system that is applied to the viewport. But in fact, this is the same as leaving
          that coordinate system in place and moving the objects in the scene instead. Except that to
          get the same effect in the final image, you have to apply the opposite transformation to the
          objects (for example, moving the window to the left is equivalent to moving the objects to the
          right).</p>

        <p>So, there is no essential distinction between transforming the window and transforming
          the object. Mathematically, you specify a geometric primitive by giving coordinates in some
          natural coordinate system, and the computer applies a sequence of transformations to those
          coordinates to produce, in the end, the coordinates that are used to actually draw the primitive
          in the image.</p>

        <p>You will think of some of those transformations as modeling transforms and some
          as coordinate transforms, but to the computer, it's all the same.</p>

        <p>We will return to this idea several times later throughout this class, but in any case, you can see that
          geometric transforms are a central concept in computer graphics. Let's look at some basic types
          of transformation in more detail.</p>

        <p>The transforms we will use in 2D graphics can be written in
          the form:</p>

        <text class="center"><b>x1 = a*x + b*y + e</b></text>

        <text class="center"><b>y1 = c*x + d*y + f</b></text>

        <p>where (x,y) represents the coordinates of some point before the transformation is applied, and
          (x1,y1 ) are the transformed coordinates.</p>

        <p>The transform is defined by the six constants a, b, c,
          d, e, and f. Note that this can be written as a function T, where</p>

        <text class="center">T(x,y) = (a*x + b*y + c, d*x + e*y + f)</text>

        <p>A transformation of this form is called an <b>affine transform</b>.</p>
        <p>An affine transform has the
          property that, when it is applied to two parallel lines, the transformed lines will also be parallel.
          Also, if you follow one affine transform by another affine transform, the result is again an affine
          transform.</p>

        <p>There are four basic types of affine transforms that are commonly used in computer graphics:</p>

        <ul>
          <li><b>Translation</b> - Moves an object by a specified distance along the x and y axes.</li>
          <li><b>Rotation</b> - Rotates an object by a specified angle around a specified point.</li>
          <li><b>Scaling</b> - Increases or decreases the size of an object by a specified factor along the x and y
            axes.</li>
          <li><b>Shearing</b> - Skews an object by a specified angle along the x or y axis.</li>
        </ul>

        <h2>Translation</h2>

        <p>A translation transform simply moves every point by a certain amount horizontally and a
          certain amount vertically.</p>

        <p>If (x,y) is the original point and (x1,y1) is the transformed point,
          then the formula for a translation is:</p>

        <text class="center"><b>x1 = x + e</b></text>

        <text class="center"><b>y1 = y + f</b></text>

        <p>where e is the number of units by which the point is moved horizontally and f is the amount by
          which it is moved vertically. (Thus for a translation, a = d = 1, and b = c = 0 in the general
          formula for an affine transform.)</p>

        <p>A 2D graphics system will typically have a function such as:</p>

        <text class="center"><b>translate(e, f)</b></text>

        <p>to apply a translate transformation. The translation would apply to everything that is drawn
          <b>after</b> the command is given. That is, for all subsequent drawing operations, e would be added
          to the x-coordinate and f would be added to the y-coordinate.
        </p>

        <p>Let's look at an example: Suppose that you draw an "F" using coordinates in which the "F" is centered at
          (0,0).</p>

        <p>If you say translate(4,2) before drawing the "F", then every point of the "F" will be moved
          horizontally by 4 units and vertically by 2 units before the coordinates are actually used, so
          that after the translation, the "F" will be centered at (4,2):</p>

        <img class="center" style="width: 50%; height: 50%" src="images/translation.png?raw=true"
          alt="Translation Transformation">

        <p>The light gray "F" in this picture shows what would be drawn without the translation; the dark
          red "F" shows the same "F" drawn after applying a translation by (4,2).</p>

        <p>The top arrow shows
          that the upper left corner of the "F" has been moved over 4 units and up 2 units. Every point
          in the "F" is subjected to the same displacement.</p>

        <p>Note that in these examples, we are assuming
          that the y-coordinate increases from bottom to top. That is, the y-axis points up.</p>

        <p>Remember that when you give the command translate(e,f), the translation applies to <b>all</b> the
          drawing that you do after that, not just to the next shape that you draw.</p>

        <p>If you apply another
          transformation after the translation, the second transform will not replace the translation.
          It will be combined with the translation, so that subsequent drawing will be affected by the
          combined transformation.</p>

        <p>For example, if you combine translate(4,2) with translate(-1,5), the
          result is the same as a single translation, translate(3,7). This is an important point, and there
          will be a lot more to say about it later.
        </p>

        <p>Also remember that you don't compute coordinate transformations yourself. You just
          specify the original coordinates for the object (that is, the object coordinates), and you specify
          the transform or transforms that are to be applied. The computer takes care of applying the
          transformation to the coordinates.</p>

        <p>You don't even need to know the exact equations that are used
          for the transformation; <b>you just need to understand what it does geometrically</b>.</p>

        <p>HTML5 Canvas has a built in translate() method - used to move an object by x and y, where:</p>

        <ul>
          <li>x is the amount to move the object horizontally.</li>
          <li>y is the amount to move the object vertically.</li>
        </ul>

        <p>For example, here is how you might use the translate function in HTML5 Canvas:</p>

        <p>First, draw one rectangle in position (10,10), then set translate() to (70,70) (This will be the new start
          point). Then draw another rectangle in position (10,10). Notice that the second rectangle now starts in
          position (80,80):</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(10, 10, 100, 50);

ctx.translate(70, 70);

ctx.fillStyle = "blue";
ctx.fillRect(10, 10, 100, 50);</code></pre>

        <p>Our first rectangle is placed at position (10, 10) but when we use translate(70,70), even though our next
          drawn rectangle is also placed
          at position (10, 10), it is actually placed at position (80, 80) because our x-position is now (70 + 10) and
          y-position is now (70 + 10).</p>
        </p>

        <h2>Rotation</h2>

        <p>A rotation transform, for our purposes here, rotates each point about the origin, (0,0). Every
          point is rotated through the same angle, called the angle of rotation.</p>

        <p>For this purpose, angles can be measured either in degrees or in radians.</p>

        <p>A rotation with a positive angle rotates objects in the direction from the positive x-axis towards
          the positive y-axis.</p>

        <p>This is counterclockwise in a coordinate system (cartesian) where the y-axis points up,
          as it does in my examples here, but it is clockwise in the usual pixel coordinates, where the
          y-axis points down rather than up.</p>

        <p>Although it is not obvious, when rotation through an angle of r radians about the origin is applied to the
          point (x,y), then the resulting point (x1,y1 ) is
          given by: </p>

        <text class="center">x1 = cos(r) * x - sin(r) * y</text>
        <text class="center">y1 = sin(r) * x + cos(r) * y</text>

        <p>That is, in the general formula for an affine transform, e = f = 0, a = d = cos(r), b = -sin(r),
          and c = sin(r).</p>

        <p>Here is a picture that illustrates a rotation about the origin by the angle
          negative 135 degrees:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/rotation.png?raw=true"
          alt="Rotation Transformation">


        <p>Again, the light gray "F" is the original shape, and the dark red "F" is the shape that results
          if you apply the rotation. The arrow shows how the upper left corner of the original “F” has
          been moved.</p>

        <p>A 2D graphics API would typically have a command rotate(r) to apply a rotation. The
          command is used before drawing the objects to which the rotation applies.</p>

        <p>For example, HTML5 Canvas
          has a rotate method that takes an angle in radians as a parameter. The rotation is applied to all
          subsequent drawing operations.</p>

        <p>As a reminder, angles are in radians, as opposed to degrees. So we use <b>(Math.PI/180)*[degree]</b> to
          convert.</p>

        <p>Let's rotate a rectangle in canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(50, 10, 100, 50);

ctx.rotate((Math.PI/180)*20); // Rotate 20 degrees

ctx.strokeStyle = "blue";
ctx.strokeRect(70, 30, 100, 50);</code></pre>

        <p>Remember that the rotation is applied to all subsequent drawing operations, so we need to
          translate to the center of the rectangle to rotate it around its center.</p>

        <h2>Scaling</h2>

        <p>A scaling transform can be used to make objects bigger or smaller.</p>

        <p>Mathematically, a scaling
          transform simply multiplies each x-coordinate by a given amount and each y-coordinate by a
          given amount.</p>

        <p>That is, if a point (x,y) is scaled by a factor of a in the x direction and by a
          factor of d in the y direction, then the resulting point (x1,y1 ) is given by:</p>

        <text class="center">x1 = a*x</text>
        <text class="center">y1 = d*y</text>

        <p>If you apply this transform to a shape that is centered at the origin, it will stretch the shape
          by a factor of a horizontally and d vertically.</p>

        <p>Here is an example, in which the original light
          gray "F" is scaled by a factor of 3 horizontally and 2 vertically to give the final dark red "F":</p>

        <img class="center" style="width: 50%; height: 50%" src="images/scaling.png?raw=true"
          alt="Scaling Transformation">


        <p>The common case where the horizontal and vertical scaling factors are the same is called
          <b>uniform scaling</b>. Uniform scaling stretches or shrinks a shape without distorting it.
        </p>

        <p>When scaling is applied to a shape that is not centered at (0,0), then in addition to being
          stretched or shrunk, the shape will be moved away from 0 or towards 0. In fact, the true
          description of a scaling operation is that it pushes every point away from (0,0) or pulls every
          point towards (0,0). If you want to scale about a point other than (0,0), you can use a sequence
          of three transforms, similar to what was done in the case of rotation.</p>

        <p>A 2D graphics API can provide a function scale(a,d) for applying scaling transformations.
          As usual, the transform applies to all x and y coordinates in subsequent drawing operations.</p>

        <p>One unit on the canvas is one pixel. If we set the scaling factor to 2, one unit becomes two pixels, and
          shapes will be drawn twice as large. If we set a scaling factor to 0.5, one unit becomes 0.5 pixels, and
          shapes will be drawn at half size.</p>

        <p>Note that negative scaling factors are allowed and will result in reflecting the shape as well
          as possibly stretching or shrinking it. For example, scale(1,-1) will reflect objects vertically,
          through the x -axis.</p>

        <p>As with the other transformations, a scaling transform is applied to all subsequent drawing
          operations. In HTML5 Canvas, you can use the scale method to apply a scaling transform:</p>

        <p>The scale() method has the following parameters: </p>

        <ul>
          <li><b>x</b> - The scaling factor for the x-axis (1 is the original size).</li>
          <li><b>y</b> - The scaling factor for the y-axis (1 is the original size).</li>
        </ul>

        <p>One unit on the canvas is one pixel.</p>

        <p>So, if we set the scaling factor to 2, one unit becomes two pixels, and
          shapes will be drawn twice as large. If we set a scaling factor to 0.5, one unit becomes 0.5 pixels, and
          shapes will be drawn at half size.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.scale(2, 2);

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>It is a fact that every affine transform can be created by combining translations,
          rotations about the origin, and scalings about the origin. </p>

        <p>Also note that a transform that is made from translations and rotations, with no scaling,
          will preserve length and angles in the objects to which it is applied. It will also preserve aspect
          ratios of rectangles.</p>

        <p>Transforms with this property are called <b>"Euclidean"</b>. If you also allow
          uniform scaling, the resulting transformation will preserve angles and aspect ratio, but not
          lengths.
        </p>

        <h2>Shearing</h2>

        <p>We will look at one more type of basic transform, a <b>shearing transform</b>.</p>

        <p>Although shears
          can in fact be built up out of rotations and scalings if necessary, it is not really obvious how
          to do so.</p>

        <p>A shear will "tilt" objects. A horizontal shear will tilt things towards the left (for
          negative shear) or right (for positive shear). A vertical shear tilts them up or down. </p>

        <p>Here is an
          example of horizontal shear:</p>

        <img class="center" style="width: 50%; height: 50%" src="images/shearing.png?raw=true"
          alt="Shearing Transformation">


        <p>A horizontal shear does not move the x-axis. Every other horizontal line is moved to the left or
          to the right by an amount that is proportional to the y-value along that line.</p>

        <p>When a horizontal shear is applied to a point (x,y), the resulting point (x1,y1) is given by:
        </p>

        <text class="center">x1 = x + b * y</text>
        <text class="center">y1 = y</text>

        <p>for some constant shearing factor b. Similarly, a vertical shear with shearing factor c is given
          by the equations:</p>

        <text class="center">x1 = x</text>
        <text class="center">y1 = c * x + y</text>

        <p>Shear is occasionally called "skew", but skew is usually specified as an angle rather than as a
          shear factor.</p>















        <h2>Combining Transformations</h2>

        <p>As we just saw, we are now in a position to see what can happen when you combine two transformations.
          Suppose that before drawing some object, you say:</p>

        <text class="center"><b>translate(4,0)</b></text>
        <text class="center"><b>rotate(90)</b></text>

        <p>Assume that angles are measured in degrees.</p>

        <p>The translation will then apply to all subsequent
          drawing. But, because of the rotation command, the things that you draw after the translation
          are <b>rotated</b> objects.</p>

        <p>That is, the translation applies to objects that have <b>already</b> been rotated.</p>

        <p>An example is shown on the left in the illustration below, where the light gray "F" is the original
          shape, and red "F" shows the result of applying the two transforms to the original.</p>

        <p>The original "F" was first rotated through a 90 degree angle, and then moved 4 units to the right.</p>

        <img class="center" style="width: 100%; height: 100%" src="images/combining%20transforms.png?raw=true"
          alt="Combining Transformations">


        <p>Note that transforms are applied to objects in the reverse of the order in which they are given
          in the code (because the first transform in the code is applied to an object that has already
          been affected by the second transform). And note that the order in which the transforms are
          applied is important.</p>

        <p>If we reverse the order in which the two transforms are applied in this
          example, by saying: </p>

        <text class="center"><b>rotate(90)</b></text>
        <text class="center"><b>translate(4,0)</b></text>

        <p>then the result is as shown on the right in the above illustration. In that picture, the original
          "F" is first moved 4 units to the right and the resulting shape is then rotated through an angle
          of 90 degrees about the origin to give the shape that actually appears on the screen.</p>

        <p>For another example of applying several transformations, suppose that we want to rotate
          a shape through an angle r about a point (p,q) instead of about the point (0,0).</p>

        <p>We can do
          this by first moving the point (p,q) to the origin, using translate(-p,-q). Then we can do a
          standard rotation about the origin by calling rotate(r). Finally, we can move the origin back
          to the point (p,q) by applying translate(p,q).</p>

        <p>Keeping in mind that we have to write the code
          for the transformations in the reverse order, we need to say: </p>

        <text class="center"><b>translate(p,q)</b></text>
        <text class="center"><b>rotate(r)</b></text>
        <text class="center"><b>translate(-p,-q)</b></text>

        <p>before drawing the shape. (In fact, some graphics APIs let us accomplish this transform with a
          single command such as rotate(r,p,q). This would apply a rotation through the angle r about
          the point (p,q).)
        </p>

        <p>In HTML5 Canvas, we have a method called transform().</p>

        <p>The transform() method replaces the current transformation matrix with the matrix described by the
          arguments of this method. The transform() method multiplies the current transformation matrix with the
          matrix described by the arguments of this method. This is useful for applying multiple transformations to
          the same shape.</p>

        <p>The transform method takes the following parameters: </p>

        <ul>
          <li><b>a</b> - Horizontal scaling. 1 is no scaling.</li>
          <li><b>b</b> - Horizontal skewing.</li>
          <li><b>c</b> - Vertical skewing.</li>
          <li><b>d</b> - Vertical scaling. 1 is no scaling.</li>
          <li><b>e</b> - Horizontal moving.</li>
          <li><b>f</b> - Vertical moving.</li>
        </ul>
        <p>Here is an example of how to use the transform() method in HTML5 Canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.transform(2, 0, 0, 2, 40, 40); // Scale by 2 and move 40 units
ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>We can scale, rotate, and translate using a mixture of these methods as well:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.translate(200, 40); // Move 200 units to the right and 40 units down
ctx.rotate((Math.PI/180)*45); // Rotate 45 degrees
ctx.scale(2, 2); // Scale by 2

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>


        <p>Note that we will talk again about transformations in the context of 3D graphics,
          where they
          become even more important.</p>


      </article>
      <br />
      <hr />
    </section>


    <section class="main-section" id="Linear_Algebra">
      <br />
      <header><b>Linear Algebra</b></header>
      <article>
        <p>Linear algebra is a branch of mathematics that deals with vectors and matrices. It is a fundamental tool in
          computer graphics, as it allows us to perform transformations on objects in 2D and 3D space.</p>

        <h2>Vectors</h2>

        <p>A vector is a quantity that has a length and a direction.</p>

        <p>A vector can be visualized as an arrow,
          as long as you remember that it is the length and direction of the arrow that are relevant, and
          that its specific location is irrelevant.</p>
        <p>Vectors are often used in computer graphics to represent
          directions, such as the direction from an object to a light source or the direction in which a
          surface faces. In those cases, we are more interested in the direction of a vector than in its
          length.
        </p>

        <p>If we visualize a 3D vector V as an arrow starting at the origin, (0,0,0), and ending at a
          point P, then we can, to a certain extent, identify V with P—at least as long as we remember
          that an arrow starting at any other point could also be used to represent V. If P has coordinates
          (a,b,c), we can use the same coordinates for V.</p>

        <p>When we think of (a,b,c) as a vector, the value
          of a represents the change in the x -coordinate between the starting point of the arrow and its
          ending point, b is the change in the y-coordinate, and c is the change in the z -coordinate. For
          example, the 3D point (x,y,z ) = (3,4,5) has the same coordinates as the vector (dx,dy,dz ) =
          (3,4,5). </p>

        <p>For the point, the coordinates (3,4,5) specify a position in space in the xyz coordinate
          system.</p>

        <p>For the vector, the coordinates (3,4,5) specify the change in the x, y, and z coordinates
          along the vector.</p>

        <p>If we represent the vector with an arrow that starts at the origin (0,0,0), then
          the head of the arrow will be at (3,4,5). But we could just as well visualize the vector as an
          arrow that starts at the point (1,1,1), and in that case the head of the arrow would be at the
          point (4,5,6).</p>

        <p>The distinction between a point and a vector is subtle. For some purposes, the distinction
          can be ignored; for other purposes, it is important. Often, all that we have is a sequence of
          numbers, which we can treat as the coordinates of either a vector or a point, whichever is more
          appropriate in the context.</p>

        <p>One of the basic properties of a vector is its length. In terms of its coordinates, the length
          of a 3D vector (x,y,z) is given by sqrt(x^2+y^2+z^2). (This is just the Pythagorean theorem in
          three dimensions.) If v is a vector, its length is denoted by |v|. </p>

        <p>The length of a vector is also
          called its norm. (We are considering 3D vectors here, but concepts and formulas are similar
          for other dimensions.)
        </p>

        <p>Vectors of length 1 are particularly important. They are called <b>unit vectors</b>. If v = (x,y,z )
          is any vector other than (0,0,0), then there is exactly one unit vector that points in the same
          direction as v.</p>

        <p>That vector is given by:</p>

        <text class="center"><b>( x/length, y/length, z/length )</b></text>

        <p>where length is the length of v. Dividing a vector by its length is said to normalize the vector:
          The result is a unit vector that points in the same direction as the original vector.</p>

        <p>Two vectors can be added. Given two vectors v1 = (x1,y1,z1) and v2 = (x2,y2,z2), their
          sum is defined as:</p>

        <text class="center"><b>v1 + v2 = ( x1+x2, y1+y2, z1+z2 );</b></text>

        <p>The sum has a geometric meaning:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/Vectors%201.png?raw=true" alt="Vectors 1" />

        <p>Multiplication is more complicated. The obvious definition of the product of two vectors,
          similar to the definition of the sum, does not have geometric meaning and is rarely used.</p>

        <p>However, there are three kinds of vector multiplication that are used: the <b>scalar product</b>, the
          <b>dot product</b>, and the <b>cross product</b>.
        </p>

        <p>If v = (x,y,z ) is a vector and a is a number, then the <b>scalar product</b> of a and v is defined
          as:</p>

        <text class="center"><b>av = ( a*x, a*y, a*z );</b></text>

        <p>Assuming that a is positive and v is not zero, av is a vector that points in the same direction as
          v, whose length is a times the length of v. If a is negative, av points in the opposite direction
          from v, and its length is |a| times the length of v. </p>

        <p>This type of product is called a scalar product
          because a number like a is also referred to as a "scalar", perhaps because multiplication by a
          scales v to a new length.
        </p>

        <p>Given two vectors v1 = (x1,y1,z1 ) and v2 = (x2,y2,z2 ), the <b>dot product</b> of v1 and v2 is
          denoted by v1 ·v2 and is defined by:</p>

        <text class="center"><b>v1·v2 = x1*x2 + y1*y2 + z1*z2;</b></text>

        <p>Note that the dot product is a number, not a vector.</p>

        <p>The dot product has several very important
          geometric meanings. First of all, note that the length of a vector v is just the square root of
          v·v. Furthermore, the dot product of two non-zero vectors v1 and v2 has the property that:</p>

        <text class="center"><b>cos(angle) = v1·v2 / (|v1|*|v2|)</b></text>

        <p>where angle is the measure of the angle between v1 and v2.</p>

        <p> In particular, <b>in the case of two
            unit vectors, whose lengths are 1, the dot product of two unit vectors is simply the cosine
            of the angle between them</b>.</p>

        <p>Furthermore, since the cosine of a 90-degree angle is zero, two
          non-zero vectors are perpendicular if and only if their dot product is zero. Because of these
          properties, the dot product is particularly important in lighting calculations, where the effect
          of light shining on a surface depends on the angle that the light makes with the surface.</p>

        <p>The scalar product and dot product are defined in any dimension.</p>

        <p>For vectors in 3D, there
          is another type of product called the <b>cross product</b>, which also has an important geometric
          meaning. For vectors v1 = (x1,y1,z1) and v2 = (x2,y2,z2), the cross product of v1 and v2 is
          denoted v1×v2 and is the vector defined by:</p>

        <text class="center"><b>v1×v2 = ( y1*z2 - z1*y2, z1*x2 - x1*z2, x1*y2 - y1*x2 )</b></text>

        <p>If v1 and v2 are non-zero vectors, then v1×v2 is zero if and only if v1 and v2 point in the same
          direction or in exactly opposite directions.</p>

        <p>Assuming v1×v2 is non-zero, then it is perpendicular
          both to v1 and to v2 ; furthermore, the vectors v1, v2, v1×v2 follow the right-hand rule (in
          a right-handed coordinate system); that is, if you curl the fingers of your right hand from v1
          to v2, then your thumb points in the direction of v1×v2.</p>

        <p>If v1 and v2 are perpendicular unit
          vectors, then the cross product v1×v2 is also a unit vector, which is perpendicular both to v1
          and to v2.</p>

        <p>Finally, I will note that given two points P1 = (x1,y1,z1 ) and P2 = (x2,y2,z2 ), the difference
          P2−P1 is defined by:</p>

        <text class="center"><b>P2−P1 = ( x2−x1, y2−y1, z2−z1 )</b></text>

        <p>This difference is a vector that can be visualized as an arrow that starts at P1 and ends at P2.</p>

        <p>Now, suppose that P1, P2, and P3 are vertices of a polygon. Then the vectors P1−P2 and
          P3−P2 lie in the plane of the polygon, and so the cross product:</p>

        <text class="center"><b>(P3−P2) × (P1−P2)</b></text>

        <p>is a vector that is perpendicular to the polygon.</p>

        <img class="center" style="width: 100%; height: 100%" src="images/Vectors%202.png?raw=true" alt="Vectors 2" />

        <p>This vector is said to be a <b>normal vector</b> for the polygon.</p>

        <p>A normal vector of length one is
          called a <b>unit normal</b>. Unit normals will be important in lighting calculations, and it will be
          useful to be able to calculate a unit normal for a polygon from its vertices.</p>

        <h2>Matrices and Transformations</h2>

        <p>A matrix is just a two-dimensional array of numbers.</p>

        <p>A matrix with r rows and c columns is
          said to be an r -by-c matrix. If A and B are matrices, and if the number of columns in A is
          equal to the number of rows in B, then A and B can be multiplied to give the matrix product
          AB. If A is an n-by-m matrix and B is an m-by-k matrix, then AB is an n-by-k matrix. In
          particular, two n-by-n matrices can be multiplied to give another n-by-n matrix.</p>

        <p>An n-dimensional vector can be thought of an n-by-1 matrix. If A is an n-by-n matrix and
          v is a vector in n dimensions, thought of as an n-by-1 matrix, then the product Av is again an
          n-dimensional vector.</p>

        <p>The product of a 3-by-3 matrix A and a 3D vector v = (x,y,z) is often
          displayed like this:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/matrix%20product.png?raw=true"
          alt="Matrix Vector Multiplication">

        <p>Note that the i-th coordinate in the product Av is simply the dot product of the i-th row of
          the matrix A and the vector v.</p>

        <p>Using this definition of the multiplication of a vector by a matrix, a matrix defines a
          transformation that can be applied to one vector to yield another vector.</p>

        <p>Transformations
          that are defined in this way are <b>linear transformations</b>, and they are the main object of study
          in linear algebra. A linear transformation L has the properties that for two vectors v and w,
          L(v+w) = L(v) + L(w), and for a number s, L(sv) = sL(v).
        </p>

        <p>Rotation and scaling are linear transformations, but translation is <b>not</b> a linear transformation. </p>

        <p>To include translations, we have to widen our view of transformation to include affine
          transformations. An affine transformation can be defined, roughly, as a linear transformation
          followed by a translation.</p>

        <p>Geometrically, an affine transformation is a transformation that
          preserves parallel lines; that is, if two lines are parallel, then their images under an affine
          transformation will also be parallel lines.</p>

        <p>For computer graphics, we are interested in affine
          transformations in three dimensions. However—by what seems at first to be a very odd trick—
          we can narrow our view back to the linear by moving into the fourth dimension.</p>

        <p>Note first of all that an affine transformation in three dimensions transforms a vector
          (x1,y1,z1) into a vector (x2,y2,z2) given by formulas:</p>

        <text class="center"><b>x2 = a1*x1 + a2*y1 + a3*z1 + t1</b></text>
        <text class="center"><b>y2 = b1*x1 + b2*y1 + b3*z1 + t2</b></text>
        <text class="center"><b>z2 = c1*x1 + c2*y1 + c3*z1 + t3</b></text>

        <p>These formulas express a linear transformation given by multiplication by the 3-by-3 matrix:</p>

        <img class="center" style="width: 45%; height: 45%" src="images/3x3%20matrix%2001.png?raw=true"
          alt="3x3 Matrix">

        <p>followed by translation by t1 in the x direction, t2 in the y direction and t3 in the z direction.</p>

        <p>The trick is to replace each three-dimensional vector (x,y,z) with the four-dimensional vector
          (x,y,z,1), adding a "1" as the fourth coordinate. And instead of the 3-by-3 matrix, we use the
          4-by-4 matrix:
        </p>

        <img class="center" style="width: 50%; height: 50%" src="images/4x4%20matrix.png?raw=true" alt="4x4 Matrix">


        <p>If the vector (x1,y1,z1,1) is multiplied by this 4-by-4 matrix, the result is precisely the vector
          (x2,y2,z2,1). That is, instead of applying an affine transformation to the 3D vector (x1,y1,z1),
          we can apply a linear transformation to the 4D vector (x1,y1,z1,1).</p>

        <p>This might seem pointless to you, but nevertheless, that is what is done in OpenGL
          and other 3D computer graphics systems: <b>An affine transformation is represented as a 4-
            by-4 matrix in which the bottom row is (0,0,0,1), and a three-dimensional vector is changed
            into a four dimensional vector by adding a 1 as the final coordinate</b>.</p>

        <p>The result is that all
          the affine transformations that are so important in computer graphics can be implemented as
          multiplication of vectors by matrices.</p>

        <p>The identity transformation, which leaves vectors unchanged, corresponds to multiplication
          by the identity matrix, which has ones along its descending diagonal and zeros elsewhere.</p>

        <p>The OpenGL function glLoadIdentity() sets the current matrix to be the 4-by-4 identity matrix.</p>

        <p>An OpenGL transformation function, such as glTranslatef (tx,ty,tz), has the effect of multiplying
          the current matrix by the 4-by-4 matrix that represents the transformation.</p>

        <p>Multiplication is on the right; that is, if M is the current matrix and T is the matrix that represents the
          transformation, then the current matrix will be set to the product matrix MT.</p>

        <p>For the record,
          the following illustration shows the identity matrix and the matrices corresponding to various
          OpenGL transformation functions:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/all%20matrices.png?raw=true" alt="4x4 Matrix">


        <p>So if we want to translate the vector (10,10,10,1) of 10 units in the X direction, we get:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/translation%20matrix%20example.png?raw=true"
          alt="Translation Matrix Example">


        <p>and we get a (20,10,10,1) homogeneous vector! Remember, the 1 means that it is a position, not a direction.
          So our transformation didn't change the fact that we were dealing with a position, which is good.</p>

        <p>If we want to scale a vector (position or direction, it doesn't matter) by 2.0 in all directions:</p>


        <img class="center" style="width: 100%; height: 100%" src="images/scaling%20matrix%20example.png?raw=true"
          alt="Scaling Matrix Example">

        <p>and the w still didn't change. You may ask: what is the meaning of "scaling a direction"? Well, often, not
          much, so you usually don't do such a thing, but in some (rare) cases it can be handy.</p>


        <h2>Homogeneous Coordinates</h2>

        <p>There is one common transformation in computer graphics that is not an
          affine transformation: In the case of a perspective projection, the projection transformation is
          not affine.</p>

        <p>In a perspective projection, an object will appear to get smaller as it moves farther
          away from the viewer, and that is a property that no affine transformation can express, since
          affine transforms preserve parallel lines and parallel lines will seem to converge in the distance
          in a perspective projection.</p>

        <p>Surprisingly, we can still represent a perspective projection as a 4-by-4 matrix, provided
          we are willing to stretch our use of coordinates even further than we have already.</p>

        <p>We have already represented 3D vectors by 4D vectors in which the fourth coordinate is 1. We now
          allow the fourth coordinate to be anything at all, except for requiring that at least one of
          the four coordinates is non-zero. </p>

        <p>When the fourth coordinate, w, is non-zero, we consider the
          coordinates (x,y,z,w) to represent the three-dimensional vector (x/w,y/w,z/w). Note that this
          is consistent with our previous usage, since it considers (x,y,z,1) to represent (x,y,z), as before.</p>

        <p>When the fourth coordinate is zero, there is no corresponding 3D vector, but it is possible to
          think of (x,y,z,0) as representing a 3D “point at infinity” in the direction of (x,y,z).</p>

        <p>Coordinates (x,y,z,w) used in this way are referred to as <b>homogeneous coordinates</b>.</p>

        <p> If we use homogeneous coordinates, then any 4-by-4 matrix can be used to transform threedimensional vectors,
          including matrices whose bottom row is not (0,0,0,1).</p>

        <p>Among the transformations that can be represented in this way is the projection transformation for a
          perspective projection. And in fact, this is what OpenGL does internally.</p>

        <p>It represents all three-dimensional points and vectors using homogeneous coordinates, and it represents
          all transformations as 4-by-4 matrices. You can even specify vertices using homogeneous
          coordinates.</p>

        <p>For example, the command:</p>

        <text class="center"><b>glVertex4f(x,y,z,w);</b></text>

        <p>with a non-zero value for w, generates the 3D point (x/w,y/w,z/w). Fortunately, you will almost
          never have to deal with homogeneous coordinates directly (we'll talk more about that again later).</p>




















      </article>
      <br />
    </section>


    <hr />
    <section class="main-section" id="Intro_to_3D_Graphics">
      <br />
      <header><b>Intro to 3D Graphics</b></header>
      <article>
        <p>Nowadays 3D Computer graphics, or CG, are everywhere. From video games to medical applications.</p>

        <p>The film industry is dominated by computers, and it's not just sci-fi and animation. While filming the
          Irishman, Martin Scorsese used computer effects to de-age actors Robert De Niro, Joe Pesci, and Al Pacino.</p>

        <p>It's kind of crazy when you realize that the first feature film to incorporate computer generated imagery was
          Westworld, starring Yul Brynner and James Brolin. That was in 1973, more than 50 years ago!</p>

        <p>Today, we have devices like the Meta Quest 3 and the Apple Vision Pro, which blend digital content with your
          physical space.</p>

        <p>With all of this, it's clear that 3D computer graphics have become an integral component of our everyday
          lives. But, how are these computer graphics created?</p>

        <p>The process usually begins with an artist or designer using 3D modeling software like Maya, Cinema4D, or
          Blender, just to name a few.</p>

        <p>Artists usually start off with a simple shape like a box or a sphere and then use different tools to modify
          this geometry.</p>

        <p>If not working with an artist (or being an artist yourself), there are various Graphics APIs.</p>

        <p>Graphics APIs and pipelines are crucial in computer graphics. They enable developers to create visuals for
          video games, animations, or simulations. These systems are important for managing how images are drawn and
          displayed on the screen.</p>

        <h2>What are Graphics APIs?</h2>

        <p>A Graphics API (Application Programming Interface) is a set of functions that allows programs to perform
          operations like drawing images and 3D surfaces.</p>

        <p>Essentially, APIs provide a way for software to communicate with hardware. Graphics APIs focus specifically
          on creating and displaying visual content.</p>

        <p>Every graphics program requires two key types of APIs:</p>

        <ul>
          <li>A Graphics API to handle the visual output</li>
          <li>A User-Interface API to manage user input</li>
        </ul>

        <h2>Types of Graphics APIs</h2>

        <p>There are two approaches for using graphics APIs:</p>

        <ol>
          <li>Integrated Approach</li>
          <p>Some languages, such as Java, have built-in graphics and user-interface APIs. This ensures portability.
            Where the code will work across different systems. Everything is standardized, making development simpler
            for beginners.</p>
          <li>Library-Based Approach</li>
          <p>Direct3D and OpenGL are example of these types, where drawing commands are part of a software library. This
            approach is more powerful but can vary from system to system. It offers more control, portability can be an
            issue unless developers use an additional library to handle user interface differences.</p>
        </ol>

        <h2>Examples of Popular Graphics APIs</h2>

        <p>Direct3D and OpenGL are the most popular and widely used Graphics APIs.</p>

        <ul>
          <li><b>Direct3D</b> - It is used in video games, the Direct3D is a graphics API that belongs to the DirectX
            suite by
            Microsoft. It focuses on rendering 3D graphics, allowing games to render realistic environments. The popular
            game Halo uses Direct3D to render its 3D environments.</li>
          <li><b>OpenGL</b> - It is a cross-platform graphics API that is widely used for developing both 2D and 3D
            graphics.
            It is particularly popular in CAD applications and scientific visualization. The graphics in Google Earth
            use OpenGL to render detailed maps and 3D landscapes.</li>
        </ul>

        <h2>What are Graphics Pipelines?</h2>

        <p>After the graphics APIs, let's look at the concept of graphics pipelines.</p>

        <p>The following image shows a monkey head model created using the free and open source software Blender:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/monkey%20head%201.png?raw=true"
          alt="monkey head 1">

        <p>If we inspect the model closer, we can see that it's made up of a collection of points joined together with
          simple geometries.
          This structure of joined points is known as a <b>Mesh</b>.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/monkey%20head%202.png?raw=true"
          alt="monkey head 2">

        <p>Each individual point is known as a <b>vertex</b>. A vertex is a point in space that has coordinates x, y,
          and z which determine its position in the 3D world.</p>

        <p>So, meshes are made up of vertices and vertices are made up of coordinate values. But how do we go from three
          numerical values to something on the screen?</p>

        <p>A <b>graphics pipeline</b> is a sequence of stages that transform data from a mathematical representation to
          something on a screen.</p>

        <p>In essence, all 3D objects are just data. Data can live in a 3D space, however, our screen is not 3D.
          We need to take vertices in 3D space through a series of stages in order to transform them to a 2D space.</p>

        <p>In other words, these are the process that computers follow to create 3D images and display them on a 2D
          screen. It is a series of steps that take the input, such as the geometry of 3D models, and turn it into
          pixels on the screen.</p>

        <p>Let's start with the first stage: <b>the Vertex Shader</b>.</p>

        <h3>The Vertex Shader</h3>

        <p>This is the first step where the positions of 3D points (vertices) are calculated. These vertices form the
          basic structure of 3D objects. For example, if we are rendering a car in a racing game, the position of each
          vertex of the car model is calculated in this stage

        <p>Imagine we have a simple cube. This cube geometry is defined as a list of 8 vertices.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/the%20vertex%20shader.png?raw=true"
          alt="the vertex shader">

        <p>The vertex shader first transforms these vertices from a space where they are relative to their own origin,
          to a new space where everything is in relation to a camera's position and orientation.</p>

        <p>Finally, we take vertices from a 3D coordinate system into a 2D plane by using <b>perspective projection</b>.
        </p>

        <p>Projection creates the illusion of depth by scaling an object's coordinates based on their distance from the
          camera. This simulates how objects appear smaller as they move away from the viewer.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/perspective%20projection.png?raw=true"
          alt="perspective projection">

        <p>Now that our vertices have been transformed to a 2D space, it's time for the second stage: <b>the primitive
            assembly</b>.</p>

        <h3>The Primitive (or Triangle) Assembly</h3>

        <p>Once vertices have been transformed, it's time for the primitive assembly.</p>

        <p>In this stage, vertices are connected through geometric primitives. We can choose to use lines, points, or
          triangles.</p>

        <p>Most modern graphics hardware are optimized to process triangles efficiently, that is why we usually choose
          them over lines and points.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/primitive%20assembly.gif?raw=true"
          alt="primitive assembly">

        <p>The primitive assembly lays the groundwork for the next step, which is <b>rasterization</b>.</p>

        <h3>Rasterization</h3>

        <p>The next important step is rasterization. This is where the 3D models are converted into pixels or fragments.
          These pixels represent the final image that will appear on the screen. For example, the 3D model of a tree in
          a game is converted into a collection of colored pixels that represent the tree on the screen.</p>

        <p>We now have geometry made up of 3 dimensional vertices projected onto a 2D screen.</p>

        <p>The rasterization step determines which pixels are inside the triangle. It breaks the shape into small
          fragments or pixels.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/rasterization.png?raw=true" alt="rasterization">

        <p>Rasterization not only determines which pixels are inside the shape, but also improves performance by
          discarding triangles which are not visible or occluded. This is known as <b>Culling</b>.</p>

        <p>Culling determines which triangles are facing away from the camera, are outside of the camera's view, or
          simply hidden by another object.</p>

        <p>In this case the triangle gets discarded and the computational load on the GPU (Graphical Processing Unit )
          is reduced.</p>

        <h3>The Fragment Shader</h3>

        <p>The next stage in the pipeline is known as the fragment shader.</p>

        <p>After rasterization, the triangle is broken into pixel fragments. These individual pixel fragments are then
          processed by the fragment shader.</p>

        <img class="center" style="width: 75%; height: 75%" src="images/fragment%20shader.png?raw=true"
          alt="the fragment shader">

        <p>In this step we determine a color for each of the pixels that belong to the triangle. We do this by taking
          into account the material properties of the object, textures, and light sources in the scene.</p>

        <p>With all of these information, we perform some calculations that determine the final color of the fragment.
        </p>

        <h3>Framebuffer</h3>

        <p>Finally, all of the shaded fragments are copied to the framebuffer, which is basically the image displayed on
          the screen.</p>

        <p>You can think of the framebuffer as a canvas where everything gets stored before being displayed on the
          screen.</p>

        <img class="center" style="width: 90%; height: 90%" src="images/the%20graphics%20pipeline.png?raw=true"
          alt="the graphics pipeline">

        <p>This is a general overview of a typical graphics pipeline. Note that there may be different pipelines, which
          have additional steps that we ignored.</p>

        <p>Earlier versions of graphics libraries offered "fixed" pipeline. This means that the process had predefined
          stages and operations. Specifically, the vertex and fragment shaders. Developers had limited control over how
          these specific stages behaved.</p>

        <p>With older pipelines, we would simply define the vertex data, material data, and lighting. Then, we would
          send this data to the GPU and we'd get a result. This was easier for beginners, but provided little room for
          modification and customization. For example, the lighting model used could not be changed.</p>

        <p>In modern graphics libraries, "fixed" pipelines are deprecated in favor of "programmable" pipelines. Modern
          pipelines provide greater flexibility and control over the rendering process. However, you must provide the
          code for the vertex and fragment shader yourself. This means more control for the developer, but an additional
          layer of difficulty for beginners.</p>

        <p>Shader programs are written in a C style language called GLSL (OpenGL Shading Language). Shader programs run
          on the GPU, which is different from how programs run on the CPU. The CPU executes tasks sequentially, while
          the GPU executes tasks in parallel.</p>

        <p>The fact is that this extra layer is worth it! We can achieve great results with these new pipelines. There
          is also a thriving community of creative individuals that are constantly pushing this technology even further.
        </p>





      </article>
      <br />
    </section>




    <hr />
    <section class="main-section" id="Graphic_APIs">
      <br />
      <header><b>Graphic APIs</b></header>
      <article>
        <p></p>

        <h2>OpenGL</h2>


        <p>OpenGL is generally considered to be a cross-platform graphics API for high-performance 2D and 3D graphics
          rendering. In fact, OpenGL is not an API. It is a specification developed and maintained by the Khronos
          organization.</p>

        <p>The specification strictly defines how each function should be executed and what their outputs should be. How
          each function is implemented internally is up to the developer.</p>



        <p>It is widely used in video games, CAD, virtual reality, scientific visualization, and more.</p>

        <h2>WebGL</h2>

        <p>WebGL (Web Graphics Library) is a JavaScript API for rendering interactive 2D and 3D graphics within any
          compatible web browser without the use of plug-ins.</p>

        <p>It is based on OpenGL ES (a subset of OpenGL for embedded systems).</p>

        <p>Major browser vendors Apple (Safari), Google (Chrome), Microsoft (Edge), and Mozilla (Firefox) are members of
          the WebGL Working Group.</p>

        <h2>Three.js</h2>

        <p>Three.js is a popular JavaScript library that simplifies the creation and display of 3D graphics in a web
          browser using WebGL.</p>

        <p>It supports VR and AR, offers cross-browser compatibility via WebGL, provides extensive tools for adding
          materials, textures, and animations, and allows for the integration of models from other 3D modeling software.
        </p>

        <h3>Key Features</h3>

        <ul>
          <li><b>Scene Graph</b>: It uses a scene graph structure, allowing developers to create and manage 3D objects,
            cameras, lights, and other elements in a hierarchical manner.</li>
          <li><b>Geometries and Materials</b>: Three.js provides a variety of built-in geometries (e.g., cubes, spheres,
            planes) and materials (e.g., basic, lambert, phong, standard) that can be easily customized and combined.
          </li>
          <li><b>Animation</b>: The library supports animations, including skeletal animations, morph targets, and
            keyframe animations, making it suitable for creating animated 3D content.</li>
          <li><b>Shaders and Post-Processing</b>: Three.js allows the use of custom shaders written in GLSL and supports
            post-processing effects such as bloom, depth of field, and motion blur.</li>
        </ul>

      </article>
      <br />
    </section>

    <hr />
    <section class="main-section" id="Intro_to_Three.js">
      <br />
      <header><b>Intro to Three.js</b></header>
      <article>
        <p>All modern browsers became more powerful and more accessible directly using JavaScript. They have adopted
          WebGL (Web Graphics Library) and as previously mentioned, WebGL is a Javascript API that allows you to render
          2D and 3D graphics without the use of plugins.
          Trying to create 3D elements with WebGL would involve writing lots of code and can get fairly complex.</p>

        <p>Three.js helps simplify the process and is essentially an abstracted layer on top of WebGL that makes it
          easier to use.</p>

        <p>Three.js is an open-source, lightweight, cross-browser, general-purpose JavaScript library. Three.js uses
          WebGL behind the scenes, so you can use it to render Graphics on an HTML canvas element in the browser.</p>

        <p>Since Three.js uses JavaScript, you can interact with other web page elements, add animations and
          interactions,
          and even create a game with some logic.</p>

        <p>Three.js works with the HTML canvas element, the same thing that we used for 2D graphics. In almost all web
          browsers, in addition to its 2D Graphics API, a canvas also
          supports drawing in 3D using WebGL, which is used by three.js and which is about as different
          as it can be from the 2D API.</p>

        <h2>Why use Three.js?</h2>

        <p>The following features make Three.js an excellent library to use:</p>

        <ul>
          <li>You can create complex 3D graphics by just using JavaScript.</li>
          <li>You can create Virtual Reality (VR) and Augmented Reality (AR) scenes inside the browser.</li>
          <li>Since it uses WebGL, it has cross-browser support. Many browsers support it.</li>
          <li>You can add various materials, textures and animate 3D objects.</li>
          <li>You can also load and work on objects from other 3D modeling software.</li>
        </ul>

        <p>With a couple of lines of JavaScript and simple logic, you can create anything, from highperformance
          interactive 3D models to photorealistic real-time scenes.</p>

        <p>We'll start by adding the following to our HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Before you can use three.js, you need somewhere to display it.</p>
        <p>To actually be able to display anything with three.js, we need three things:</p>

        <ul>
          <li>Scene</li>
          <li>Camera</li>
          <li>Renderer</li>
        </ul>

        <pre><code class="javascript">const scene = new THREE.Scene(); 
const camera = new THREE.PerspectiveCamera(75, 500 / 400, 0.1, 1000);  
const renderer = new THREE.WebGLRenderer(); 
renderer.setSize(500, 400); 
document.body.appendChild(renderer.domElement);</code></pre>

        <p>Now, let's break down each of these components.</p>

        <h2>Scene</h2>

        <p>A scene is the container object that includes everything we will be rendering, such as geometries, lights,
          and cameras. In a sense, it's much like a stage where you place all the elements you want to display.</p>

        <p>We create an instance of the scene object using the constructor with no parameters: <b>THREE.Scene()</b></p>

        <p>The function scene.add(item) can be used to add cameras, lights, and graphical objects to
          the scene. It is probably the only scene function that you will need to call. The function
          scene.remove(item), which removes an item from the scene, is also occasionally useful.</p>

        <h2>Camera</h2>

        <p>Next, we create a camera which will be how we view the scene.</p>

        <p>The camera displays the objects in the scene, with varying points of view depending on which type of camera
          is used.</p>

        <p>The Three.js library provides two main cameras: <b>Orthographic</b> and <b>Perspective</b>.</p>

        <ul>
          <li><b>OrthographicCamera</b>: uses orthographic projection, meaning that elements viewed with this camera
            <b>maintain a constant size, regardless of their distance from the camera</b>. <br /><br />
            This can be popular in top down 2D games like SimCity and UI elements, amongst other
            things.
          </li>
          <br />
          <li><b>PerspectiveCamera</b>: uses perspective projection. This type of camera is designed to mimic the
            way the human eye sees. <b>So the closer an object is, the larger it will appear and the farther it is, the
              smaller it will appear</b>.
            <br /><br />It is the most common projection mode used for rendering a 3D scene.
          </li>
        </ul>

        <img class="center" style="width: 75%; height: 75%" src="images/orthographic%20v%20perspective.png?raw=true"
          alt="orthographic vs perspective">

        <p>In a perspective view (left), <b>edges that are farther away appear shorter</b>.</p>

        <p>In an Orthographic view (right), <b>far-away edges are the same size as nearby ones</b>.</p>

        <p>The constructors specify
          the projection, using parameters that are familiar from OpenGL:</p>

        <text><b>camera = new THREE.OrthographicCamera(left, right, top, bottom, near, far);</b></text>

        <p>or: </p>

        <text><b>camera = new THREE.PerspectiveCamera(fieldOfViewAngle, aspect, near, far);</b></text>

        <p>Since the PerspectiveCamera is more common, let's go over its four attributes:</p>

        <ul>
          <li>Field of View (FOV)</li>
          <li>Aspect</li>
          <li>Near</li>
          <li>Far</li>
        </ul>

        <p>The first parameter it takes is the <b>field of view (FOV)</b>, <b>the maximum area of the scene that can be
            viewed</b>. To help visualize, picture a cone coming out of a camera lens, and the flat end of the cone is
          the space the camera can see. This value is measured in degrees.</p>

        <p>Next is the <b>aspect ratio</b>, which is the proportion of the width to the height of the element.</p>

        <p>You almost always want to use <b>the width of the element divided by the height</b>, or you'll get the same
          result as
          when you play old movies on a widescreen TV - the image looks squished.</p>

        <p>The <b>near and far attributes</b> are the <b>minimum and maximum distance from the camera at which the
            camera will render scene objects</b>.
          Anything beyond these values, either too close or too far from the camera, will not be rendered.</p>

        <p>Finally, once the camera is set up, you need to add it to the scene.</p>

        <p>Remember to append everything to your container to make it show on your scene.</p>

        <p>Keep in mind that by default, when you add something to the scene, it will be placed in the center at the XYZ
          coordinates 0, 0, 0 respectively. So if you have trouble seeing anything you have added render in the scene,
          chances are you might need to position your camera further out of view.</p>

        <p>For example, setting the camera's Z coordinate to 5:</p>

        <pre><code class="javascript">camera.position.z = 5;</code></pre>

        <h2>Renderer</h2>

        <p>The last step is to render the scene. The renderer is what compiles it all together and draws the scene.</p>

        <p>We need to create the renderer instance, we will use WebGLRenderer() but
          three.js comes with others as well, often used as fallbacks for users with
          older browsers/who don't have WebGL support.</p>

        <p>We also need to set the size at which we want it to render our app.</p>

        <p>In the following examples we will use a width of 500 and a height of 400, though it is
          common to use the width and height of the browser window (window.innerWidth, window.innerHeight) as well.</p>

        <p>Lastly, we add the renderer element to our HTML document. This is a canvas element the renderer uses to
          display the scene to us.</p>

        <p>A renderer is an instance of the class THREE.WebGLRenderer. Its constructor has one
          parameter, which is a JavaScript object containing settings that affect the renderer.</p>

        <p>All together our code should look something like this:</p>

        <pre><code class="javascript">const scene = new THREE.Scene(); //Creates a new instance of the Scene class.
const camera = new THREE.PerspectiveCamera(50, 500 / 400, 0.1, 1000); //Creates a new instance of the PerspectiveCamera class with a field of view of 50 degrees, an aspect ratio of 500/400, a near clipping plane of 0.1, and a far clipping plane of 1000.
camera.position.z = 5; //Moves the camera 5 units away from the center of the scene.
const renderer = new THREE.WebGLRenderer(); //Creates a new instance of the WebGLRenderer class.
renderer.setSize(500, 400); //Sets the size of the renderer to 500px by 400px.
document.body.appendChild(renderer.domElement); //Appends the renderer's canvas element to the document body to display the rendered scene.</code></pre>

        <p>However, just defining an instance of the renderer is not enough to see the results of our code yet, because
          we have not actually rendered anything as of
          yet.</p>

        <p>To achieve this, we will need to create a render or animate loop, like this:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}

animate();</code></pre>

        <p>We have now created a loop named animate() that causes the renderer to
          draw the scene every time the screen is refreshed (on a typical screen, this is
          about 60 times per second), and called it immediately after.</p>

        <p>Using requestAnimationFrame() has a number of advantages, such as
          pausing when the user navigates to another browser tab, saving processing
          power and battery life.</p>

        <p>Now we've successfully set up out scene! Now for the fun part, we can start creating the shapes that we would
          like
          to add to our scene.</p>
        <p>Much like how we need three elements to set a scene, objects require three elements:
        </p>

        <ul>
          <li>Geometry</li>
          <li>Material</li>
          <li>Mesh</li>
        </ul>

        <p>Let's start with the first element, the geometry.</p>

        <h2>Geometry</h2>

        <p>The geometry defines the shape of the objects we draw in Three.js.</p>

        <p>It is made up of a collection of vertices and faces which combine
          three vertices into a triangle face.</p>

        <p>There are two main types of geometries in Three.js:</p>

        <ul>
          <li><b>Custom Geometry</b>: created by defining the vertices and faces of an object from
            scratch (by using THREE.BufferGeometry())</li>
          <br />
          <li><b>Built-in Geometry</b>: the Three.js library provides a number of different geometries you can use and
            customize yourself. Each geometry has its own set of parameters. (i.e. a BoxGeometry accepts attributes for
            width, height, and depth while a SphereGeometry's parameters are radius, widthSegments, and heightSegments,
            etc.)</li>
        </ul>

        <img class="center" style="width: 80%; height: 80%" src="images/three%20js%20geometries.png?raw=true"
          alt="geometry">

        <p>Here are some constructors, listing all
          parameters (but keep in mind that most of the parameters are optional):</p>

        <ul>
          <li>new THREE.BoxGeometry(width, height, depth,
            widthSegments, heightSegments, depthSegments)</li>
          <li>new THREE.PlaneGeometry(width, height, widthSegments, heightSegments)</li>
          <li>new THREE.RingGeometry(innerRadius, outerRadius, thetaSegments, phiSegments,
            thetaStart, thetaLength)
          </li>
          <li>new THREE.ConeGeometry(radiusBottom, height, radiusSegments,
            heightSegments, openEnded, thetaStart, thetaLength)</li>
          <li>new THREE.SphereGeometry(radius, widthSegments, heightSegments,
            phiStart, phiLength, thetaStart, thetaLength)</li>
          <li>new THREE.TorusGeometry(radius, tube, radialSegments, tubularSegments, arc)
          </li>
        </ul>

        <p>The class <b>BoxGeometry</b> represents the geometry of a rectangular box centered at the origin.</p>

        <p>Its constructor has three parameters to give the size of the box in each direction; their default
          value is one.</p>

        <p>The last three parameters give the number of subdivisions in each direction, with
          a default of one; values greater than one will cause the faces of the box to be subdivided into
          smaller triangles.</p>

        <p>The class <b>PlaneGeometry</b> represents the geometry of a rectangle lying in the xy-plane,
          centered at the origin. Its parameters are similar to those for a cube. </p>

        <p>A <b>RingGeometry</b> represents an annulus, that is, a disk with a smaller disk removed from its center. The
          ring lies in the
          xy-plane, with its center at the origin. You should always specify the inner and outer radii of
          the ring.</p>

        <p>The constructor for <b>ConeGeometry</b> has exactly the same form and effect as the constructor
          for <b>CylinderGeometry</b>, with the radiusTop set to zero. That is, it constructs a cone with axis
          along the y-axis and centered at the origin</p>

        <p>For <b>SphereGeometry</b>, all parameters are optional.</p>

        <p>The constructor creates a sphere centered
          at the origin, with axis along the y-axis. </p>

        <p>The first parameter, which gives the radius of the
          sphere, has a default of one.</p>

        <p>The next two parameters give the numbers of slices and stacks,
          with default values 32 and 16.</p>

        <p> The last four parameters allow you to make a piece of a sphere;
          the default values give a complete sphere. The four parameters are angles measured in radians.
          phiStart and phiLength are measured in angles around the equator and give the extent in
          longitude of the spherical shell that is generated.</p>

        <p>For example:</p>

        <text><b>new THREE.SphereGeometry(5, 32, 16, 0, Math.PI)</b></text>

        <p>creates the geometry for the "western hemisphere" of a sphere.</p>

        <p>The last two parameters are
          angles measured along a line of latitude from the north pole of the sphere to the south pole.</p>

        <p>For example, to get the sphere's "northern hemisphere":</p>

        <text><b>new THREE.SphereGeometry(5, 32, 16, 0, 2*Math.PI, 0, Math.PI/2)</b></text>

        <p>For <b>TorusGeometry</b>, the constructor creates a torus lying in the xy-plane, centered at the
          origin, with the z -axis passing through its hole.</p>

        <p>The parameter radius is the distance from the
          center of the torus to the center of the torus's tube, while tube is the radius of the tube. The
          next two parameters give the number of subdivisions in each direction.</p>

        <p>The last parameter,
          arc, allows you to make just part of a torus. It is an angle between 0 and 2*Math.PI, measured
          along the circle at the center of the tube.</p>

        <p>There are also geometry classes representing the regular polyhedra: THREE.TetrahedronGeometry,
          THREE.OctahedronGeometry, THREE.DodecahedronGeometry, and THREE.IcosahedronGeometry.
          (For a cube use a BoxGeometry.) </p>

        <p>The constructors for these four classes take two parameters.
          The first specifies the size of the polyhedron, with a default of 1. The size is given as the radius
          of the sphere that contains the polyhedron. </p>

        <p>The second parameter is an integer called detail.
          The default value, 0, gives the actual regular polyhedron. Larger values add detail by adding
          additional faces. </p>

        <p>As the detail increases, the polyhedron becomes a better approximation for a
          sphere. This is easier to understand with an illustration:</p>

        <!--ADD IMAGE HERE-->

        <img class="center" style="width: 100%; height: 100%" src="images/mesh%20objects%20example.png?raw=true"
          alt="icosahedral geometries">

        <p>The image shows four mesh objects that use icosahedral geometries with detail parameter equal
          to 0, 1, 2, and 3.</p>

        <h2>Material</h2>

        <p>Materials describe the appearance of objects.</p>

        <p>They are defined in a (mostly) renderer-independent way, so you don't have
          to rewrite materials if you decide to use a different renderer.</p>

        <p>There are different types of materials:</p>

        <ul>
          <li><b>MeshBasicMaterial</b>: This is the most boring material you can get in Three.js world, but the benefit
            is, it doesn't need a light for it to show.
            <br /><br />
            Just pass a color in as a parameter to get a solid colored
            object, which has no shading and is not affected by lights
          </li>
          <br />
          <li><b>MeshNormalMaterial</b>: colors the faces of the mesh differently based on the
            face's normal or what direction they are facing</li>
          <p>The next materials however, DO require lights in order to be seen:</p>
          <li><b>MeshLambertMaterial</b>: responds to lights and gives our geometry shading
            with a dull surface, computes lighting only at the vertices</li>
          <br />
          <li><b>MeshPhongMaterial</b>: similar to Lambert, responds to lights but adds a
            metallic luster to the surface, reflecting light with more intensity, computes
            lighting at every pixel</li>
          <br />
          <li><b>MeshStandardMaterial</b>: combines Lambert and Phong into a single material,
            has properties for roughness and metalness and adjusting these can create
            both dull or metallic looking surfaces
          </li>
          <br />
          <li><b>MeshDepthMaterial</b>: draws the mesh grayscale from black to white based on
            the depth of the content</li>
          <br />
          <li><b>MeshToonMaterial</b>: toon shading or cel shading is a type of non-photorealistic rendering technique
            designed to make 3D computer graphics appear more cartoonish by using less shading color instead of a smooth
            gradient effect.</li>
        </ul>

        <img class="center" style="width: 80%; height: 80%" src="images/three%20js%20materials.png?raw=true"
          alt="material">

        <h2>Mesh</h2>

        <p>A Mesh is a class representing triangular polygon mesh based objects.</p>

        <p>A Mesh pairs a Geometry and a Material in order to draw/create an object.</p>

        <p>Both Material objects and Geometry objects can be used by multiple Mesh
          objects.</p>

        <img class="center" style="width: 80%; height: 80%" src="images/three%20js%20mesh.png?raw=true" alt="mesh">

        <p>Now that we have our scene, camera, renderer, geometry, material, and mesh, we can start creating our
          objects.</p>

        <h2>Example: Creating a Basic Triangle</h2>

        <p>Although we'll mostly focus on 3D objects in Three.js, let's start with a simple 2D triangle.</p>

        <p>We can create this triangle using BufferGeometry:</p>

        <pre><code class="javascript">var size = 2;

function createTriGeometry(size) {
    const triGeom = new THREE.BufferGeometry();

    // Define the vertices of the triangle
    const vertices = new Float32Array([
        0, 0, 0, // Vertex 1
        size, 0, 0, // Vertex 2
        0, size, 0  // Vertex 3
    ]);

    // Set the vertices as attribute
    triGeom.setAttribute('position', new THREE.BufferAttribute(vertices, 3));

    // Define the indices for the single triangle face
    const indices = new Uint16Array([
        0, 1, 2 // A single face made up of the three vertices
    ]);

    // Set the indices
    triGeom.setIndex(new THREE.BufferAttribute(indices, 1));

    return triGeom;
}

// Create the triangle geometry
var triGeom1 = createTriGeometry(size);

// Define the material for the triangle
var triMaterial = new THREE.MeshBasicMaterial({ 
    color: new THREE.Color("red"), 
    side: THREE.DoubleSide // Render both sides of the triangle
});

// Create the mesh with geometry and material
var triMesh1 = new THREE.Mesh(triGeom1, triMaterial);

// Add triangle to the scene
scene.add(triMesh1);</code></pre>

        <p>In this code we:</p>

        <ul>
          <li>Create a BufferGeometry object for the triangle.</li>
          <li>Define the vertices of the triangle and set them as an attribute of the geometry.</li>
          <li>Define the indices for the single triangle face and set them.</li>
          <li>Create a MeshBasicMaterial object for the triangle.</li>
          <li>Create a Mesh object by passing in the geometry and material objects, and add it to the scene.</li>
        </ul>

        <p>Now, when we run the code, we should see a red triangle rendered in the scene.</p>

        <p>Let's adjust the background color:</p>

        <pre><code class="javascript
">scene.background = new THREE.Color("blue");</code></pre>

        <p>Now our red triangle should be displayed on a blue background.</p>

        <h2>Example: Creating a Simple Cube</h2>

        <p>Let's create a simple square using BufferGeometry:</p>

        <pre><code class="javascript">const geometry = new THREE.BufferGeometry();
// create a simple square shape. We duplicate the top left and bottom right
// vertices because each vertex needs to appear once per triangle.
const vertices = new Float32Array( [
	-1.0, -1.0,  1.0, // v0
	 1.0, -1.0,  1.0, // v1
	 1.0,  1.0,  1.0, // v2

	 1.0,  1.0,  1.0, // v3
	-1.0,  1.0,  1.0, // v4
	-1.0, -1.0,  1.0  // v5
]);

geometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));
const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
const cube = new THREE.Mesh(geometry, material);

scene.add(cube);</code></pre>

        <p>Now, when you run the code, you should see a green square (cube) rendered in the scene.</p>

        <p>Here we create a BufferGeometry object and pass in an array
          of vertices that define the square's shape.</p>

        <p>Next, we create a MeshBasicMaterial object and pass in a color
          parameter to give the square a green color.</p>

        <p>Finally, we create a Mesh object by passing in the geometry and
          material objects, and add it to the scene.</p>

        <p>Notice this only makes a 2D square out of two triangles, but we can
          make a 3D cube by adding more vertices and faces.</p>

        <pre><code class="javascript">// Define vertices for a cube
const vertices = new Float32Array([
    // Front face
    -1.0, -1.0,  1.0,
     1.0, -1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0, -1.0,  1.0,

    // Back face
    -1.0, -1.0, -1.0,
    -1.0,  1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0, -1.0, -1.0,
    -1.0, -1.0, -1.0,

    // Top face
    -1.0,  1.0, -1.0,
    -1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0, -1.0,
    -1.0,  1.0, -1.0,

    // Bottom face
    -1.0, -1.0, -1.0,
     1.0, -1.0, -1.0,
     1.0, -1.0,  1.0,
     1.0, -1.0,  1.0,
    -1.0, -1.0,  1.0,
    -1.0, -1.0, -1.0,

    // Right face
     1.0, -1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0, -1.0,  1.0,
     1.0, -1.0, -1.0,

    // Left face
    -1.0, -1.0, -1.0,
    -1.0, -1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0,  1.0, -1.0,
    -1.0, -1.0, -1.0
]);

// Create geometry and material
const geometry = new THREE.BufferGeometry();
geometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));

const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });

// Create cube mesh
const cube = new THREE.Mesh(geometry, material);
scene.add(cube);</code></pre>

        <p>Now we should be able to see a cube since we have defined the vertices of each of the six faces.</p>

        <p>Understanding this is important, although not entirely necessary since we are merely creating a cube.
          We can simplify this process by using BoxGeometry()
          instead of BufferGeometry().</p>

        <p>BoxGeometry is a geometry class for a rectangular cuboid with a given 'width', 'height',
          and 'depth'. How can we obtain the same scaled 2D shape using BoxGeometry()?</p>

        <p>To create a cube without declaring all vertices, we use BoxGeometry. This is an object that contains all the
          points (vertices) and fill (faces) of the cube.</p>

        <p>Let's set the width, height, and depth to 2 respectively, because our previous square had a width, height,
          and depth of 2 as well! (if we do not set it at all, the w/h/d defaults to 1, try changing the values! How
          does this change the object?)</p>

        <pre><code class="javascript">const geometry = new THREE.BoxGeometry(2, 2, 2);</code></pre>

        <p>Notice how since our structure is a simple cube we have no need for a
          group of vertices since the same result can be obtained in one line of
          code!</p>

        <p>We'll definitely use vertices for more irregular polygons but for
          now, BoxGeometry works just fine!</p>

        <p>Let's add X and Y-axis rotation to our cube. We can do this simply by adding
          the following to animate():</p>

        <pre><code class="javascript">//Now to make it more interesting and dimensional, let's add a rotation animation
cube.rotation.x += 0.01;
cube.rotation.y += 0.01;</code></pre>

        <p>We can increase/decrease the speed of rotation by changing the values of
          cube.rotation.x and cube.rotation.y</p>

        <p>Now, when you run the code, you should see the cube rotating on both the X and Y axes.</p>

        <p>Let's make a multi-colored cube:</p>

        <p>We can achieve this using an array of materials.</p>

        <pre><code class="javascript
">const materials = [
    new THREE.MeshBasicMaterial({ color: "green" }),
    new THREE.MeshBasicMaterial({ color: "blue" }),
    new THREE.MeshBasicMaterial({ color: "red" }),
    new THREE.MeshBasicMaterial({ color: "yellow" }),
    new THREE.MeshBasicMaterial({ color: "orange" }),
    new THREE.MeshBasicMaterial({ color: "purple" })
];</code></pre>

        <p>Now, when we run the code, we should see a cube with different colored faces.</p>

        <p>Let's revert back to a singularly colored cube.</p>

        <p>We can also change our cube to simply display the wireframe of our geometry by modifying our material:</p>

        <pre><code class="javascript
">const material = new THREE.MeshBasicMaterial({ color: "green", wireframe: true });</code></pre>

        <p>Now let's change the material of our cube to MeshNormalMaterial:</p>

        <pre><code class="javascript">const material = new THREE.MeshNormalMaterial();</code></pre>

        <p>Now, let's look at a wireframe to our cube:</p>

        <pre><code class="javascript">const wireframe = new THREE.WireframeGeometry(geometry);
const line = new THREE.LineSegments(wireframe);
line.material.depthTest = false;
line.material.opacity = 1;
line.material.transparent = true;
scene.add(line);</code></pre>

        <p>This is good if we want a wireframe in addition to our solid object.</p>

        <p>If we want it to rotate remember to update the rotations as well.</p>

        <h2>Additional Tools</h2>

        <p>A helpful tool to implement when creating your projects is <b>dat.GUI</b>.</p>

        <p>In Javascript, dat.GUI is a lightweight graphics controller API. It allows the user to manipulate variables
          easily and activate functions inside the application by providing a graphical controller interface.</p>

        <p>This API is frequently used in Three.js for making changes to a scene without having to refer directly to the
          code.</p>

        <p>It will be displayed as a small box in the corner of your screen and you can play around with the parameters
          and see the newly applied changes happen live.</p>

        <p>It's a great tool to use when you want to experiment with different values and see how they affect your
          project.</p>

        <p>Here's how we can add dat.GUI to our project:</p>

        <p>First, we need to ensure that we have access to dat.GUI by adding this line to our HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Next, we can add the following code to our JavaScript:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
gui.add(cube.rotation, 'x', 0, Math.PI * 2);
gui.add(cube.rotation, 'y', 0, Math.PI * 2);</code></pre>

        <p>Now, when we run the code, we should see the dat.GUI box in the corner of our screen.</p>

        <p>By changing the values of the sliders, we can adjust the rotation of the cube on the X and Y axes.</p>

        <p>And that's it! We've successfully added dat.GUI to our project.</p>

        <p>Let's add a color changer to our dat.GUI as well:</p>

        <pre><code class="javascript">// Add GUI for controls
const gui = new dat.GUI();
const cubeFolder = gui.addFolder('Cube Properties');

const cubeParams = {
    color: `#${material.color.getHexString()}`,
    rotationX: 0,  // New parameter for X rotation angle
    rotationY: 0   // New parameter for Y rotation angle
};

// Color control
cubeFolder.addColor(cubeParams, 'color').onChange((value) => {
    cube.material.color.set(value);
});

// X rotation control
cubeFolder.add(cubeParams, 'rotationX', -Math.PI, Math.PI).name('Rotation X').onChange((value) => {
    cube.rotation.x = value;
});

// Y rotation control
cubeFolder.add(cubeParams, 'rotationY', -Math.PI, Math.PI).name('Rotation Y').onChange((value) => {
    cube.rotation.y = value;
});

cubeFolder.open();</code></pre>

        <h2>Exercise: Let's Create a Sphere: </h2>

        <p>Now that we've created a cube, let's create a sphere using SphereGeometry().</p>

        <pre><code class="javascript">const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(50, 500 / 400, 0.1, 1000);
const renderer = new THREE.WebGLRenderer();
renderer.setSize(500, 400);
document.body.appendChild(renderer.domElement);

camera.position.z = 7;

//sphere:
const material = new THREE.MeshBasicMaterial({
    color: "orange"
});
const geometry = new THREE.SphereGeometry(2, 50, 50); //radius, widthSegments, heightSegments
const sphere = new THREE.Mesh(geometry, material);
scene.add(sphere);

function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}

animate();</code></pre>

        <p>We should see a basic Three.js code snippet creating an orange sphere on canvas. Nothing fancy. The sphere is
          made from a basic material which does not respond to light, therefore we can see the model as is.</p>

        <p>We can turn this sphere into a wireframe like we did before, by using WireframeGeometry or we can simply add
          another specification to our original material.</p>

        <p>Let's add a wireframe to our sphere:</p>

        <pre><code class="javascript">const material = new THREE.MeshBasicMaterial({
    color: "orange",
    wireframe: true //sets the wireframe
});</code></pre>

        <p>Now, when we run the code, we should see an orange wireframe sphere rendered in the scene.</p>

        <p>Let's rotate it by adding the following lines to our animate function:</p>

        <pre><code class="javascript">sphere.rotation.x += 0.01;
sphere.rotation.y += 0.01;</code></pre>

        <p>Now, when you run the code, you should see the sphere rotating on both the X and Y axes.</p>

        <p>Let's add dat.GUI to our sphere that only includes a checkbox that helps us toggle from our original orange
          color to a blue wireframe:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
const sphereFolder = gui.addFolder('Sphere Properties');

const sphereParams = {
    blue: false
};

// Wireframe color control
sphereFolder.add(sphereParams, 'blue').name('blue wireframe').onChange((value) => {
    if (value) {
        sphere.material.color.set('blue');    
    } else {
        sphere.material.color.set('orange');    
    }
});

sphereFolder.open(); //to start with the folder open</code></pre>

        <p>Now when you run the code, you should see a checkbox in the dat.GUI box that allows you to toggle between the
          original orange color and a blue wireframe for the sphere.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Lights_and_Interactivity">
      <br />
      <header><b>Lights</b></header>
      <article>

        <p>So we've seen objects with simple materials like MeshBasicMaterial and MeshNormalMaterial that did not need
          lighting, however
          to render a scene realistically we need to add lights.</p>

        <p>There are several types of lights in Three.js:</p>

        <ul>
          <li><b>AmbientLight</b>: This light is used to simulate global illumination. It lights up all objects in the
            scene equally.</li>
          <li><b>HemisphereLight</b>: This light is used to simulate the light coming from the sky. It is used to create
            a gradient sky color.</li>
          <li><b>DirectionalLight</b>: This light is used to simulate light that is coming from a specific direction. It
            is similar to sunlight.</li>
          <li><b>PointLight</b>: This light is used to simulate light that is coming from a specific point in space. It
            is similar to a light bulb.</li>
          <li><b>SpotLight</b>: This light is used to simulate light that is coming from a specific point in space in a
            specific direction. It is similar to a flashlight.</li>
        </ul>

        <h2>Ambient Light</h2>

        <p>AmbientLight does not point or direct light toward a direction, so it cannot
          cast shadows, contributes just a little light/brightness to the scene. </p>

        <p>The first parameter is the color and the second parameter is the intensity:</p>

        <text class="center"><b>var light = new THREE.AmbientLight(color, intensity);</b></text>

        <pre><code class="javascript">// Ambient light
const ambientLight = new THREE.AmbientLight("green", 1); //color, intensity
scene.add(ambientLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/abByRov">Ambient Light Example</a></p>

        <p>Notice how if we remove the light from our scene we cannot see the cube anymore. That is because it is made
          out of MeshPhongMaterial, we'll talk more about that a bit later with shading.</p>

        <p>Just know that MeshPhongMaterial is a material that responds to lights and adds a metallic luster to the
          surface, reflecting
          light with more intensity. It computes lighting at every pixel.</p>

        <p>If all you have is an AmbientLight, you'll have the same effect as for a MeshBasicMaterial because all faces
          of the geometries will be lit equally.</p>

        <h2>Hemisphere Light</h2>

        <p>Like ambient light, hemisphere light has no position or direction. But unlike ambient light it has two
          colors.</p>

        <ul>
          <li>One to simulate the color coming from above, like a sun or ceiling light source.</li>
          <li>The other is the color of light coming from below, to simulate the light reflected off the floor or ground
            surface.</li>
        </ul>

        <p>This is to provide a bit more realism than a globally applied ambient light.</p>

        <text class="center"><b>var light = new THREE.HemisphereLight(skyColor, groundColor, intensity);</b></text>

        <pre><code class="javascript
">// Hemisphere light
const hemisphereLight = new THREE.HemisphereLight("blue", "green", 1); //skyColor, groundColor, intensity
scene.add(hemisphereLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/qBqXJBN">Hemisphere Light Example</a>
        </p>

        <p>Notice how the cube is lit from the top with a blue light and from the bottom with a green light.</p>

        <h2>Directional Light</h2>

        <p>Direcional lights resemble the Sun. They're a distant powerful light source pointing in one direction.</p>

        <p>They are positioned infinitely far away and emit light in a specific direction.</p>

        <p>They have a color and intensity.</p>

        <p>Directional lights are useful for simulating sunlight in a scene.</p>

        <text class="center"><b>var light = new THREE.DirectionalLight(color, intensity);</b></text>

        <pre><code class="javascript
">// Directional light
const directionalLight = new THREE.DirectionalLight("white", 1); //color, intensity
directionalLight.position.set(0, 1, 0); //x, y, z
scene.add(directionalLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/rNWzqaN">Directional Light Example</a>
        </p>

        <p>Notice how the cube is lit from the top right corner. This is because the directional light is positioned at
          (0, 1, 0).</p>

        <p>Let's add a second directional light to our scene:</p>

        <pre><code class="javascript
">// Directional light 2
const directionalLight2 = new THREE.DirectionalLight("red", 1); //color, intensity
directionalLight2.position.set(0, -1, 0); //x, y, z
scene.add(directionalLight2);</code></pre>

        <p>Now how does the cube look?</p>

        <h2>Point Light</h2>

        <p>Point lights are like light bulbs in your scene.</p>

        <p>They are positioned at a specific location and radiate light outwards in all directions from that postion.
          They have a color and intensity.</p>

        <p>Point lights allow you to set the distance from the light at which point it's intensity is zero. You can also
          set the decay which is the amount the light dims along the distance.</p>

        <text class="center"><b>var light = new THREE.PointLight(color, intensity, distance, decay);</b></text>

        <pre><code class="javascript">// Point light
const pointLight = new THREE.PointLight("white", 1); //color, intensity
pointLight.position.set(5, 5, 5); //x, y, z
scene.add(pointLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/ZEBJqYg">Point Light Example</a></p>

        <p>Notice how the cube is lit from the top right corner. This is because the point light is positioned at (5, 5,
          5).</p>

        <p>Let's add a second point light to our scene:</p>

        <pre><code class="javascript">// Point light 2
const pointLight2 = new THREE.PointLight("blue", 1); //color, intensity
pointLight2.position.set(-5, -5, -5); //x, y, z
scene.add(pointLight2);</code></pre>

        <h2>Spot Light</h2>

        <p>Spotlights are just that, spotlights.</p>

        <p>They are lights that point in one direction radiating out in a cone shape.</p>

        <p>We can pass an angle to the light as a parameter which is the maximum angle of the light cone in radians.</p>

        <p>Penumbra affects the intensity or softness of outer shadow or light fall-off as it fades to darkness.</p>

        <p>Because spotlights have a direction they suffer from the same rotation behavior as directional lights,
          requiring them to point to a target object's position.</p>

        <text class="center"><b>var light = new THREE.SpotLight(color, intensity, distance, angle, penumbra, decay);</b>
        </text>

        <pre><code class="javascript">// Spot light
const spotLight = new THREE.SpotLight("white", 1); //color, intensity
spotLight.position.set(5, 5, 5); //x, y, z this is the position of the light
spotLight.target.position.set(0, 0, 0); //x, y, z this is the direction the light is pointing
scene.add(spotLight);
scene.add(spotLight.target);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/wvoqYKx">Spot Light Example</a></p>

        <p>Notice how the cube is lit from the top right corner. This is because the spot light is positioned at (5, 5,
          5).</p>

        <p>Let's add a second spot light to our scene:</p>

        <pre><code class="javascript
">// Spot light 2 
const spotLight2 = new THREE.SpotLight("green", 1); //color, intensity
spotLight2.position.set(-5, -5, -5); //x, y, z this is the position of the light
spotLight2.target.position.set(0, 0, 0); //x, y, z this is the direction the light is pointing
scene.add(spotLight2);
scene.add(spotLight2.target);</code></pre>

        <h2>Phong Shading</h2>

        <p>As previously mentioned, MeshPhongMaterial is a material for shiny surfaces
          with specular highlights.</p>

        <p>Shading is calculated using a <b>Phong shading</b> model. The Phong shading model calculates <b>shading per
            pixel</b> (i.e. in the fragment shader, AKA pixel shader) which gives more accurate results/displays more
          realistic highlights.</p>

        <p>The Phong model is made up of three distinct components:</p>

        <ul>
          <li>Ambient</li>
          <li>Diffuse</li>
          <li>Specular</li>
        </ul>

        <img class="center" style="width: 100%; height: 100%" src="images/Phong_components.png?raw=true"
          alt="Phong shading model" />

        <p>And uses four vectors:</p>

        <ul>
          <li>To source: <b>L</b></li>
          <li>To viewer: <b>V</b></li>
          <li>Normal: <b>N</b></li>
          <li>Perfect reflector: <b>R</b></li>
        </ul>

        <img class="center" style="width: 60%; height: 60%" src="images/phong%20vector.png?raw=true"
          alt="Phong shading model vectors" />

        <p>The following equation breaks it down:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/phong%20equation.png?raw=true"
          alt="Phong shading model equation" />

        <p>The first part of the equation computes the <b>diffuse light</b>: shows the
          direction of the light and can show depth in an object.</p>

        <p>The second part of the equation computes the <b>specular highlight</b>: for
          shiny objects and it reflects the bright spot on the surface. And the
          <b>shininess coefficient α</b>
        </p>

        <p>The third part of the equation computes the <b>ambient light</b>: the
          background light for the object.</p>

        <p>In summary:</p>

        <ul>
          <li><b>kd</b> is the diffuse reflection coefficient</li>
          <li><b>Id</b> is the diffuse light intensity</li>
          <li><b>ks</b> is the specular reflection coefficient</li>
          <li><b>Is</b> is the specular light intensity</li>
          <li><b>α</b> is the shininess coefficient</li>
          <li><b>ka</b> is the ambient reflection coefficient</li>
          <li><b>Ia</b> is the ambient light intensity</li>
        </ul>

        <h2>Blinn-Phong</h2>

        <p>There is also a <b>Blinn-Phong</b> shading model which is a modification of the Phong shading model.</p>

        <p>Blinn-Phong, also known as the modified Phong model, uses the same
          equation, but measures the angle between the surface normal and the
          halfway vector instead of finding the angle between the view vector and the
          reflection vector.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/c79773fa83ed58d460ee5273f4ac98a1a428f766/courses/CISC3620/images/Blinn_Vectors.svg"
          alt="Blinn-Phong shading model vectors" />

        <p>where:</p>

        <img class="center" style="width: 100%; height: 100%" src="images/blinn%20phong%20equation.png?raw=true"
          alt="Blinn-Phong shading model" />

        <p>Although the material is called Phong it actually uses the Blinn-Phong reflection model rather than a pure
          Phong Reflection model.</p>

        <p>Let's create a new material to see this in action:</p>

        <pre><code class="javascript">const material = new THREE.MeshPhongMaterial({
   color: "red", // color to use for light, default is white
   shininess : 60, // shininess default is 30
   specular: "blue" // color of the specular highlight, default is white
});</code></pre>

        <p>Let's add some point lights so we can actually see the cube:</p>

        <pre><code class="javascript">//Create point light and add to scene 
const pointLight = new THREE.PointLight("white", 1); //color, intensity
pointLight.position.set(50, 50, 50); //x, y, z
scene.add(pointLight);


// Point light 2
const pointLight2 = new THREE.PointLight("white", 1); //color, intensity
pointLight2.position.set(-50, -50, -50); //x, y, z
scene.add(pointLight2);</code></pre>

        <p>Now, when you run the code, you should see a red cube with a blue specular highlight rendered in the scene.
        </p>

        <h2>Interactivity</h2>

        <p>So far, we made the cube rotate automatically as it is rendered. This
          time let's try to control its rotation with user interaction.</p>

        <p>To do this, we will comment out the rotational behavior and write our own.</p>

        <p>We will use the arrow keys to control how to cube rotates. How do we make
          sure that the program is aware when we press a specific key and react
          accordingly?</p>

        <p>As we saw a bit in 2D graphics, we will have to add an<b>event listener</b> to attach an event handler to a
          specified element. In this case, we do not need to specify a particular
          element so we can attach an event handler to the document directly.</p>

        <p>This is proper syntax for creating an event listener for the arrow keys:</p>

        <pre><code class="javascript">// Add listener for keyboard
document.addEventListener('keydown', keyPressed);

//behavior for directional keys
function keyPressed(e){
  switch(e.key) {
    case "ArrowLeft": //left arrow
      //move left on the y-axis
    	cube.rotation.y -= 0.1; 
    	break;
  	case "ArrowUp": //up arrow
      //move up on the x-axis
    	cube.rotation.x -= 0.1; 
    	break;
    case "ArrowRight": //right arrow
      //move right on the y-axis
    	cube.rotation.y += 0.1;
    	break;  
    case "ArrowDown": //down arrow
      //move down on the x-axis 
    	cube.rotation.x += 0.1; 
    	break;
  }
}</code></pre>

        <p>Now, when we run the code, we should be able to control the rotation of the cube using the arrow keys.</p>

        <p>We've successfully added basic interactivity to our project.</p>

        <h2>Orbit Controls</h2>

        <p>Another way to add interactivity to your Three.js project is by using OrbitControls.</p>

        <p>OrbitControls is a utility that allows you to control the camera in your scene using the mouse.</p>

        <p>It allows you to rotate, zoom, and pan the camera around the scene.</p>

        <p>Here's how you can add OrbitControls to your project:</p>

        <p>First, you need to include the OrbitControls script in your HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/controls/OrbitControls.js"&gt;&lt;/script&gt;</code></pre>


        <p>Next, you can add the following code to your JavaScript:</p>

        <pre><code class="javascript">const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>Now, when you run the code, you should be able to control the camera in your scene using the mouse.</p>

        <p>And that's it! We've successfully added both mouse and keyboard interaction.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Shadows">
      <br />
      <header><b>Shadows</b></header>
      <article>
        <p>Shadows are an important part of creating a realistic 3D scene.</p>

        <p>The light that is coming from a specific direction can cast shadows. First, we should make the scene ready
          for casting shadows.</p>

        <p>We should first tell the renderer that we want to enable shadows. Casting shadows is an expensive operation.
          WebGLRenderer only supports this functionality.</p>

        <p>It uses Shadow mapping, a technique specific to WebGL, performed directly on the GPU.</p>

        <pre><code class="javascript">renderer.shadowMapEnabled = true;</code></pre>

        <p>The above line of code tells the renderer to cast shadows in the scene.</p>

        <p>Note - Three.js, by default, uses shadow maps. Shadow map works for light that casts shadows.
          The scene renders all objects marked to cast shadows from the point of view of the light.</p>

        <p>There are two types of shadows in Three.js:</p>

        <p></p><b>Shadow Mapping</b>: This is the most common technique used to create shadows in 3D graphics. It works
        by rendering the scene from the perspective of the light source and storing the depth values of the scene in a
        texture called the shadow map. When rendering the scene from the camera's perspective, the depth values of
        the scene are compared to the depth values in the shadow map to determine if a pixel is in shadow or not.
        </p>


        <p>Shadow mapping is the method of choice for creating shadows in high-end rendering for motion pictures and
          television. However, it has been problematic to use shadow mapping in real-time applications, such as video
          games, because of aliasing problems in the form of magnified jaggies. </p>

        <p>Shadow mapping involves projecting a shadow map on geometry and comparing the shadow map values with the
          light-view depth at each pixel. If the projection causes the shadow map to be magnified, aliasing in the form
          of large, unsightly jaggies will appear at shadow borders.</p>

        <p>Aliasing can usually be reduced by using higher-resolution shadow maps and increasing the shadow map
          resolution,
          using techniques such as perspective shadow maps (Stamminger and Drettakis 2002). </p>

        <p>However, using perspective shadow-mapping techniques and increasing shadow map resolution does not work when
          the light is traveling nearly parallel to the shadowed surface, because the magnification approaches infinity.
        </p>

        <p>High-end rendering software solves the aliasing problem by using a technique called <b>percentage-closer
            filtering</b>.</p>


        <p><b>PCF (Percentage-Closer Filtering)</b>: This is a technique used to smooth out the edges of shadows by
          taking multiple samples around each pixel and averaging the results. This helps to reduce the jagged
          appearance of shadows and create a more realistic effect.</p>

        <p>Unlike normal textures, shadow map textures cannot be prefiltered to remove aliasing. Instead, multiple
          shadow map comparisons are made per pixel and averaged together.</p>

        <p>This technique is called percentage-closer filtering (PCF) because it calculates the percentage of the
          surface that is closer to the light and, therefore, not in shadow.</p>

        <p>The original PCF algorithm, described in Reeves et al. 1987, called for mapping the region to be shaded into
          shadow map space and sampling that region stochastically (that is, randomly). </p>

        <p>The algorithm was first implemented using the REYES rendering engine, so the region to be shaded meant a
          four-sided micropolygon. The figure below shows an example of that implementation.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/percentage-closer%20filtering.jpg?raw=true"
          alt="PCF shadow mapping" />

        <p>However, this algorithm advances over time.</p>

        <p>For instance, NVIDIA GPUs have changed the PCF algorithm slightly to make it easy and efficient to apply.
          Instead of
          calculating the region to be shaded in shadow map space, they simply use a 4x4-texel sample region everywhere.
        </p>

        <p>This region is large enough to significantly reduce aliasing, but not so large as to require huge numbers of
          samples or stochastic sampling techniques to achieve good results. Note that the sampling region is not
          aligned to texel boundaries. </p>

        <p>PCF is a simple and effective way to reduce aliasing in shadow maps. It is not perfect, but it is a good
          compromise between quality and performance.</p>

        <p>Since we are on the topic of antialiasing, we should also make sure that our scene is rendered with it
          enabled.
        </p>

        <pre><code class="javascript
">renderer.antialias = true;</code></pre>

        <p>By doing this we can reduce the jagged edges of our objects.</p>

        <p>Let's create a scene with shadows:</p>

        <p>First, we need to enable shadows in our renderer:</p>

        <pre><code class="javascript
">renderer.shadowMap.enabled = true;</code></pre>

        <p>Next, we need to enable shadows for each light that we want to cast shadows:</p>

        <pre><code class="javascript
">// Enable shadows for directional light
const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(5, 5, 5); // Position the light
light.castShadow = true; // Enable shadow casting for the light
scene.add(light);</code></pre>

        <p>We should configure objects to cast shadows. We can inform Three.js which objects can cast shadows and which
          objects can receive shadows.</p>

        <pre><code class="javascript">object1.castShadow = true;
object2.recieveShadow = true;</code></pre>

        <p>Let's create a simple cube that casts a shadow on a floor:</p>

        <pre><code class="javascript
">// Create a cube that casts a shadow
const cubeGeometry = new THREE.BoxGeometry(2, 2, 2);
const cubeMaterial = new THREE.MeshStandardMaterial({ color: "red" });
const cube = new THREE.Mesh(cubeGeometry, cubeMaterial);
cube.position.set(0, -1, 0);
cube.castShadow = true;
scene.add(cube);

// Create a floor that receives shadows
const floorGeometry = new THREE.PlaneGeometry(20, 20);
const floorMaterial = new THREE.MeshStandardMaterial({ color: "gray" });
const floor = new THREE.Mesh(floorGeometry, floorMaterial);
floor.rotation.x = -Math.PI / 2;
floor.position.y = -2;
floor.receiveShadow = true;

scene.add(floor);</code></pre>

        <p>Let's see how the shadow looks with a BasicShadowMap: </p>

        <pre><code class="javascript">renderer.shadowMapType = THREE.BasicShadowMap;</code></pre>

        <p>If our shadow looks a bit blocky around its edges, it means the shadow map is too small.</p>

        <p>To increase the shadow map size, we can define shadowMapHeight and shadowMapWidth properties for the light.
        </p>

        <p>Alternatively, we can also try to change the shadowMapType property of WebGLRenderer. We can set this to
          THREE.BasicShadowMap, THREE.PCFShadowMap, or THREE.PCFSoftShadowMap.</p>

        <p>Now let's try to antialias the shadow:</p>

        <pre><code class="javascript">// to antialias the shadow
renderer.shadowMapType = THREE.PCFSoftShadowMap;
// or
directionalLight.shadowMapWidth = 2048;
directionalLight.shadowMapHeight = 2048;</code></pre>

        <h2>Exercise: Let's Create a Scene with Shadows</h2>

        <p>Now that we've learned how to add shadows to our scene, add another object that casts a shadow on the floor:
        </p>

        <p>Let's add a sphere that casts a shadow:</p>

        <pre><code class="javascript
">const sphereGeometry = new THREE.SphereGeometry(1, 32, 32);
const sphereMaterial = new THREE.MeshStandardMaterial({ color: "blue" });
const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphere.position.set(5, -1, 0);
sphere.castShadow = true;
scene.add(sphere);</code></pre>

        <p>Now, when we run the code, we should see a red cube and a blue sphere casting shadows on a gray floor in
          the scene.</p>

        <p>And that's it! We've successfully created a scene with shadows.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Textures">
      <br />
      <header><b>Textures</b></header>
      <article>
        <p>When we create a mesh, such as our humble cube, we pass in two components: a geometry and a material.</p>

        <pre><code class="javascript">const geometry = new THREE.BoxGeometry(2, 2, 2);
const material = new THREE.MeshStandardMaterial({color: 'purple'});
const cube = new THREE.Mesh(geometry, material);
scene.add(cube);</code></pre>

        <p>The geometry defines the mesh's shape, and the material defines various surface properties of the mesh, in
          particular, how it reacts to light. The geometry and the material, along with any light and shadows affecting
          the mesh, control the appearance of the mesh when we render the scene.</p>

        <p>Currently, our scene contains a single mesh with a shape defined by a BoxGeometry and a surface defined by a
          MeshStandardMaterial with the color parameter set to purple.</p>

        <p>Let's illuminate the scene by a single DirectionalLight, so when we render the scene, the result is this
          simple purple box:</p>

        <pre><code class="javascript">//Create a DirectionalLight
const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(5,2,15); //x, y, z
scene.add(light);</code></pre>

        <p>Compare this to a concrete box in the real world - or a wooden box, or a metal box, or a box made from nearly
          any substance except smooth plastic, and we can immediately see that our 3D box is not at all realistic.
          Objects in the real world are usually scratched, broken, and dirty.</p>

        <p>However, the material applied to our box doesn't look like this. Rather, it consists of a single color
          applied smoothly over the entire surface of the mesh. Unless we want all of our creations to look like
          brand-new plastic, this won't do.</p>

        <p>Materials have many parameters besides color, and we can use these to adjust various attributes of an
          object's surface, like the roughness, metalness, opacity, and so on.</p>

        <p>However, just like the color parameter, these parameters are applied uniformly over the entire surface of the
          mesh.</p>

        <p>If we increase the material's .roughness property, for example, the entire surface of the object will become
          rougher. Just like if we set the .color to red, the entire object will become red.</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({color: 'purple', roughness: 0.0});</code></pre>

        <p>When we set the roughness property to 0.0, the surface of the object becomes perfectly smooth. If we set it
          to 1.0, the surface becomes perfectly rough.</p>

        <p>How does this affect the cube that we are currently looking at?</p>

        <p>By contrast, the surface properties of most real-world objects change from one point to the next.</p>

        <p>Once again, it consists of a geometry and a material, just like our cube mesh.</p>

        <p>The large scale features, like the eyes, nose, ears, neck, and chin, are defined by the geometry.</p>

        <p>However, a lot more than a well-crafted geometry goes into creating a realistic face. Looking closely at the
          skin, we can see there are many small bumps, wrinkles, and pores, not to mention eyebrows, lips, and a slight
          beard.</p>

        <p>When creating a complex model like a face, an artist must decide what parts of the model to represent using
          geometry, and what parts to represent at the material level, bearing in mind that it's usually cheaper to
          represent things using the material than the geometry.</p>

        <p>This is an especially important consideration when the model has to run on a mobile device, where high
          performance is paramount.</p>

        <p>For example, while it would be possible to model every hair in the eyebrows in geometry, doing so would make
          this model unsuitable for real-time use on all but the most powerful of devices. Instead, we must represent
          small features like hair at the material level, and reserve the geometry for large scale features like the
          eyes, nose, and ears.</p>

        <p>Note, also, that this face is made from a single geometry. We usually want to avoid splitting a geometry up
          more than necessary since every mesh can have only one geometry, so each separate geometry corresponds to a
          new mesh in our scene.</p>

        <p>Having fewer objects in a scene usually results in better performance, and it's also easier for both the
          developer and the 3D artist to work with. In other words, we don't want to be forced to create different
          geometries for the ears, and eyes. In any case, this wouldn't be practical. Looking closely at the lips, we
          can see there is no sharp divide between the red of the lips and the skin tone of the chin. This means we need
          some way of modifying material properties so that they can change smoothly across the surface of an object.
        </p>

        <p>We need to be able to say things like this:</p>

        <ul>
          <li>the part of the geometry making up the lips is red</li>
          <li>the part of the geometry making up the chin is a skin tone overlaid by a slight beard</li>
          <li>the part of the geometry making up the eyebrows is hair colored</li>
        </ul>

        <p>… and so on. And this doesn't only apply to color. The skin is shinier than the hair and lips, for example.
          So, we also need to be able to specify how other properties like roughness change from one point to the next
          across the geometry.</p>

        <p>This is where <b>texture mapping</b> comes in. In the simplest possible terms, texture mapping means taking
          an image
          and stretching it over the surface of a 3D object.</p>

        <p>We refer to an image used in this manner as a texture, and we can use textures to represent material
          properties like color, roughness, and opacity.</p>

        <p>While it's easy to take a 2D texture and stretch it over a regular shape like a cube, it's much harder to do
          that with an irregular geometry like a face, and over the years, many texture mapping techniques have been
          developed. Perhaps the simplest technique is <b>projection mapping</b>, which projects the texture onto an
          object (or scene) as if it has been shone through a film projector. Imagine holding your hand in front of a
          film
          projector and seeing the image projected onto your skin.</p>

        <p>While projection mapping and other techniques are still widely used for things like creating shadows (or
          simulating projectors), that's not going to work for attaching the face's color texture to the face geometry.
        </p>

        <p>Instead, we use a technique called <b>UV mapping</b> which allows us to create a connection between points on
          the geometry and points on the face.</p>

        <p>Using UV mapping, we divide the texture up into a 2D grid with the point (0,0) at the bottom left and the
          point (1,1) at the top right. Then, the point (0.5,0.5) will be at the exact center of the image.</p>

        <img class="center" style="width: 50%; height: 50%" src="images/texture%20example%201.png?raw=true"
          alt="UV mapping" />

        <p>Likewise, every point in a geometry has a position in the 3D local space of the mesh. UV mapping, then, is
          the process of assigning 2D points in the texture to 3D points in the geometry.</p>

        <p>For example, suppose the lips in the face model are at the point (0,0,0). We can see that the lips in the
          texture are close to the center, somewhere around (0.5,0.5).</p>

        <p>So, we'll create a mapping:</p>

        <text class="center"><b>(0.5,0.5)⟶(0,0,0)</b></text>

        <p>Now, when we assign the texture as a color map in the material, the center of the texture will be mapped onto
          the lips.</p>

        <p>Next, we must do the same for many other points in the geometry, assigning the ears, eyes, eyebrows, nose,
          and chin to the appropriate points of the texture. If this sounds like a daunting procedure, don't worry,
          because it's rare to do this manually.</p>

        <p>For this model, the UV mapping was created in an external program, and in general, that's the recommended way
          to create UV mappings.</p>

        <p>Data representing the UV mapping is stored on the geometry. The Three.js geometries like the BoxGeometry have
          already got UV mapping set up, and in most cases, when you load a model like a face that was created in an
          external program, it will also have UV mapping ready for use.</p>

        <p>Once we have a geometry with a UV mapping, we can take any texture and apply it to the geometry and it will
          immediately work.</p>

        <p>However, it might be hard to find other textures that will look good with a face model since the UV map must
          be carefully coordinated to match the texture to the correct points on the face, and doing this well is the
          work of a skilled 3D artist.</p>

        <p>However, for simple shapes like a cube we can use nearly any image as a texture, turning the box into a
          wooden box, or a concrete box, or a crate, and so on.</p>

        <p>Before we proceed with loading a texture and applying it to our cube, let's go over all the technical terms
          that we'll be using when working with textures.</p>

        <h2>What's the Difference Between an Image and a Texture?</h2>

        <p>We'll see the terms <b>texture</b> and <b>image</b> a lot in computer graphics literature. These are even
          often stored in the same format, such as PNG or JPG. What’s the difference?</p>

        <ul>
          <li>An <b>image</b> is a 2D picture designed to be viewed by a human</li>
          <li>A <b>texture</b> is specially prepared data used for various purposes in 3D graphics.</li>
        </ul>

        <p>The individual pixels that make up an image represent color. Another way of looking at this is that an image
          is a 2D array of colors.</p>

        <p>In the early days of computer graphics, that was the case for textures too, but over time more and more uses
          were found for textures and now it's more correct to say that a texture is a 2D array of data.</p>

        <p>This data can represent anything. Nowadays it's even possible to store geometry or animations in a texture.
        </p>

        <p>When we use a texture in Three.js, we're usually using it to represent material properties like color,
          roughness, and opacity.</p>

        <p>Textures are applied to a mesh by creating a Texture object and passing it to the map property of the
          material used to render the mesh.</p>

        <h2>Texture Map</h2>

        <p>Although technically incorrect, a texture is also often referred to as a <b>map</b>, or even a texture map,
          although
          map is most commonly used when assigning a texture to a material.</p>

        <p>When using a texture to represent color, we'll say that we are assigning a texture to the color map slot on a
          material.</p>

        <h2>UV Mapping</h2>

        <p>UV mapping is a method for taking a 2-dimensional texture and mapping it onto a 3-dimensional geometry.</p>

        <p>Imagine a 2D coordinate system on top of the texture, with (0,0) in the bottom left and (1,1) in the top
          right. Since we already use the letters X, Y and Z for our 3D coordinates, we'll refer to the 2D texture
          coordinate using the letters U and V.</p>

        <p>This is where the name UV mapping comes from.</p>

        <p>Here's the formula used in UV mapping:</p>

        <text class="center"><b>(u,v)⟶(x,y,z)</b></text>

        <p>(u,v) represents a point on the texture, and (x,y,z) represents a point on the geometry, defined in local
          space. Technically, a point on a geometry is called a vertex.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/3c4901326e43478f13f54ae0d1b60d7ca2b4c6ef/courses/CISC3620/images/geometry_uv_map.svg"
          alt="UV mapping" />

        <p>In the figure above, the top left corner of the texture has been mapped to a vertex on the corner of the cube
          with coordinates (−1,1,1):</p>

        <text class="center"><b>(0,1)⟶(−1,1,1)</b></text>

        <p>Similar mappings are done for the other five faces of the cube, resulting in one complete copy of the texture
          on each of the cube's six faces:</p>

        <img class="center" style="width: 75%; height: 75%" src="images/texture%20map%20example.png?raw=true"
          alt="UV mapping example" />

        <p>Note that there is no mapping for the point (0.5,0.5), the center of the texture. Only the corners of the
          texture are mapped, onto the eight corners of the cube, and the rest of the points are "guessed" from these.
        </p>

        <p>By contrast, a complex model like a face must have many more UV coordinates defined to map the parts of the
          texture representing the nose, ears, eyes, lips, and so on, to the correct points of the geometry.</p>

        <p>Fortunately, we rarely need to set up UV mapping manually since all the three.js geometries, including the
          BoxGeometry, have UV mapping built-in. We only need to load the texture and apply it to our material and
          everything will work.</p>

        <h2>How to Load a Texture</h2>

        <p>Three.js has a built-in function <b>TextureLoader()</b> to load textures into your Three.js project.</p>

        <p>First, we should create a loader:</p>

        <pre><code class="javascript">const loader = new THREE.TextureLoader();</code></pre>

        <p>Then we can load any texture or image by specifying its path in the load() function:</p>

        <pre><code class="javascript">const texture = loader.load('path/to/texture.jpg');</code></pre>

        <p>Now, we can apply the texture to the material by setting the map property of the material to the texture:</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ map: texture });</code></pre>


        <p>Now, let's add a basic texture to our cube:</p>

        <p>This is a basic image of a side of a wooden crate:</p>

        <img class="center" style="width: 30%; height: 30%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate.gif"
          alt="crate texture" />

        <p>First, we should create a loader, then we can load our texture or image by specifying its path in the load()
          function.</p>

        <pre><code class="javascript">const loader = new THREE.TextureLoader();
const texture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate.gif');</code></pre>

        <p>Now, we can apply the texture to the material by setting the map property of the material to the texture.</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ map: texture });</code></pre>

        <p>And add a light source so we can actually see our crate:</p>

        <pre><code class="javascript">//Create a AmbientLight
const light = new THREE.AmbientLight( 0xffffff, 1 );
scene.add(light);</code></pre>

        <p>Now, when we render the scene, we should see the cube with the crate texture applied to its surface.</p>

        <p>Let's now create a crate positioned on a floor (plane)</p>

        <p>First let's set our scene, camera, and renderer:</p>

        <pre><code class="javascript">let width = 500;
let height = 400;
// Create the scene
const scene = new THREE.Scene();
scene.background = new THREE.Color("green"); //green screen to better view environment
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
// Position the camera
camera.position.set(0, 3, 10); // Move it back on the z-axis and up on the y-axis
          
const renderer = new THREE.WebGLRenderer();</code></pre>

        <p>Now let's enable our renderer to include shadows:</p>

        <pre><code class="javascript">renderer.shadowMap.enabled = true;</code></pre>

        <p>And set size and add our renderer to view:</p>

        <pre><code class="javascript">renderer.setSize(width, height);
document.body.appendChild(renderer.domElement); //add renderer to view</code></pre>

        <p>Don't forget our animate function:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>Now above our animate function, let's create a plane to act as our floor:</p>

        <pre><code class="javascript">// Create plane geometry and material
const planeGeometry = new THREE.PlaneGeometry(50, 50);
const planeTexture = new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/concrete%20floor.jpg');
const planeMaterial = new THREE.MeshStandardMaterial({ map: planeTexture });
const plane = new THREE.Mesh(planeGeometry, planeMaterial);
plane.rotation.x = -Math.PI / 2; // Rotate the plane to be horizontal
plane.receiveShadow = true; // Allow the plane to receive shadows
scene.add(plane);</code></pre>

        <p>Now let's create a crate to place on the floor:</p>

        <pre><code class="javascript">// Create crate geometry and material
const geometry = new THREE.BoxGeometry(5, 5, 5);
const crateTexture = new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate3.jpg');
const material = new THREE.MeshStandardMaterial({ map: crateTexture });

const crate = new THREE.Mesh(geometry, material);
crate.position.y = 2.5; // Position it above the plane
crate.castShadow = true; // Allow the cube to cast shadows
scene.add(crate);</code></pre>

        <p>Now when you run the code, you should see a cube positioned on a floor, however due to lack of lights it is
          hard to make out.</p>

        <p>Now let's create a directional light so we can actually illuminate our scene:</p>

        <pre><code class="javascript">const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(7, 7, 7); // Position the light
light.castShadow = true; // Enable shadow casting for the light
scene.add(light);</code></pre>

        <p>Now we should see a cube textured to look like a wooden crate positioned on a concrete floor, illuminated by
          a directional light.</p>

        <p>Let's add some orbit controls to take a closer look at the scene:</p>

        <pre><code class="javascript
">// Create orbit controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>It looks good but upon further inspection, notice how our floor texture looks more pixelated the closer we
          get. This is because the texture is being stretched over a large area.</p>

        <p>One way to fix this is to increase the resolution of the texture. However, this can be expensive in terms of
          memory and performance.</p>

        <p>Another way to fix this is to use a technique called <b>texture tiling</b>. Texture tiling involves repeating
          the texture multiple times over the surface of the mesh.</p>

        <p>Let's add texture tiling to our floor:</p>

        <pre><code class="javascript">//to improve by repeating texture
planeTexture.wrapS = THREE.RepeatWrapping; // Repeat the texture in the x-direction
planeTexture.wrapT = THREE.RepeatWrapping; // Repeat the texture in the y-direction
planeTexture.repeat.set(4, 4); // Repeat the texture 4 times in both directions</code></pre>

        <p>Now we see the floor texture repeated multiple times over the surface of the plane, which improves the
          appearance of the texture.</p>

      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="A_More_Complex_Scene">
      <br />
      <header><b>A More Complex Scene</b></header>
      <article>
        <p>So far, we've learned how to create a scene, add objects to it, add lights, cast shadows, and apply textures
          to those objects.</p>

        <p>For example, we've played around with cubes but if we want realistic scenes that are not just made up of
          crates and boxes we need to start building more complex objects.</p>

        <p>Let's say we want to create a table. What does a table actually look like?</p>

        <p>A basic table has a flat surface and four legs. How can we create this in Three.js?</p>

        <p>One way to do this is to use <b>primitive geometries</b> like BoxGeometry to create
          custom models.</p>

        <p>For example, to create a table, we can use a BoxGeometry for the table top and four additional BoxGeometries
          for the legs.</p>

        <p>First, let's set up our scene:</p>

        <pre><code class="javascript">let width = 500;
let height = 400;
// Create the scene
const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
// Position the camera
camera.position.z = 10; // Move it back on the z-axis
camera.position.y = 2; // Move it up on the y-axis
const renderer = new THREE.WebGLRenderer();
renderer.shadowMap.enabled = true; // Enable shadows in the renderer
renderer.setSize(width, height);
renderer.antialias = true; // Enable antialiasing
document.body.appendChild(renderer.domElement); //add renderer to view</code></pre>

        <p>Let's also add orbit controls so we can get a better view of our scene:</p>

        <pre><code class="javascript">// Create orbit controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>And don't forget our animate() function so we can actually see our scene:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>Let's add a floor for our objects to sit atop:</p>

        <pre><code class="javascript">// Create a floor that receives shadows
const floorGeometry = new THREE.PlaneGeometry(20, 20);
const floorMaterial = new THREE.MeshBasicMaterial({ color: "gray" });
const floor = new THREE.Mesh(floorGeometry, floorMaterial);
floor.rotation.x = -Math.PI / 2; // Rotate the floor to be horizontal
floor.position.y = -2;
floor.receiveShadow = true;
scene.add(floor);</code></pre>

        <p>Now let's actually create our table. First we need a table top:</p>

        <pre><code class="javascript
">// Create table top geometry and material
const tableTopGeometry = new THREE.BoxGeometry(8.5, 0.75, 4); //width, height, depth
const tableTopMaterial = new THREE.MeshBasicMaterial({ color: "brown" });
const tableTop = new THREE.Mesh(tableTopGeometry, tableTopMaterial);
tableTop.position.y = 0.85; // Position the table top above the floor
tableTop.castShadow = true; // Allow the table top to cast shadows
scene.add(tableTop);</code></pre>

        <p>Now let's create the legs of the table:</p>

        <pre><code class="javascript">const legGeometry = new THREE.BoxGeometry(0.5, 3, 0.5); //width, height, depth
const legMaterial = new THREE.MeshBasicMaterial({ color: "brown" });
const leg1 = new THREE.Mesh(legGeometry, legMaterial);
const leg2 = new THREE.Mesh(legGeometry, legMaterial);
const leg3 = new THREE.Mesh(legGeometry, legMaterial);
const leg4 = new THREE.Mesh(legGeometry, legMaterial);
leg1.position.set(-4, -1, 1.75); // Position the legs at the corners of the table top
leg2.position.set(-4, -1, -1.75);
leg3.position.set(4, -1, 1.75);
leg4.position.set(4, -1, -1.75);
leg1.castShadow = true;
leg2.castShadow = true;
leg3.castShadow = true;
leg4.castShadow = true;
scene.add(leg1, leg2, leg3, leg4);</code></pre>

        <p>This is okay but what if we needed to reposition the table now? Because we added each part of the table to
          the scene as it's own individual object, we would have to manually adjust the
          position of each leg. This is not ideal.</p>

        <p>Instead of adding each part of the table as a separate object, we can group them together using a
          <b>Group</b> object.
        </p>

        <p>Let's group the table top and legs together:</p>

        <pre><code class="javascript">// Create a group to hold the table top and legs
const table = new THREE.Group();
table.add(tableTop, leg1, leg2, leg3, leg4);
scene.add(table);</code></pre>

        <p>Now, when we want to move the table, we can simply move the table object and all of its children will move
          with it.</p>

        <p>Let's better position the table so the legs are not going through the floor:</p>

        <pre><code class="javascript">table.position.y = 0.5;</code></pre>

        <p>We should now have a simple table in our scene. We can add more objects to the scene in a similar way to
          create more complex scenes.</p>

        <p>Notice that we used MeshBasicMaterial since we did not set up any lights. Let's fix that:</p>

        <p>First, we need to change our material to something that uses lights, i.e MeshStandardMaterial.</p>

        <p>When we change the material of the table, we should see a simple black silhouette of the brown table we once
          had. Don't worry we will see it again once we add light.</p>

        <p>Now, let's add some light:</p>

        <pre><code class="javascript
">// Create a directional light
const light = new THREE.DirectionalLight("white", 1);
light.position.set(7, 7, 7);
light.castShadow = true;

// Add the light to the scene
scene.add(light);</code></pre>

        <p>Now we should see a table with legs positioned on a floor, illuminated by a directional light.</p>

        <p>Let's make sure to update our floor material as well. Now we should see a shadow of the table being cast onto
          the floor.</p>

        <p>Our table looks good but it's a bit plain. Let's add a texture to the table top:</p>

        <p>First, let's load our texture:</p>

        <pre><code class="javascript">// Create a texture loader
const loader = new THREE.TextureLoader();
const tableTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/wood%20table%20top%20texture.jpg');</code></pre>

        <p>Now, let's apply the texture to the table top:</p>

        <pre><code class="javascript">const tableTopMaterial = new THREE.MeshStandardMaterial({ map: tableTexture });</code></pre>

        <p>And do the same to the legs:</p>

        <pre><code class="javascript">const legMaterial = new THREE.MeshStandardMaterial({ map: tableTexture });</code></pre>

        <p>Now our table should have a wooden texture applied to the table top and legs and look more realistic.</p>

        <p>In addition to our standard primitive geometries like BoxGeometry and PlaneGeometry, Three.js also provides
          a number of built-in geometries for creating more complex objects, for example: TeapotGeometry</p>

        <p>We'll add a teapot to our scene but first let's understand TeapotGeometry:</p>

        <p>TeapotGeometry is a geometry that represents a teapot. It's a complex geometry that is not built into
          Three.js by default, but we can add it by importing it from the Three.js examples.</p>

        <p>TeapotGeometry has multiple parameters that can be set to customize the teapot, such as size, segments, and
          detail.</p>

        <p>However, the only required parameter is the size of the teapot.</p>

        <p>We need to add the following script to our HTML file to import the TeapotGeometry:</p>

        <pre><code class="HTML">&lt;script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/geometries/TeapotGeometry.js"&gt;&lt;/script&gt;</code></pre>

        <p>Now we can create a teapot and add it to our scene:</p>

        <pre><code class="javascript
">// Create teapot geometry and material
const teapotGeometry = new THREE.TeapotGeometry(0.75); // Set the size of the teapot
const teapotMaterial = new THREE.MeshStandardMaterial({ color: "white" });
const teapot = new THREE.Mesh(teapotGeometry, teapotMaterial);
teapot.position.y = 2.5; // Position the teapot to sit atop the table
teapot.castShadow = true;
scene.add(teapot);</code></pre>

        <p>We should now see a white teapot sitting atop our wooden table. However, there is no shadow on the table from
          our teapot. This is because although we have enable shadow casting for the teapot, we have not enabled shadow
          receiving for the table top.</p>

        <p>Let's enable shadow receiving for the table top:</p>

        <pre><code class="javascript">// Enable shadow receiving for the table
tableTop.receiveShadow = true;</code></pre>

        <p>Now we should see a shadow of the teapot being cast onto the table.</p>

        <p>Now we have a simple scene with a table and a teapot. We can add more objects to the scene in a similar way
          to create more complex scenes.</p>

        <p>Let's add texture to our floor:</p>

        <pre><code class="javascript">const floorTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/wood%20floor.jpg');</code></pre>

        <p>And remember to update our floorMaterial to apply the texture:</p>

        <pre><code class="javascript
">const floorMaterial = new THREE.MeshStandardMaterial({ map: floorTexture });</code></pre>

        <p>And now our scene looks more realistic than it did when we began!</p>

        <p>We can modify this scene by adding more objects, such as walls, furniture, lights, etc.</p>

        <p>Let's create walls for our scene so we have a room. We will only need three walls as the fourth will be
          occupied by our camera.</p>

        <p>First, let's create a wall:</p>

        <pre><code class="javascript
">// Create wall geometry and material
const wallGeometry = new THREE.PlaneGeometry(20, 15);
const wallMaterial = new THREE.MeshStandardMaterial({ color: "#a0d6b4" }); // Turquoise Green
const wall = new THREE.Mesh(wallGeometry, wallMaterial);
wall.position.z = -10; // Position the wall behind the table
wall.position.y = 5.5; // Position the wall at the same height as the table
wall.receiveShadow = true; // Allow the wall to receive shadows
scene.add(wall);</code></pre>

        <p>We need to ensure that the wall has two sides or else we will only see the wall from one side. We can do this
          by setting the side property of the material to THREE.DoubleSide:</p>


        <pre><code class="javascript">wall.material.side = THREE.DoubleSide;</code></pre>

        <p>While on the topic, let's also make sure our floor has two sides:</p>

        <pre><code class="javascript">floor.material.side = THREE.DoubleSide;</code></pre>


        <p>Now we should see a turquoise green wall behind our table. Let's add two more walls:</p>

        <pre><code class="javascript">const wall2 = wall.clone(); // Clone the wall
wall2.rotation.y = Math.PI / 2; // Rotate the wall to be perpendicular to the first wall
wall2.position.x = -10; // Position the wall to the left of the table
wall2.position.z = 0.0001; // Position so edges of the walls meet
scene.add(wall2);

const wall3 = wall.clone();
wall3.rotation.y = Math.PI / 2; // Rotate the wall to be perpendicular to the first wall
wall3.position.x = 10; // Position the wall to the right of the table
wall3.position.z = 0.0001; // Position so edges of the walls meet
scene.add(wall3);</code></pre>

        <p>Now how our right wall is shadowed, that is due to lack of light on that side. Let's add a point light above
          (like a ceiling light) to view better:</p>

        <pre><code class="javascript">// Create a point light
const light2 = new THREE.PointLight("white", 1);
light2.position.set(0, 20, 0);
light2.castShadow = true;
scene.add(light2);</code></pre>

        <p>Now our scene should be more illuminated.</p>

        <p>How does having these two different lights in different positions affect our scene? How does it affect our
          shadows?</p>

        <p>What does our scene look like if we rid of our directional light? How does having just the center point light
          change the scene?</p>


        <p>If our teapot's shadow looks a bit blocky from the point light, we can fix this by applying some
          antialiasing:</p>
        <pre><code class="javascript">// Antialias the point light shadow
light2.shadow.mapSize.width = 2048; // Increase shadow map size for better quality
light2.shadow.mapSize.height = 2048;
light2.shadow.radius = 4; // Increase shadow radius for softer shadows</code></pre>

        <p>This should make our shadow look smoother.</p>

        <p>And there we have it! We've used what we've learned so far to build a more complex scene with a table,
          teapot, walls, and lights.</p>

        <p>Now think about tables that we have seen in reality, they can vary from material. A standard table like our
          object can be made out of wood (like our example), marble (for which we would use a different texture and a
          shinier material),
          or even glass.</p>

        <p>Let's talk about the properties of glass.</p>

        <p>Glass is transparent, meaning light can pass through it. This is different from our current table material
          which is opaque, meaning light cannot pass through it.</p>

        <p>In order to create transparent objects, first we need to set the transparent property of our material to
          true.
          We also need to set the opacity property to a value between 0 and 1, where 0 is completely transparent and 1
          is
          completely opaque.</p>

        <p>Let's create a glass table:</p>

        <pre><code class="javascript">const tableMaterial = new THREE.MeshPhongMaterial({ color : "pink", 
    transparent : true, 
    opacity: 0.75
}); //opacity will not work if transparent is not set to true</code></pre>

        <p>Now we should see a pink glass table in our scene.</p>

        <p>Let's add a GUI to toggle our lights on and off:</p>

        <p>First, we need to add the following script to our HTML file to import the GUI library:</p>

        <pre><code class="HTML">&lt;script src="https://cdn.jsdelivr.net/npm/dat.gui/build/dat.gui.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Now we can create a GUI to toggle our lights on and off:</p>

        <pre><code class="javascript">// GUI setup
const gui = new dat.GUI();
const guiParams = {
    directionalLight: true, // Directional light toggle
    pointLight: true, // Point light toggle
};</code></pre>

        <p>Now we can add controls to our GUI to toggle our lights:</p>

        <pre><code class="javascript">gui.add(guiParams, 'directionalLight').name('Directional Light').onChange((value) => {
    light.visible = value;
});
      
gui.add(guiParams, 'pointLight').name('Point Light').onChange((value) => {
    light2.visible = value;
});</code></pre>

        <p>Now we should see a GUI with toggles for our directional and point lights.</p>

        <p>Let's try a different scene, a tree on some grass.</p>

        <p>Let's set our scene:</p>

        <pre><code class="javascript">// Scene setup
let width = 500;
let height = 400;
const scene = new THREE.Scene();
scene.background = new THREE.Color("skyblue"); // Set sky color

const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
const renderer = new THREE.WebGLRenderer();
renderer.setSize(width, height);
renderer.shadowMap.enabled = true;
renderer.antialias = true;
document.body.appendChild(renderer.domElement);
        
// Camera position
camera.position.set(0, 1, 10); // Set camera position so we can view the scene
       
// Add OrbitControls to enable mouse movement
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();




function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>And ensure that we have an light source:</p>

        <pre><code class="javascript">// Create a directional light
const light = new THREE.DirectionalLight("white", 1);
light.position.set(-10, 7, 7);
light.castShadow = true;
scene.add(light);</code></pre>

        <p>Now we need our grass ground:</p>

        <pre><code class="javascript">// Create grass geometry and material
const groundGeometry = new THREE.PlaneGeometry(100, 100);
const groundMaterial = new THREE.MeshStandardMaterial({ color: "green", side: THREE.DoubleSide });
const ground = new THREE.Mesh(groundGeometry, groundMaterial);
ground.rotation.x = -Math.PI / 2; // Rotate to make it flat
ground.receiveShadow = true;
scene.add(ground);</code></pre>

        <p>And now we'll write a function to make a pine tree. First, think about how a pine tree looks. A standard one
          has a triangular shape with a brown trunk and green leaves. We can create this using a ConeGeometry for the
          leaves and a CylinderGeometry for the trunk.</p>

        <p>Let's create a pine tree:</p>

        <pre><code class="javascript">// Function to create a pine tree
function createPineTree() {
    const tree = new THREE.Group(); // Group to hold tree components

    // Create the trunk of the tree
    const trunkGeometry = new THREE.CylinderGeometry(0.1, 0.25, 2, 8); // Radius top, radius bottom, height, number of segments
    const trunkMaterial = new THREE.MeshStandardMaterial({ color: "brown" }); // Brown color for the trunk
    const trunk = new THREE.Mesh(trunkGeometry, trunkMaterial);
    trunk.castShadow = true;
    trunk.position.y = 0.5;  // Position trunk at the bottom of the tree
    tree.add(trunk);  // Add trunk to the tree group

    // Create foliage using cones
    const foliageGeometry = new THREE.ConeGeometry(0.8, 1.5, 8); // Radius, height, number of segments
    const foliageMaterial = new THREE.MeshStandardMaterial({ color: "green" }); // Green color for foliage

    // Create multiple layers of foliage for a better shape
    for (let i = 0; i < 3; i++) {
        const foliage = new THREE.Mesh(foliageGeometry, foliageMaterial);
        foliage.position.y = 1 + i * 0.5;  // Adjust position for layers
        foliage.rotation.y = Math.random() * Math.PI;  // Random rotation
        foliage.castShadow = true;
        tree.add(foliage);  // Add foliage layer to tree
    }

    return tree;  // Return the complete tree object
}</code></pre>

        <p>Now we can add our pine tree to the scene:</p>

        <pre><code class="javascript">// Create a pine tree
const pineTree = createPineTree();
pineTree.scale.set(2,2,2); // Scale the tree to make it larger
pineTree.position.y = 1; // Position the tree above the ground
pineTree.position.x = -5; // Position the tree to the left
scene.add(pineTree);</code></pre>

        <p>Now we should see a pine tree (and it's shadow) on some grass in our scene.</p>

        <p>Let's add a simple house next to the tree:</p>

        <pre><code class="javascript">// Create a simple house
const houseStructure = new THREE.Group(); // to group all our objects
const houseGeometry = new THREE.BoxGeometry(4, 4, 4);
const houseMaterial = new THREE.MeshStandardMaterial({ color: 0xffcc00 });
const house = new THREE.Mesh(houseGeometry, houseMaterial);
house.castShadow = true;
house.position.set(0, 2, 0); 
houseStructure.add(house);

// Create a simple roof for the house
const roofGeometry = new THREE.ConeGeometry(2.8, 2, 4);
const roofMaterial = new THREE.MeshStandardMaterial({ color: 0x8B0000 });
const roof = new THREE.Mesh(roofGeometry, roofMaterial);
roof.castShadow = true;
roof.position.set(0, 5, 0);
roof.rotation.y = Math.PI / 4; // Rotate to place it correctly on top of the house
houseStructure.add(roof);

// Create a door for the house
const doorGeometry = new THREE.BoxGeometry(1, 2, 0.1);
const doorMaterial = new THREE.MeshStandardMaterial({ color: 0x654321 }); // Door color
const door = new THREE.Mesh(doorGeometry, doorMaterial);
door.position.set(0, 1, 2); // Position the door
houseStructure.add(door);

// Create windows for the house
const windowGeometry = new THREE.BoxGeometry(1, 1, 0.1);
const windowMaterial = new THREE.MeshPhongMaterial({ color: 0xadd8e6 }); // Window color

// Left window
const leftWindow = new THREE.Mesh(windowGeometry, windowMaterial);
leftWindow.position.set(-1.25, 2, 2); // Position left window
houseStructure.add(leftWindow);
        
// Right window
const rightWindow = new THREE.Mesh(windowGeometry, windowMaterial);
rightWindow.position.set(1.25, 2, 2); // Position right window
houseStructure.add(rightWindow);

scene.add(houseStructure);</code></pre>

        <p>Let's move the house forward a bit:</p>

        <pre><code class="javascript
">houseStructure.position.z = 2;</code></pre>

        <p>And we should move out camera back on the z-axis to reflect this.</p>

        <p>Now we should see a pine tree, a house, and some grass in our scene.</p>

        <p>Let's add a GUI to toggle our objects on and off:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
const guiParams = {
    tree: true, // Tree toggle
    house: true, // House toggle
};

gui.add(guiParams, 'tree').name('Pine Tree').onChange((value) => {
    pineTree.visible = value;
});

gui.add(guiParams, 'house').name('House').onChange((value) => {
    houseStructure.visible = value;
});</code></pre>

        <p>Now we should be able to toggle our pine tree and house visible by our GUI.</p>

        <p>Let's modify our house with some textures for the base, roof, and door.</p>

        <p>Let's create our texture loader:</p>

        <pre><code class="javascript">// Create a texture loader
const loader = new THREE.TextureLoader();</code></pre>

        <p>Now let's load our textures:</p>

        <pre><code class="javascript">const houseTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/brown%20house%20texture.jpg');
const roofTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/red%20roof%20texture.png');
const doorTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/pixel_door.png');</code></pre>

        <p>Now let's apply the textures to our house:</p>

        <pre><code class="javascript">const houseMaterial = new THREE.MeshStandardMaterial({ map: houseTexture });
const roofMaterial = new THREE.MeshStandardMaterial({ map: roofTexture });
const doorMaterial = new THREE.MeshStandardMaterial({ map: doorTexture });</code></pre>

        <p>And now our house should have textures applied to the base, roof, and door.</p>

        <p>Let's improve our roof with some texture tiling:</p>

        <pre><code class="javascript
">roofTexture.wrapS = THREE.RepeatWrapping; // Repeat the texture in the x-direction
roofTexture.wrapT = THREE.RepeatWrapping; // Repeat the texture in the y-direction
roofTexture.repeat.set(4, 4); // Repeat the texture 4 times in both directions</code></pre>

        <p>Now our roof should have a tiled texture applied to it.</p>

        <p>And there we have it! We've used what we've learned so far to successfully build a more complex (although
          still rather simple) scene both indoors and outdoors.</p>


      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Custom_Geometry">
      <br />
      <header><b>Custom Geometry</b></header>
      <article>
        <p>So far, we've used primitive geometries like BoxGeometry, ConeGeometry, and CylinderGeometry to create
          objects in Three.js.</p>

        <p>However, sometimes we may want to create custom geometries that are not built into Three.js.</p>

        <p>We've seen a bit of this with BufferGeometry, which allows us to create custom geometries by specifying the
          vertices and faces of the geometry.</p>

        <p>Another way to create custom geometries is to use the THREE.Shape class</p>

        <p>First, we need to define the shape. We can do this by creating a path using the
          <b>THREE.Shape</b> class.
        </p>

        <p>The THREE.Shape class allows us to define 2D shapes by specifying a series of points. This is similar to how
          we define paths in SVG, Adobe Illustrator, or like we did in HTML5 Canvas.</p>

        <h2>Example 1: Custom Triangle with THREE.Shape</h2>

        <p>Let's create a custom geometry using THREE.Shape:</p>

        <pre><code class="javascript">const tri = new THREE.Shape();
tri.moveTo(0, 1); 
tri.lineTo(1, -1);
tri.lineTo(-1, -1);</code></pre>

        <p>We've created a triangle shape with vertices at (0, 1), (1, -1), and (-1, -1).</p>

        <p>Now we can create a custom geometry using the shape:</p>

        <pre><code class="javascript">const geometry = new THREE.ShapeGeometry(tri);</code></pre>

        <p>THREE.ShapeGeometry() takes a shape as a parameter and creates a geometry based on that shape. We can then
          create a mesh using the custom geometry and a material and add it to the scene.</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ color: "blue" });
const mesh = new THREE.Mesh(geometry, material);
scene.add(mesh);</code></pre>

        <p>Now we should see a blue triangle in our scene.</p>

        <p>We can create a variety of custom geometries by defining different shapes and paths.</p>

        <p>Another way to create custom geometries is to use <b>ExtrudeGeometry</b>.</p>

        <p>ExtrudeGeometry allows us to create geometries by extruding a 2D shape along a path in 3D space. This is
          similar to how we create 3D objects by extruding 2D shapes in 3D modeling software.</p>

        <p>ExtrudeGeometry is useful for creating complex geometries like buildings, furniture, and other objects with
          depth.</p>

        <h2>Example 1.5: Pyramid with ExtrudeGeometry</h2>

        <p>Let's extrude our triangle to create a pyramid:</p>

        <pre><code class="javascript">const extrudeSettings = {
    steps: 1, // Number of steps to extrude the shape
    depth: 1, // Depth of the extrusion
    bevelEnabled: false, // Disable beveling
};

const pyramidGeometry = new THREE.ExtrudeGeometry(tri, extrudeSettings);
const pyramidMaterial = new THREE.MeshStandardMaterial({ color: "red" });
const pyramidMesh = new THREE.Mesh(pyramidGeometry, pyramidMaterial);
scene.add(pyramidMesh);</code></pre>

        <p>ExtrudeGeometry takes two parameters: the shape we want to extrude and extrusion settings.</p>
        <p>Extrusion settings include:</p>
        <ul>
          <li>steps: the number of steps to extrude the shape - this means how many times the shape is extruded along
            the path</li>
          <li>depth: the depth of the extrusion - this means how far the shape is extruded along the path</li>
          <li>bevelEnabled: whether to enable beveling which creates rounded edges</li>
        </ul>

        <h2>Example 2: Custom Cube with ExtrudeGeometry</h2>

        <p>Let's create a custom cube:</p>

        <pre><code class="javascript">const shape = new THREE.Shape();
shape.moveTo(0, 0); // Move to the starting point
shape.lineTo(0, 1); // Draw a line to the next point
shape.lineTo(1, 1); // Draw a line to the next point
shape.lineTo(1, 0); // Draw a line to the next point
shape.lineTo(0, 0); // Draw a line back to the starting point</code></pre>

        <p>Now we can create our custom geometry using ExtrudeGeometry:</p>

        <pre><code class="javascript">const extrudeSettings = {
    steps: 2, 
    depth: 1, 
    bevelEnabled: false, // Disable beveling which creates rounded edges
};

const geometry = new THREE.ExtrudeGeometry(shape, extrudeSettings);</code></pre>

        <p>Now we can create a mesh using our custom geometry:</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ color: "red" });
const mesh = new THREE.Mesh(geometry, material);
scene.add(mesh);</code></pre>

        <p>Now we should see a red extruded shape (cube) in our scene.</p>

        <h2>Example 3: Custom Star Shape with ExtrudeGeometry</h2>

        <p>For example, we can create a custom geometry by extruding a star shape:</p>

        <pre><code class="javascript">// Create a star shape
const starShape = new THREE.Shape();
starShape.moveTo(0, 0.5);
starShape.lineTo(0.15, 0.15);
starShape.lineTo(0.5, 0.15);
starShape.lineTo(0.2, -0.15);
starShape.lineTo(0.3, -0.5);
starShape.lineTo(0, -0.25);
starShape.lineTo(-0.3, -0.5);
starShape.lineTo(-0.2, -0.15);
starShape.lineTo(-0.5, 0.15);
starShape.lineTo(-0.15, 0.15);
starShape.lineTo(0, 0.5);

// Extrude the star shape
const extrudeSettings = {
    steps: 2,
    depth: 0.1,
    bevelEnabled: false,
};

const starGeometry = new THREE.ExtrudeGeometry(starShape, extrudeSettings);
const starMaterial = new THREE.MeshStandardMaterial({ color: "yellow" });
const starMesh = new THREE.Mesh(starGeometry, starMaterial);
starMesh.scale.set(4,4,4);
scene.add(starMesh);</code></pre>

        <p>Now we should see a yellow extruded star shape in our scene.</p>

        <p>Or let's create a simple heart shape.</p>

        <h2>Example 4: Custom Heart Shape with ExtrudeGeometry</h2>

        <p>To draw a heart shape, we need to define a series of points that form the shape of a heart. The easiest way
          to do this is
          with bezier curves.</p>

        <p>To refresh your memory, a bezier curve is a parametric curve that is defined by a set of control points.
          In Three.js (like in HTML5 Canvas), we can use the bezierCurveTo() method of the Shape class to create bezier
          curves.</p>

        <p>The parameters of bezierCurveTo() are the x and y coordinates of the control point and the x and y
          coordinates of the end point of the curve.</p>

        <pre><code class="javascript">const heartShape = new THREE.Shape();
heartShape.moveTo( 2.5, 2.5 );
heartShape.bezierCurveTo( 2.5, 2.5, 2.0, 0, 0, 0 ); 
heartShape.bezierCurveTo( - 3.0, 0, - 3.0, 3.5, - 3.0, 3.5 );
heartShape.bezierCurveTo( - 3.0, 5.5, - 1.0, 7.7, 2.5, 9.5 );
heartShape.bezierCurveTo( 6.0, 7.7, 8.0, 5.5, 8.0, 3.5 );
heartShape.bezierCurveTo( 8.0, 3.5, 8.0, 0, 5.0, 0 );
heartShape.bezierCurveTo( 3.5, 0, 2.5, 2.5, 2.5, 2.5 );

const extrudeSettings = {
    steps: 2,
    depth: 1,
    bevelEnabled: true,
}; 
const geometry = new THREE.ExtrudeGeometry( heartShape, extrudeSettings );
geometry.rotateX(Math.PI * 1);
geometry.center();

const material = new THREE.MeshStandardMaterial({color: "#FF69B4"});

const heartMesh = new THREE.Mesh( geometry, material);

heartMesh.scale.set(0.25, 0.25, 0.25);

scene.add(heartMesh);</code></pre>

        <p>It's starting to come along but it's not quite right. We can adjust even further to get the shape we're
          after:</p>

        <pre><code class="javascript">const extrudeSettings = {
    steps: 2,
    depth: 1,
    bevelEnabled: true,
    curveSegments: 40, // Number of points on the curves
    bevelSegments: 20, // Number of segments for the bevel
    bevelThickness: 0.75, // Thickness of the bevel
    bevelSize: 0.75 // Size of the bevel
};</code></pre>

        <p>These modifications should give us a more accurate heart shape by adding more curve segments and bevel
          segments
          to the extrusion.</p>

        <p>Now we should see a pink extruded heart shape in our scene.</p>

        <p>We can also "punch" a shaped hole into other shapes.</p>

        <h2>Example 5: Custom Triangle with Hole using ExtrudeGeometry</h2>

        <pre><code class="javascript">const tri = new THREE.Shape();
tri.moveTo(0, 2);
tri.lineTo(2, -2);
tri.lineTo(-2, -2);
// add the hole
const hole = new THREE.Shape();
hole.arc(0, -0.8, 1.0, 0, Math.PI * 2);

tri.holes.push(hole); // push to the shapes holes array

const extrudeSettings = {
    steps: 2,
    depth: 1,
    bevelEnabled: false
};
const geometry = new THREE.ExtrudeGeometry(tri, extrudeSettings);
geometry.rotateX(Math.PI * 1); 
geometry.center();

const mesh = new THREE.Mesh(geometry, new THREE.MeshNormalMaterial());

scene.add(mesh);</code></pre>

        <p>This code creates a triangle with a hole in the center. The hole is created by adding a circle shape to the
          triangle's holes array and extruding the shape with the hole.</p>

        <p>We can also create custom geometries by setting the vertices of the geometry directly using the
          <b>setFromPoints</b> method of the Shape class.
        </p>

        <p>We can set the state of a path by way of an
          array of <b>THREE.vector2</b> class objects. The Vector2 class is a class that is used to represent a point
          in 2D space.</p>

        <p>We can create an array of these Vector2 objects by any means
          that we would like, and then we can just create a shape, and then call the set from points method off of the
          shape
          and pass the array of Vector2 objects.</p>

        <h2>Example 6: Custom Parabola Shape with ExtrudeGeometry</h2>

        <pre><code class="javascript">// creating an array of THREE.Vector2 objects
const points = []; // array to hold the points
const len = 100;
let i = 0;
while(i < len){ // loop to create the points
    const a = i / len;
    const x = -3  + 6 * a; // x position that will be used to create a parabola
    const y = Math.sin( Math.PI * 1.0 * a ) * 4; //to create a parabola
    points.push(new THREE.Vector2(x, y));
    i += 1;
}
const shape = new THREE.Shape();
// creating the path by using the set from points method with the array of THREE.Vector2 objects
shape.setFromPoints(points);
  
const extrudeSettings = {
    steps: 2,
    depth: 1.5,
    bevelEnabled: false
};
const geometry = new THREE.ExtrudeGeometry( shape, extrudeSettings );
geometry.center(); //to center the geometry
  
const material = new THREE.MeshStandardMaterial({ color: "blue" });
const mesh = new THREE.Mesh( geometry, material );
  
scene.add(mesh);</code></pre>


        <p>And there we have it! We've used what we've learned so far to create custom geometries using THREE.Shape and
          ExtrudeGeometry.</p>

        <p>Now think about what other custom geometries you could create using these methods. What shapes could you
          create? What objects could you model?</p>

        <p>What other methods could you use to create custom geometries in Three.js?</p>

        <p>What are some other ways you could modify the custom geometries we've created?</p>

        <p>What are some other ways you could use custom geometries in your Three.js projects?</p>


      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Loading_Models">
      <br />
      <header><b>Loading Models</b></header>
      <article>
        <p>So far we've created geometries using Three.js's built-in geometries and custom geometries.</p>

        <p>But as aforementioned the 3D Graphics pipeline actually starts with a 3D model. A 3D model is a digital
          representation of a 3D object or scene.</p>

        <p>To create beautiful 3D models, a sophisticated modeling program is required, like Blender, Maya, 3ds Max, or
          SketchUp.
          We can use Three.js to build any kind of 3D application, however, building a modeling app from scratch would
          be a huge amount of work. A
          much simpler solution is to use an existing program and export your work for use in Three.js ... or, "cheat",
          and
          download any of the millions of amazing models and other scene assets that are available for free in many
          places around the web.</p>

        <h2>Types of 3D Models</h2>

        <p>Once a 3D model is created, it can be exported to a file format that can be read by a 3D graphics library
          like Three.js.</p>

        <p>Three.js supports a variety of 3D model file formats including:</p>
        <ul>
          <li><b>OBJ</b> (Wavefront)</li>
          <li><b>glTF</b> (GL Transmission Format)</li>
          <li><b>FBX</b> (Filmbox)</li>
          <li><b>DAE</b> (Collada)</li>
          <li><b>STL</b> (Stereolithography)</li>
          <li><b>3DS</b> (3D Studio)</li>
          <li><b>PLY</b> (Polygon File Format)</li>
        </ul>

        <p>Let's talk about each of these formats:</p>

        <p><b>OBJ</b> (Wavefront) is a simple text-based file format that stores information about the 3D model's
          geometry,
          materials, and textures.</p>
        <p><b>glTF</b> (GL Transmission Format) is a binary file format that stores information about the 3D model's
          geometry,
          materials, textures, and animations. GLTF is designed to be compact and efficient for web-based 3D
          applications.</p>
        <p><b>FBX</b> (Filmbox) is a proprietary file format developed by Autodesk that stores information about the 3D
          model's
          geometry, materials, textures, animations, and other data. FBX is widely used in the film and game industry.
        </p>
        <p><b>DAE</b> (Collada) is an XML-based file format that stores information about the 3D model's geometry,
          materials,
          textures, animations, and other data. Collada is designed to be an open and interoperable format for 3D
          content.</p>
        <p><b>STL</b> (Stereolithography) is a file format that stores information about the 3D model's geometry as a
          series of
          triangles. STL is commonly used for 3D printing and rapid prototyping.</p>
        <p><b>3DS</b> (3D Studio) is a file format developed by Autodesk that stores information about the 3D model's
          geometry,
          materials, textures, and animations. 3DS is widely used in the film and game industry.</p>
        <p><b>PLY</b> (Polygon File Format) is a file format that stores information about the 3D model's geometry as a
          series
          of vertices, edges, and faces. PLY is commonly used for 3D scanning and computer graphics research.</p>

        <p>There have been many attempts at creating a standard 3D asset exchange format over the last thirty years or
          so. FBX, OBJ (Wavefront) and DAE (Collada) formats were the most popular of these until recently, although
          they all have problems that prevented their widespread adoption.</p>

        <p>For example, OBJ doesn't support animation, FBX is a closed format that belongs to Autodesk, and the Collada
          spec is overly complex, resulting in large files that are difficult to load.</p>

        <p>However, recently, a newcomer called <b>glTF</b> has become the de facto standard format for exchanging 3D
          assets on
          the web. <b>glTF (GL Transmission Format)</b>, sometimes referred to as the JPEG of 3D, was created by the
          Kronos
          Group, the same people who are in charge of WebGL, OpenGL, and a whole host of other graphics APIs. </p>

        <p>Originally released in 2017, glTF is now the best format for exchanging 3D assets on the web, and in many
          other fields. In this class, we will always use glTF, and if possible, you should do the same. It's designed
          for sharing models on the web, so the file size is as small as possible and your models will load quickly.</p>

        <p>glTF files can contain models, animations, geometries, materials, lights, cameras, or even entire scenes.
          This means you can create an entire scene in an external program then load it into Three.js.</p>



        <h2>Types of glTF Files</h2>

        <p>glTF files come in standard and binary form. These have different extensions:</p>

        <ul>
          <li>Standard <b>.gltf files</b> are uncompressed and may come with an extra .bin data file</li>
          <li>Binary <b>.glb files</b> include all data in one single file.</li>
        </ul>

        <p>Both standard and binary glTF files may contain textures embedded in the file or may reference external
          textures. Since <b>binary .glb files are considerably smaller</b>, it's best to use this type. On the other
          hand,
          <b>uncompressed .gltf are easily readable in a text editor</b>, so they may be useful for debugging purposes.
        </p>

        <h2>Loading 3D Models</h2>

        <p>Now that we know about the different types of 3D model file formats, let's learn how to
          load a 3D model into a Three.js scene.</p>

        <p>We'll start with three simple and beautiful models of a parrot, a flamingo, and a stork, created by the
          talented people at
          <a href="https://mirada.com/">mirada.com</a>. These three models are low poly, meaning they'll run on even the
          most low-power
          of mobile devices, and they are even animated.
        </p>

        <p>We will load Parrot.glb, Flamingo.glb, and Stork.glb and then add the bird-shaped meshes each file contains
          to our scene.
          Later, we will learn how to play the flying animation that is included with each bird.</p>

        <p>In order to load a 3D model into Three.js, we need to use a loader. A loader is a class that reads a 3D model
          file and creates a Three.js object from it. More specifically, since we're using glTF models, we'll use the
          GLTFLoader class.</p>

        <p>Since glTF is relatively new, your favorite application might not have an exporter yet. In that
          case, you can convert your models to glTF before using them, or use another loader such as the FBXLoader or
          OBJLoader. All Three.js loaders work the same way, so if you do need to use another loader, everything will
          still apply, with only minor differences.</p>

        <p>To load glTF files, first, we need to add the GLTFLoader plugin to our environment.
          This works the same way as adding the OrbitControls plugin:</p>

        <pre><code class="HTML">&lt;script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/loaders/GLTFLoader.js"&gt;&lt;/script&gt;</code></pre>

        <p>Now we can now create an instance of the GLTFLoader class:</p>

        <pre><code class="javascript">const loader = new THREE.GLTFLoader();</code></pre>

        <p>Next, we can use the load() method of the loader to load a glTF model file:</p>

        <pre><code class="javascript">loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Parrot.glb', function (gltf) {
    const parrot = gltf.scene;
    scene.add(parrot);
});</code></pre>

        <p>The load() method takes two parameters: the URL of the model file and a callback function that is called when
          the model is loaded. The callback function receives a gltf object that contains the loaded 3D model. We can
          access the 3D model scene by using gltf.scene and then add it to our Three.js scene.</p>

        <p>Let's add the stork and flamingo models to our scene:</p>

        <pre><code class="javascript">loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Stork.glb', function (gltf) {
    const stork = gltf.scene;
    scene.add(stork);
});

loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Flamingo.glb', function (gltf) {
    const flamingo = gltf.scene;
    scene.add(flamingo);
});</code></pre>

        <p>It is possible for models loaded from a glTF file to have a position already specified, but that's not the
          case here, so all three models start at the point (0,0,0), all jumbled together on top of each other.</p>

        <p>We'll adjust the position of each bird to make it look like they are flying in formation:</p>

        <pre><code class="javascript">parrot.position.set(0, 20, 2.5); //before we add the parrot to the scene
stork.position.set(70, -40, -10); //before we add the stork to the scene
flamingo.position.set(-70, 0, -10); //before we add the flamingo to the scene</code></pre>

        <p>Now the birds should be in a more visually appealing formation.</p>

        <p>We will revisit these birds later but first let's pivot back to stationary objects like furniture.</p>

        <h2>Importing Models By Other Artists</h2>

        <p>If you are interested in creating your own 3D models, you can start by learning how to use one of these
          software programs. However, for those who are not interested in creating their own 3D models, there are many
          websites that offer free and paid 3D models that you can download and use in your projects.</p>

        <p>Some of these include:</p>

        <ul>
          <li><a href="https://poly.pizza/" target="_blank">Poly Pizza</a></li>
          <li><a href="https://www.turbosquid.com/" target="_blank">TurboSquid</a></li>
          <li><a href="https://sketchfab.com/" target="_blank">Sketchfab</a></li>
          <li><a href="https://www.cgtrader.com/" target="_blank">CGTrader</a></li>
          <li><a href="https://www.blendswap.com/" target="_blank">Blend Swap</a></li>
          <li><a href="https://free3d.com/" target="_blank">Free3D</a></li>
        </ul>

        <p>We'll use 3D models from Poly Pizza in this class, as they offer a variety of free 3D models that we can
          use in our projects as long as we credit the creator under the Creative Commons license. However you can use
          any of the above websites to find 3D models for your projects or create your own.</p>

        <p>Let's load a glTF model of a table from Poly Pizza into our scene:</p>

        <pre><code class="javascript"> // Load GLB Table Model
const loader = new THREE.GLTFLoader();
//Model Credit: Table Round Small by Quaternius
loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Table%20Round%20Small.glb', function (gltf) {
    const tableModel = gltf.scene;
    tableModel.scale.set(20, 20, 20); // Scale the model
    tableModel.position.y = -9.5; // Position the model
    scene.add(tableModel);
});</code></pre>

        <p>Now we should see a 3D model of a table in our scene.</p>


        <p>Apart from transformations like scaling up (or down) the model we can also adjust how the model looks by
          changing the material of the model.</p>

        <p>Let's change the material of the table model as we like it:</p>

        <pre><code class="javascript">// Load GLB Table Model
const loader = new THREE.GLTFLoader();
//Model Credit: Table Round Small by Quaternius
loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Table%20Round%20Small.glb', function (gltf) {
    const tableModel = gltf.scene;
    tableModel.scale.set(20, 20, 20); // Adjust table model scale if needed
    tableModel.position.y = -9.5; // Adjust table model position if needed
    const tableMaterial = new THREE.MeshStandardMaterial({ color: "white" });
    tableModel.traverse((child) => {
        if (child.isMesh) {
            child.material = tableMaterial;
            child.castShadow = true; // The table will cast shadows
            child.receiveShadow = true; // The table will receive shadows
        }
    });

    scene.add(tableModel);
});</code></pre>

        <p>This code creates a new MeshStandardMaterial with a white color and then uses the traverse() method to
          iterate over all the child objects of the table model. For each child object that is a mesh, it sets the
          material to the new material.</p>

        <p>Now we should see a white table model in our scene.</p>

        <p>Let's add another model, a flower pot, to sit on the table:</p>

        <pre><code class="javascript">// Load GLB Flower Pot Model
const loader2 = new THREE.GLTFLoader();
//Model Credit: Flower by jeremy [CC-BY] via Poly Pizza
loader2.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Flower.glb', function (gltf) {
    const flowerPotModel = gltf.scene;
    flowerPotModel.scale.set(20, 20, 20); // Adjust flower pot model scale if needed
    flowerPotModel.position.y = -5; // Adjust flower pot model position if needed
    // Set shadow properties
    flowerPotModel.traverse((child) => {
        if (child.isMesh) {
            child.castShadow = true; // The flower pot will cast shadows
            child.receiveShadow = true; // The flower pot can receive shadows
        }
    });   

    scene.add(flowerPotModel);
});</code></pre>

        <p>This code loads a GLTF model of a flower pot and adds it to the scene. We can adjust the scale and position
          of the flower pot model as needed.</p>

        <p>Now we should see a flower pot model on the table in our scene.</p>

        <p>Let's group the models together so they are easier to reposition:</p>

        <pre><code class="javascript">// Create a group to hold the table and flower pot models
const modelsGroup = new THREE.Group();
//load table model
modelsGroup.add(tableModel); // Add the table model to the group

//load flower pot model
modelsGroup.add(flowerPotModel); // Add the flower pot model to the group

// Position the group
modelsGroup.position.set(0, 0, 0);
modelsGroup.rotation.y = Math.PI / 2;

scene.add(modelsGroup);</code></pre>

        <p>This code creates a new Group object to hold the table and flower pot models. It adds the table and flower
          pot models to the group and then positions the group in the scene.</p>

        <p>This now makes it easier to move around. Let's create a room for the models to be displayed in:</p>

        <p>First we need a floor, then three walls, and a ceiling:</p>

        <pre><code class="javascript">// Create a floor
const floorGeometry = new THREE.PlaneGeometry(100, 100);
const floorMaterial = new THREE.MeshStandardMaterial({ color: "gray", side: THREE.DoubleSide });
const floor = new THREE.Mesh(floorGeometry, floorMaterial);
floor.rotation.x = -Math.PI / 2; // Rotate the floor to be horizontal
floor.position.y = -10; // Position the floor below the models
scene.add(floor);

// Create walls
const wallGeometry = new THREE.BoxGeometry(100, 60, 1);
const wallMaterial = new THREE.MeshStandardMaterial({ color: "white" });

const leftWall = new THREE.Mesh(wallGeometry, wallMaterial);
leftWall.rotation.y = -Math.PI / 2;
leftWall.position.set(-50, 20, 0); // Position the left wall
scene.add(leftWall);

const rightWall = new THREE.Mesh(wallGeometry, wallMaterial);
rightWall.rotation.y = -Math.PI / 2;
rightWall.position.set(50, 20, 0); // Position the right wall
scene.add(rightWall);

const backWall = new THREE.Mesh(wallGeometry, wallMaterial);
backWall.position.set(0, 20, -50); // Position the back wall
scene.add(backWall);

// Create a ceiling
const ceilingGeometry = new THREE.PlaneGeometry(100, 100);
const ceilingMaterial = new THREE.MeshStandardMaterial({ color: "white", side: THREE.DoubleSide});
const ceiling = new THREE.Mesh(ceilingGeometry, ceilingMaterial);
ceiling.rotation.x = Math.PI / 2; // Rotate the ceiling to be horizontal
ceiling.position.y = 50; // Position the ceiling above the models
scene.add(ceiling);</code></pre>

        <p>This code creates a floor, three walls, and a ceiling to create a room for the models to be displayed in. The
          floor is a horizontal plane below the models, the walls are vertical planes around the models, and the ceiling
          is a horizontal plane above the models.</p>

        <p>Now we should see the models displayed in a room with a floor, walls, and ceiling.</p>

        <p>Let's add some lights to the scene to illuminate the models:</p>

        <pre><code class="javascript">// Create a directional light
const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
directionalLight.position.set(0, 10, 0); // Position the light
directionalLight.castShadow = true; // Enable shadow casting  
scene.add(directionalLight);

// Create an ambient light
const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
scene.add(ambientLight);</code></pre>

        <h2>Model Viewer</h2>

        <p>Now that we have loaded and positioned models in the scene, we can create a simple model viewer that
          allows us to interact with the models using the mouse.</p>

        <p>Let's revisit our three birds.</p>

        <p>Notice how we already have radio buttons for the different models and a button to toggle the animation of the
          birds. We'll use these to toggle between the models.</p>

        <p>These models were loaded from the binary glTF files parrot.glb, flamingo.glb, and stork.glb. Alongside the
          bird models, each of these files also contains an animation clip of the bird flying.</p>

        <h2>Animated Models</h2>

        <p>The Three.js animation system is a <b>complete animation mixing desk</b>. Using this system you can animate
          virtually any aspect of an object, such as position, scale, rotation, a material's color or opacity, the bones
          of a skinned mesh, morph targets, and many other things besides. </p>

        <p>We can also blend and mix animations, so, for example, if we have a "walk" animation and a "run" animation
          attached to a human character you can make the character speed up from a walk to a run by blending these
          animations.</p>

        <p><b>The animation system uses keyframes to define animations.</b></p>

        <p>Keyframes are snapshots of the object's state at a particular point in time. The animation system then
          interpolates between these keyframes to create smooth animations.</p>

        <p>To create an animation, we set keyframes at particular points in time, and then the animation system fills in
          the gaps for us using a process known as <b>tweening</b>.</p>

        <p>To animate a bouncing ball, for example, you can specify the points at the top and bottom of the bounce, and
          the ball will smoothly animate across all the points in between.</p>

        <p>The amount of keyframes you need depends on the complexity of the animation. A very simple animation may only
          need one keyframe per second, or less, while a complex animation will need more, up to a maximum of sixty
          keyframes per second (any more than this will be ignored on a standard 60Hz display).</p>

        <p>The animation system is built from a number of components that work together to create animations, attach
          them to objects in the scene, and control them.</p>

        <p>We'll split these into two categories, <b>animation creation</b>, and <b>animation playback and control</b>.
          We'll briefly introduce both categories here, and then we'll use our new knowledge to set up the flying
          animations that we have loaded from the three glTF files.</p>

        <p>Normally to animate a mesh in Three.js we would need to create a keyframe animation, but luckily for us, the
          models we
          are using already have animations attributed to them.</p>

        <p>To animate an object such as a mesh using the animation system, we must connect it to an
          <b>AnimationMixer</b>. From
          here on, we'll refer to an AnimationMixer as simply a mixer. We need <b>one mixer for each animated object in
            the
            scene</b>. The mixer does the technical work of making the model move in time to the animation clip, whether
          that
          means moving the feet, arms, and hips of a dancer, or the wings of a flying bird.
        </p>

        <pre><code class="javascript">let width = 500;
let height = 400;
const scene = new THREE.Scene();
scene.background = new THREE.Color("green");
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
const renderer = new THREE.WebGLRenderer({ antialias: true });
renderer.setSize(width, height);
document.body.appendChild(renderer.domElement);

camera.position.set(0, 30, 200);
const controls = new THREE.OrbitControls(camera, renderer.domElement);

// Lights
const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
scene.add(directionalLight);
const ambientLight = new THREE.AmbientLight(0xffffff, 0.5);
scene.add(ambientLight);

// Loader to load models
const loader = new THREE.GLTFLoader();

const models = {
  stork: null,
  flamingo: null,
  parrot: null
};

let mixers = {}; // Store mixers for each model
let activeMixer = null;
let animating = false;

// Load models
function loadModel(url, name) {
  loader.load(url, function (gltf) {
    models[name] = gltf.scene;
    // If there are animations, create a mixer
    if (gltf.animations && gltf.animations.length) {
      const mixer = new THREE.AnimationMixer(gltf.scene);
      gltf.animations.forEach((clip) => {
        mixer.clipAction(clip).play();
      });
      mixers[name] = mixer; // Store the mixer
    }
    const box = new THREE.Box3().setFromObject(gltf.scene); // Get the bounding box of the model
    const center = box.getCenter(new THREE.Vector3()); // Get the center of the model
    models[name].position.sub(center); // Center the model at the origin
  });
}

loadModel(
  "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Stork.glb",
  "stork"
);
loadModel(
  "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Flamingo.glb",
  "flamingo"
);
loadModel(
  "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Parrot.glb",
  "parrot"
);

// Animation handling
function animate() {
  requestAnimationFrame(animate);

  // Only update the active mixer if the animate checkbox is checked
  const animateCheckbox = document.getElementById("animate");
  if (activeMixer && animateCheckbox.checked) {
    activeMixer.update(0.005); // Update animation based on the current frame time
  }

  renderer.render(scene, camera);
}

// Selection of the model
function selectModel(modelName) {
  Object.keys(models).forEach((name) => {
    if (models[name]) {
      if (name === modelName) {
        scene.add(models[name]);
        activeMixer = mixers[name] || null; // Set active mixer

      } else {
        scene.remove(models[name]);
      }
    }
  });
}

// Event listeners for model selection
document.getElementById("storkCheckbox").onchange = function () {
  selectModel("stork");
};

document.getElementById("flamingoCheckbox").onchange = function () {
  selectModel("flamingo");
};

document.getElementById("parrotCheckbox").onchange = function () {
  selectModel("parrot");
};

// Animation checkbox handling
document.getElementById("animate").onchange = function () {
  const animateCheckbox = this;
  if (animateCheckbox.checked && activeMixer) {
    startAnimation();
  } else {
    pauseAnimation();
  }
};

function startAnimation() {
  if (!animating && activeMixer) {
    animating = true;
    animate(); // Start the animation loop
  }
}

function pauseAnimation() {
  animating = false;
  // The animation will pause when the animation loop stops
}

// Start the rendering loop
animate();</code></pre>

        <p>Apart from simply loading the models, we have now also added the ability to toggle between the models and
          toggle the animation of the birds. We have also added the ability to start and pause the animation of the
          birds by checking the animate checkbox.</p>

        <p>We contain each of the models' animations in a mixer object and store the mixers in the mixers object. We
          then set the activeMixer to the mixer of the selected model. We update the active mixer in the animate()
          function to update the animations based on the current frame time.</p>

        <p>When the animate checkbox is selected and the activeMixer is not null, the startAnimation() function is
          called
          to start the animation loop. When the animate checkbox is deselected, the pauseAnimation() function is called
          to
          pause the animation.</p>

        <p>Now we have a simple model viewer.</p>

        <p>Let's add another model, a horse, to our viewer:</p>

        <p>Our horse model is stored in the file Horse.glb, so it is a binary glTF file with accompanying animations
          like our bird models.</p>

        <p>We will need to add a radio button for the horse in our HTML:</p>

        <pre><code class="HTML">&lt;label style="margin-left: 10px"/&gt;&lt;input type="radio" name="model" id="horseCheckbox"&gt;Horse&lt;/label&gt;</code></pre>

        <p>Add our horse to our models:</p>

        <pre><code class="javascript">const models = {
  stork: null,
  flamingo: null,
  parrot: null,
  horse: null // Add horse model
};</code></pre>

        <p>Then we will load the horse model and add it to our scene:</p>

        <pre><code class="javascript">loadModel("https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/master/courses/CISC3620/models/Horse.glb",
    "horse"
);</code></pre>

        <p>And add an event listener for the horse radio button:</p>

        <pre><code class="javascript">document.getElementById("horseCheckbox").onchange = function () {
  selectModel("horse");
};
</code></pre>

        <p>And lastly our horse model is actually larger than the rest of the models (to be understood as a horse is
          more than three times the size of a bird), so we will scale it down:</p>

        <p>In our loadModel() function, let's add:</p>

        <pre><code class="javascript">if (name === 'horse') {
    models[name].scale.set(0.5,0.5,0.5);
}</code></pre>

        <p>This will scale our horse model down to a more convenient size.</p>

        <p>Now we should see a horse model in our model viewer.</p>

        <p>Our model viewer is now complete.</p>

        <p>This code should work, however, if the animate checkbox is checked and then unchecked and AGAIN late
          rechecked, you might notice something interesting. The speed of the animation has increased from when we last
          saw it.</p>

        <p>This is because the animation is being updated based on the current frame time, and when we recheck the
          animate checkbox, the animation is being updated based on the current frame time again.</p>
        <p>To fix this, we can simply set the animation time to 0 when we pause the animation:</p>

        <pre><code class="javascript">function pauseAnimation() {
  animating = false;
  if (activeMixer) {
    activeMixer.time = 0; // Reset animation time to 0
  }
  // The animation will pause when the animation loop stops
} </code></pre>
        <p>This will reset the animation time to 0 when we pause the animation, so when we recheck the animate checkbox,
          the animation will start from the beginning.</p>

        <p>However, for animations, a more appropriate solution would be what we refer to as <b>delta time</b>. Delta
          time is
          <b>the time between the last frame and the current frame</b>, and it is used to update the animation based on
          the
          current frame time.
        </p>
        <p>We can use the delta time to update the animation based on the current frame time, so when we recheck the
          animate checkbox, the animation will start from the last frame.</p>
        <p>To do this, we can simply add a variable to store the last time and then use the delta time to update the
          animation:</p>

        <pre><code class="javascript">let lastTime = 0; // Store the last time
...
...
// Animation handling
function animate(timestamp) {
    requestAnimationFrame(animate);

    // Calculate delta time
    const delta = timestamp - lastTime;
    lastTime = timestamp;

    // Only update the active mixer if the animate checkbox is checked
    const animateCheckbox = document.getElementById("animate");
    if (activeMixer && animateCheckbox.checked) {
        activeMixer.update(delta * 0.0025); // Update animation based on delta time in seconds
    }

    renderer.render(scene, camera);
}</code></pre>

        <p>This will update the animation based on the delta time in seconds, so when we recheck the animate checkbox,
          the animation will start from the last frame.</p>

        <p>We need to update our startAnimation() to reflect this:</p>

        <pre><code class="javascript">function startAnimation() {
    if (!animating && activeMixer) {
        animating = true;
        lastTime = performance.now(); // Initialize lastTime when starting the animation
        animate(lastTime); // Start the animation loop
    }
}</code></pre>

        <p>And lastly, update our animate() call to:</p>

        <pre><code class="javascript">animate(performance.now());</code></pre>

        <p>Which will pass the current time to the animate() function.</p>
        <p>Now we should see the animation speed is consistent regardless of how many times we check and uncheck the
          animate checkbox.</p>































      </article>
      <br />
    </section>

    <hr />

    <section class="main-section" id="Global_Lighting_Models">
      <br />
      <header><b>Global Lighting Models</b></header>
      <article>
        <p>So far when we speak about lighting, we have only been speaking of <b>local lighting models</b>.</p>

        <p>Local lighting models are perfect for a pipeline architecture like OpenGL's, because very little information
          is taken into account in choosing the RGB. This enhances speed at the price of quality. To determine the color
          of a polygon, we need the following information:</p>
        <ul>
          <li><b>material</b>: what kind of stuff is the object made of? Blue silk is different from blue jeans. Blue
            jeans are different from black jeans.</li>
          <li><b>surface geometry</b>: is the surface curved? How is it oriented? What direction is it facing? How would
            we even define the direction that a curved surface is facing?</li>
          <li><b>lights</b>: what lights are in the scene? Are they colored? How bright are they? What directions does
            the light go?</li>
        </ul>

        <p>But it does not take into account the light that is reflected from other objects in the scene.</p>

        <p>Global lighting models take into account the properties of the whole scene and account interactions of light
          with objects in the room.</p>

        <p>For example:</p>

        <ul>
          <li>light will bounce off one object and onto another, lighting it</li>
          <li>objects may block light from a source</li>
          <li>shadows may be cast</li>
          <li>reflections may be cast</li>
          <li>diffraction may occur</li>
        </ul>

        <p>Global lighting algorithms fall into two basic categories, <b>radiosity</b> and <b>ray-tracing</b>
          algorithms:</p>

        <h3>Radiosity</h3>

        <p>Using <b>radiosity</b>, <b>any surface that is not completely black is treated as a light source, as if it
            glows</b>.</p>

        <p>Of course, the color that it emits depends on the color of light that falls on it. The light falling on the
          surface is determined by direct lighting from the light sources in the scene and also indirect lighting from
          the other objects in the scene. Thus, every object's color is determined by every other object's color.</p>

        <p>You can see the dilemma: how can you determine what an object's color is if it depends on another object
          whose color is determined by the first object's color? How to escape?</p>

        <p><b>Radiosity algorithms typically work by iterative improvement</b> (successive approximation): first
          handling
          direct lighting, then primary effects (other objects' direct lighting color), then secondary effects (other
          objects' indirect lighting color) and so on, until there is no more change.</p>

        <h3>Ray-tracing</h3>

        <p>The color and brightness of an object within a scene are predominantly determined by how light interacts with
          the material of the object.</p>

        <p>Light consists of photons, electromagnetic particles that embody both electric and magnetic properties. These
          particles carry energy and oscillate similarly to sound waves, traveling in direct lines.</p>

        <p>Sunlight is a prime example of a natural light source emitting photons. When photons encounter an object,
          they can be absorbed, reflected, or transmitted, with the outcome varying depending on the material's
          properties.</p>

        <p>However, a universal principle across all materials is the conservation of photon count: the sum of absorbed,
          reflected, and transmitted photons must equal the initial number of incoming photons.</p>

        <p>For instance, if 100 photons illuminate an object's surface, the distribution of absorbed and reflected
          photons must total 100, ensuring energy conservation.</p>

        <p>Materials are broadly categorized into two types: <b>conductors</b>, which are metals, and
          <b>dielectrics</b>, encompassing non-metals such as glass, plastic, wood, and water.
        </p>

        <p>Interestingly, dielectrics are insulators of electricity, with even pure water acting as an insulator. These
          materials may vary in their transparency, with some being completely opaque and others transparent to certain
          wavelengths of electromagnetic radiation, like X-rays penetrating human tissue.</p>

        <p>Moreover, materials can be composite or layered, combining different properties. For example, a wooden object
          might be coated with a transparent layer of varnish, giving it a simultaneously diffuse and glossy appearance,
          similar to the effect seen on colored plastic balls.</p>

        <p>This complexity in material composition adds depth and realism to the rendered scene by mimicking the
          multifaceted interactions between light and surfaces in the real world.</p>

        <p><b>Ray-tracing is essentially a system of simulating how light travels, interacts with various objects in the
            environment, and ultimately reaches our eyes</b>.</p>

        <p>Ray-tracing simulates how light behaves in the real world by <b>tracing the path of light rays as they
            interact
            with objects in a scene</b>. In simple terms, ray tracing starts with a virtual camera that "shoots" rays of
          light
          into a 3D scene. Each ray travels from the camera's origin through a pixel and then into the virtual scene
          until it hits a diffuse surface. As the ray travels through the scene, it interacts with objects it encounter.
        </p>

        <p>We can break this down into two concepts: <b>forward ray-tracing</b> and <b>backward ray-tracing</b>.
        </p>

        <h3>Forward Ray-Tracing</h3>

        <p><b>Forward ray-tracing</b> or <b>light tracing</b> follows the light particles (photons) from the light
          source to the object. Although
          forward ray tracing can most accurately determine the coloring of each object, it is <b>highly
            inefficient</b>. This
          is because many rays from the light source never come through the viewplane and into the eye. Tracking every
          light ray from the light source down means that <b>many rays will go to waste because they never contribute to
            the final image as seen from the eye</b>.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/forward%20ray%20tracing.png"
          alt="Forward Ray Tracing" />

        <p>In the image above, countless photons emitted by the light source hit the green sphere, but only one will
          reach the eye's surface.</p>

        <p>Consider the analogy of attempting to paint a teapot by dotting a black sheet of paper with a white marker,
          with each dot representing a photon. Initially, only a sparse number of photons intersect the teapot, leaving
          vast areas unmarked. Increasing the dots gradually fills in the gaps, making the teapot progressively more
          discernible.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/teapotracing.gif"
          alt="Teapot Tracing" />

        <p>However, deploying even thousands or multiples thereof of photons cannot guarantee complete coverage of the
          object's surface.</p>

        <p>This method's inherent flaw necessitates running the program until we subjectively deem enough photons have
          been applied to accurately depict the object. This process, requiring constant monitoring of the rendering
          process, is impractical in a production setting. <b>The primary cost in ray tracing lies in detecting
            ray-geometry intersections, not in generating photons, but in identifying all their intersections within the
            scene, which is exceedingly resource-intensive</b>.</p>

        <p>In summary, Forward ray-tracing or light tracing, which involves casting rays from the light source, can
          theoretically replicate natural light behavior on a computer. However, as discussed, this technique is neither
          efficient nor practical for actual use. Let's discuss an alternative apporach:</p>

        <h3>Backward Ray-Tracing</h3>

        <p>To make ray tracing more efficient, the method of backward ray-tracing is introduced.</p>

        <p>In contrast to the natural process where rays emanate from the light source to the receptor (like our eyes),
          backward ray-tracing reverses this flow by initiating rays from the receptor towards the objects.</p>

        <p>In backward ray-tracing, an eye ray is created at the eye; it passes through the viewplane and on into the
          world. The first object the eye ray hits is the object that will be visible from that point of the viewplane.
          After the ray tracer allows that light ray to bounce around, it figures out the exact coloring and shading of
          that point in the viewplane and displays it on the corresponding pixel on the computer monitor screen.
        </p>

        <p>Upon impacting an object, we evaluate the light it receives by dispatching another ray—termed a light or
          shadow ray—from the contact point towards the light source. If this "light ray" encounters obstruction by
          another object, it indicates that the initial point of contact is shadowed, receiving no light. Hence, these
          rays are more aptly called <b>shadow rays</b>. The inaugural ray shot from the eye (or camera) into the scene
          is
          referred to in computer graphics literature as a <b>primary ray</b>, <b>visibility ray</b>, or <b>camera
            ray</b>.</p>

        <p>Backward ray-tracing is also known as <b>eye tracing</b>.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/backward%20ray%20tracing.gif"
          alt="Backward Ray Tracing" />

        <p>In the image above, we trace a ray from the eye to a point on the sphere, then a ray from that point to the
          light source.</p>

        <p>The downfall of backward ray tracing is that it assumes only the light rays that come through the viewplane
          and on into the eye contribute to the final image of the scene. In certain cases, this assumption is flawed.
        </p>

        <p>For example, if a lens is held at a distance on top of a table, and is illuminated by a light source directly
          above, there will exist a focal point beneath the lens with a large concentration of light. If backward ray
          tracing tries to re-create this image, it will miscalculate because shooting light rays backward only confirms
          that rays traveled through the lens; backward rays have no way of recognizing that forward rays are bent when
          they go through the lens.</p>

        <p>Therefore, if only backward ray tracing is performed, there will only be an even patch of light beneath the
          lens, just as if the lens were a normal piece of glass and light is transmitted straight through it.</p>

        <p>The technique of initiating rays either from the light source or from the eye is encapsulated by the term
          <b>path tracing</b> in computer graphics.
        </p>

        <p>While ray-tracing is a synonymous term, path tracing emphasizes the methodological essence of generating
          computer-generated imagery by tracing the journey of light from its source to the camera, or vice versa. This
          approach facilitates the realistic simulation of optical phenomena such as caustics or indirect illumination,
          where light reflects off surfaces within the scene.</p>

        <h3>The Ray-Tracing Algorithm</h3>

        <p>The essence of the ray-tracing algorithm is to render an image pixel by pixel.</p>

        <p>For each pixel, it launches a primary ray into the scene, its direction determined by drawing a line from the
          eye through the pixel's center.</p>

        <p>This primary ray's journey is then tracked to ascertain if it intersects with any scene objects. In scenarios
          where multiple intersections occur, the algorithm selects the intersection nearest to the eye for further
          processing.</p>

        <p>A secondary ray, known as a shadow ray, is then projected from this nearest intersection point towards the
          light source as shown in the following figure:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/ray%20tracing%20algorithm%201.gif"
          alt="Ray Tracing" />

        <p>In the image above, a primary ray is cast through the pixel center to detect object intersections. Upon
          finding one, a shadow ray is dispatched to determine the illumination status of the point.</p>

        <p>An intersection point is deemed illuminated if the shadow ray reaches the light source unobstructed.
          Conversely, if it intersects another object en route, it signifies the casting of a shadow on the initial
          point.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/ray%20tracing%20algorithm%202.gif"
          alt="Ray Tracing 2" />

        <p>A shadow is cast on the larger sphere by the smaller one, as the shadow ray encounters the smaller sphere
          before reaching the light.</p>

        <p>Repeating this procedure across all pixels yields a two-dimensional depiction of our three-dimensional scene.
        </p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/ray%20tracing%20algorithm%203.gif"
          alt="Ray Tracing 3" />

        <p>Rendering a frame involves dispatching a primary ray for every pixel within the frame buffer.</p>

        <p>The elegance of ray-tracing lies in its simplicity and direct correlation with the physical world, allowing
          for the creation of a basic ray-tracer in as few as 200 lines of code. This simplicity contrasts sharply with
          more complex algorithms, like scanline rendering, making ray tracing comparatively effortless to implement.
        </p>

        <p>Regardless of which global lighting model chosen, <b>global lighting models are very expensive to
            compute</b>. According to Tony DeRose, rendering a single frame of the Pixar movie Finding Nemo took four
          hours. For The Incredibles, the next Pixar movie, rendering each frame took ten hours, which means that the
          algorithms have gotten more expensive even though the hardware is speeding up.</p>

        <p>Three.js manages to fall somewhere in between, because (as we have already seen) it can use the <b>Scene
            Graph</b>
          data structure to compute
          shadows, but it <b>doesn't do the full ray-tracing or radiosity computation</b>.</p>

        <h3>Adding Reflection and Refraction</h3>

        <p>Another key benefit of ray-tracing is its capacity to seamlessly simulate intricate optical effects such as
          <b>reflection</b> and <b>refraction</b>.These capabilities are crucial for accurately rendering materials like
          glass or mirrored surfaces.
        </p>

        <p>First, let's talk about <b>reflection</b>. Surfaces like metal and glass exhibit a high degree of
          reflectivity, causing them to mirror their surroundings. This effect is particularly pronounced in materials
          such as polished metals and glass, where the surface reflects light in a manner akin to a mirror.</p>

        <p>Reflections in Three.js can be accomplished quite easily using a <b>CubeCamera</b>.This is a unique camera
          that contains six perspective cameras that will display what each camera sees onto any object in which you
          place it. It fits well on all default three dimensional shapes provided by the core threejs library.</p>

        <p>CubeCamera is a special type of camera that captures the scene from six different angles, creating a cube
          map.This cube map can then be used to create realistic reflections on objects in the scene.</p>

        <pre><code class="javascript">// Create the scene
const scene = new THREE.Scene();
scene.background = new THREE.Color(0x87ceeb); // Set background color
const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);

// Create a WebGLRenderer
const renderer = new THREE.WebGLRenderer();
renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);

// Add lighting
const ambientLight = new THREE.AmbientLight(0x404040, 1); // Soft white light
scene.add(ambientLight);

const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
directionalLight.position.set(5, 5, 5);
scene.add(directionalLight);

const controls = new THREE.OrbitControls(camera, renderer.domElement);

// Create texture loader
const textureLoader = new THREE.TextureLoader();

// Create a CubeCamera for reflections
const cubeRenderTarget = new THREE.WebGLCubeRenderTarget(128, { //128 is the size of the cube map
  format: THREE.RGBFormat, //the color format of the cube map
  generateMipmaps: true, //generate mipmaps for better quality
  //mipmaps are precomputed textures that are used to improve the quality of the texture when viewed at a distance
  minFilter: THREE.LinearMipmapLinearFilter //filter used to sample the texture when viewed at a distance
});

const near = 0.1;
const far = 100;
const cubeCamera = new THREE.CubeCamera(near, far, cubeRenderTarget);
scene.add(cubeCamera);

// Load a texture for the plane
const planeTexture = textureLoader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/checkered_pattern.jpg');

// Plane material
const planeMaterial = new THREE.MeshStandardMaterial({
    map: planeTexture, 
    roughness: 1,
    metalness: 0, 
});

// Create the plane
const planeGeometry = new THREE.PlaneGeometry(20, 20);
const plane = new THREE.Mesh(planeGeometry, planeMaterial);
plane.rotation.x = -Math.PI / 2; 
plane.position.y = 0; // Position it on the ground
plane.receiveShadow = true; 
scene.add(plane);

// Sphere material
const sphereMaterial = new THREE.MeshStandardMaterial({
    metalness: 1, 
    roughness: 0, 
    envMap: cubeRenderTarget.texture 
});

// Create the reflective sphere
const sphereGeometry = new THREE.SphereGeometry(2, 32, 32);
const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphere.position.y = 4; // Position it above the plane
sphere.castShadow = true; 
scene.add(sphere);

// Position the camera
camera.position.set(0, 5, 10);
controls.update(); // Update controls

// Animation loop
function animate() {
  requestAnimationFrame(animate);

  // Update the CubeCamera position to match the sphere's position
  cubeCamera.position.copy(sphere.position);
  // Update the CubeCamera to capture the environment
  cubeCamera.update(renderer, scene);

  // Render the scene
  renderer.render(scene, camera);
}

animate();</code></pre>

        <p>Now we have a reflective sphere on a checkered plane. The CubeCamera captures the scene from the sphere's
          perspective and creates a cube map that is used to create realistic reflections on the sphere.</p>

        <p>This is a simple way to create reflections on surfaces so that we can have more realistic renderings of our
          objects such as metals.</p>

        <p>Three.js can also do <b>refraction</b>. Refraction occurs when light passes through a transparent or
          translucent object. A ray of light will be bent as it passes between the inside of the object and the outside.
          This is what happens when you look at a straw in a glass of water and it looks bent.
        </p>

        <p>The amount of bending depends on the so-called <b>"indices of refraction"</b> of the material outside and the
          material inside the object. More exactly, it depends on the ratio between the two indices. Even a perfectly
          transparent object will be visible because of the distortion induced by this bending (unless the ratio is 1,
          meaning that there is no bending of light at all).</p>

        <p>In Three.js, refraction is implemented using environment maps.</p>

        <p>As with reflection, a refracting object does not show its actual environment; it refracts the cubemap texture
          that is used as the environment map. For refraction, a special "mapping" must be used for the environment map
          texture.</p>

        <p>The mapping property of a texture tells how that texture will be mapped to a surface. For a cubemap texture
          being used for refraction, our <b>CubeRenderTargets texture.mapping should be set to
            THREE.CubeRefractionMapping</b>. (The default value of this
          property in a cubemap texture is appropriate for reflection rather than refraction.) </p>

        <p>And we need to add the <b>refractionRatio</b> property to our sphereMaterial.</p>

        <p>So let's start by adjusting out CubeCamera and CubeRenderTargets:</p>

        <pre><code class="javascript">// CubeCamera target
const cubeRenderTarget = new THREE.WebGLCubeRenderTarget(128, {
    format: THREE.RGBFormat,
    generateMipmaps: true,
    minFilter: THREE.LinearMipmapLinearFilter
});

// IMPORTANT TO SET REFRACTION
cubeRenderTarget.texture.mapping = THREE.CubeRefractionMapping;

// Set the near and far for the cubeCamera
const near = 0.1;
const far = 100;
const cubeCamera = new THREE.CubeCamera(near, far, cubeRenderTarget);
scene.add(cubeCamera);</code></pre>

        <p>And adjust our sphere properties:</p>

        <pre><code class="javascript">// Sphere
const sphereGeometry = new THREE.SphereGeometry(2, 32, 32);
const sphereMaterial = new THREE.MeshPhongMaterial({
    shininess: 100, //high shininess for a shiny surface
    color: "white", //white color
    specular: "white", //white specular color
    envMap: cubeRenderTarget.texture, //the environment map texture
    refractionRatio: 0.5, //the ratio of the indices of refraction
    transparent: true, //enable transparency
    side: THREE.BackSide, //the side of the object to render
    combine: THREE.MixOperation //mix the environment map with the color of the object
});
const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphere.position.y = 4; // Slightly above the plane
scene.add(sphere);</code></pre>

        <p>We can also now add a GUI to adjust our refraction ratio to see how that changes the effect on the sphere:
        </p>

        <pre><code class="javascript">// GUI to adjust refraction ratio
const gui = new dat.GUI();
const guiParams = { refractionRatio: sphereMaterial.refractionRatio };

const refractionFolder = gui.addFolder("Refraction");
refractionFolder.add(guiParams, "refractionRatio", 0, 1, 0.01).onChange((v) => {
    sphereMaterial.refractionRatio = v;
});
refractionFolder.open();</code></pre>

        <p>And lastly remember to update our cubeCamera in our animate() function:</p>

        <pre><code class="javascript">// Animate function
function animate() {
  requestAnimationFrame(animate);

  // Update cube camera position to match the position of the sphere
  cubeCamera.position.copy(sphere.position);

  // Update cube camera to capture the environment
  cubeCamera.update(renderer, scene);

  // Render scene
  renderer.render(scene, camera);
}</code></pre>

        <p>Now we should be able to see the plane refracted through the "glass sphere".</p>

        <p>How do the two examples differ? Reflection vs refraction?</p>

        <p>Reflection is the bouncing of light off a surface, while refraction is the bending of light as it passes
          through a medium. In the case of the <b>reflective sphere</b>, the <b>light is bouncing off the surface and
            creating a
            mirror-like effect</b>. In the case of the <b>refractive sphere</b>, the <b>light is bending as it passes
            through the glass and
            creating a distorted effect</b>.</p>

        <p>The important thing to remember is that in the <b>law of (specular) reflection</b>, <b>angle of incidence is
            equal to
            the angle of reflection</b>.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/law%20of%20reflection.gif"
          alt="Law of Reflection" />

        <p>This is similar to what happens to a tennis ball when it hits the surface of the floor. It bounces back in a
          direction that is symmetrical to the incident direction about the surface normal at the point of impact.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/tennis%20ball%20example%201.png"
          alt="Law of Reflection Example 1" />

        <p>In the image above, the angle of incidence is equal to the angle of reflection.</p>

        <p>Computing the reflection direction can be done using simple geometry.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/tennis%20ball%20example%202.png"
          alt="Law of Reflection Example 2" />

        <p>As you can see in image above, the vectors <b><i>I</i></b> and <b><i>R</i></b> can be expressed in terms of
          the vectors <b><i>A</i></b> and <b><i>B</i></b>:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
            <mtr>
              <mtd>
                <mi>I</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>A</mi>
                <mo>+</mo>
                <mi>B</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>R</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>A</mi>
                <mo>&#x2212;</mo>
                <mi>B</mi>
              </mtd>
            </mtr>
          </mtable>
        </math>

        <p>The vector <b><i>B</i></b> can easily be computed. It is the projection of the vector <b><i>I</i></b> or
          <b><i>R</i></b> onto the vector <b><i>N</i></b>. We can compute
          this vector using the following equation:
        </p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mi>B</mi>
          <mo>=</mo>
          <mi>cos</mi>
          <mo data-mjx-texclass="NONE">&#x2061;</mo>
          <mo stretchy="false">(</mo>
          <mi>&#x3B8;</mi>
          <mo stretchy="false">)</mo>
          <mo>&#x2217;</mo>
          <mi>N</mi>
        </math>

        <p>As the term cos(θ) is equal to the dot product between the two vectors <b><i>N</i></b> and <b><i>I</i></b>.
        </p>

        <p>We can now replace <b><i>B</i></b> in both equations:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
            <mtr>
              <mtd>
                <mi>I</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>A</mi>
                <mo>+</mo>
                <mi>cos</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <mo stretchy="false">(</mo>
                <mi>&#x3B8;</mi>
                <mo stretchy="false">)</mo>
                <mo>&#x2217;</mo>
                <mi>N</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>R</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>A</mi>
                <mo>&#x2212;</mo>
                <mi>cos</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <mo stretchy="false">(</mo>
                <mi>&#x3B8;</mi>
                <mo stretchy="false">)</mo>
                <mo>&#x2217;</mo>
                <mi>N</mi>
              </mtd>
            </mtr>
          </mtable>
        </math>

        <p>And we can re-write the first equation as follows:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mi>A</mi>
          <mo>=</mo>
          <mi>I</mi>
          <mo>&#x2212;</mo>
          <mi>cos</mi>
          <mo data-mjx-texclass="NONE">&#x2061;</mo>
          <mo stretchy="false">(</mo>
          <mi>&#x3B8;</mi>
          <mo stretchy="false">)</mo>
          <mo>&#x2217;</mo>
          <mi>N</mi>
        </math>

        <p>We can now compute the vector <b><i>R</i></b> as: </p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
            <mtr>
              <mtd>
                <mi>R</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>I</mi>
                <mo>&#x2212;</mo>
                <mi>cos</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <mo stretchy="false">(</mo>
                <mi>&#x3B8;</mi>
                <mo stretchy="false">)</mo>
                <mo>&#x2217;</mo>
                <mi>N</mi>
                <mo>&#x2212;</mo>
                <mi>cos</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <mo stretchy="false">(</mo>
                <mi>&#x3B8;</mi>
                <mo stretchy="false">)</mo>
                <mo>&#x2217;</mo>
                <mi>N</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>R</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>I</mi>
                <mo>&#x2212;</mo>
                <mn>2</mn>
                <mi>cos</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <mo stretchy="false">(</mo>
                <mi>&#x3B8;</mi>
                <mo stretchy="false">)</mo>
                <mi>N</mi>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <mi>R</mi>
              </mtd>
              <mtd>
                <mo>=</mo>
              </mtd>
              <mtd>
                <mi>I</mi>
                <mo>&#x2212;</mo>
                <mn>2</mn>
                <mo stretchy="false">(</mo>
                <mi>N</mi>
                <mo>&#x22C5;</mo>
                <mi>I</mi>
                <mo stretchy="false">)</mo>
                <mi>N</mi>
              </mtd>
            </mtr>
          </mtable>
        </math>

        <p>A reflection of a light ray can <b>only be seen if the reflected ray direction is traveling in the same
            direction as the view direction</b>.</p>

        <p>The fact that the reflected image of the objects in the scene from which these light rays are emitted changes
          with the view direction is the reason why we say that reflection is <b>view-dependent</b>.</p>

        <p>If you look at the reflection of a static object in the mirror and change direction, you will see that the
          image of that object changes. This is something that we find natural when we look at an object from a
          different angle, but that we find maybe less natural when we change our position with respect to a mirror
          reflecting that same object, though the reason why this is happening is essentially the same. We look at a
          different part of the object.</p>

        <p>By opposition, we say that <b>diffuse reflections are view independent</b> because they don't vary with the
          angle of view.</p>

        <p>Note that the reflection process is <b>potentially recursive</b>. It is entirely possible to have a situation
          in which a ray intersects a reflective surface from which we cast another reflection ray, that will intersect
          in turn another reflective surface, etc. In other words, the process will keep casting reflection rays unless
          the ray intersects an object which is not a mirror or if it doesn't intersect anything at all (in which case
          we return the background color).</p>

        <p>If a ray kept reflecting other reflecting surfaces without ever reflecting anything else, we would then enter
          some kind of infinite recursive process. To prevent this from happening, we generally put a cap or limit on
          the number of recursions.</p>

        <p>The number of times a reflection ray is reflected off of surfaces is called the <b>ray depth</b>. When we
          cast a reflection ray from the primary ray, we say that the ray has a depth of 1. After two reflections, the
          ray has a depth of 2, and so on.</p>

        <p>For most scenes using a depth much greater than 4 or 5 generally doesn't make much of a visual difference.
          It's only when very complex transparent surfaces are rendered (such as water splashes) that using a depth much
          greater than 5 is necessary for producing images that are similar to the real thing.</p>

        <p>Keep in mind that another reason for putting a cap on the recursion depth is also because ray-tracing is
          expensive.
          <b>The higher the recursion the longer it will take to render a frame</b>.
        </p>

        <p>In the case of refraction, the angle of incidence is <b>not</b> equal to the angle of refraction. The amount
          of
          bending depends on the <b>index of refraction (ior)</b>.</p>

        <p>The index of refraction for glass and water is around 1.5 and 1.3 respectively. For a fixed angle of
          incidence, the amount of bending depends on the index of refraction (as shown in the image below):</p>

        <img class="center" style="width: 90%; height: 90%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/refraction.png"
          alt="Refraction" />

        <p>Refraction is described by <b>Snell's law</b>, which states that for a given pair of media, <b>the ratio of
            the sines of the angle of incidence
            and angle of refraction
            is equivalent to the opposite ratio of the indices of refraction</b>.</p>

        <p>Snell's Law:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mstyle displaystyle="true" scriptlevel="0">
            <mfrac>
              <mrow>
                <mi>sin</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <msub>
                  <mi>&#x3B8;</mi>
                  <mn>1</mn>
                </msub>
              </mrow>
              <mrow>
                <mi>sin</mi>
                <mo data-mjx-texclass="NONE">&#x2061;</mo>
                <msub>
                  <mi>&#x3B8;</mi>
                  <mn>2</mn>
                </msub>
              </mrow>
            </mfrac>
          </mstyle>
          <mo>=</mo>
          <mstyle displaystyle="true" scriptlevel="0">
            <mfrac>
              <msub>
                <mi>&#x3B7;</mi>
                <mn>2</mn>
              </msub>
              <msub>
                <mi>&#x3B7;</mi>
                <mn>1</mn>
              </msub>
            </mfrac>
          </mstyle>
          <mo>.</mo>
        </math>

        <p>Let's look at them side by side: <a href="https://codepen.io/amaraauguste/pen/wBvRRed">here</a></p>

        <p>Notice how both spheres either reflect or refract the environment around them.</p>

        <p>The problem with this image though, is that it is not completely realistic. Glass spheres as well as pretty
          much every other transparent surface (water, diamonds, crystals, etc.) transmit as well as reflect light.
          <b>They
            are both refractive and reflective (not either or)</b>. The problem is how do we know how much light they
          transmit vs. the amount
          of light they reflect?
        </p>

        <p>This ratio is given by the <b>Fresnel equations</b>.</p>

        <p>Transparent objects such as glass or water are both refractive and reflective. How much light they reflect vs
          the amount they transmit depends on the angle of incidence. <b>The amount of transmitted light increases when
            the angle of incidence decreases</b>. </p>

        <p>And since by the principle of the conservation of energy, <b>the amount of reflected light plus the amount of
            refracted light is necessarily equal to the total amount of incident light</b>, you can deduce that <b>the
            amount of reflected light increases when the angle of incidence increases</b>, up to 100% as the angle gets
          closer to 90 degrees.</p>

        <p>Technically, the edges of a glass ball are 100% reflective. In its center though, the sphere only reflects
          about 6% of the incident light.</p>

        <p>The amount of reflected vs. refracted light can be computed using what we call the <b>Fresnel equations</b>.
          Explaining the origin of these equations and how they can be derived goes far beyond the level of explanation
          we are willing to give in this lesson.</p>

        <p>Light is composed of two perpendicular waves which we call <b>parallel</b> and <b>perpendicular polarised
            light</b>. Don't worry too much if you don't know about this detail.</p>

        <p>Suffice it to know that we need to compute the ratio of reflected light for these two waves using two
          different equations (one for each type of wave) and average the results to find the solution.</p>

        <p>The two Fresnel equations are:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <mtable columnalign="left" columnspacing="1em" rowspacing="4pt">
            <mtr>
              <mtd>
                <msub>
                  <mi>F</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>R</mi>
                    <mo>&#x2225;</mo>
                  </mrow>
                </msub>
                <mo>=</mo>
                <msup>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mstyle displaystyle="true" scriptlevel="0">
                      <mfrac>
                        <mrow>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>2</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>&#x2212;</mo>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>1</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>2</mn>
                          </msub>
                        </mrow>
                        <mrow>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>2</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>1</mn>
                          </msub>
                          <mo>+</mo>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>1</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>2</mn>
                          </msub>
                        </mrow>
                      </mfrac>
                    </mstyle>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                  </mrow>
                  <mn>2</mn>
                </msup>
                <mo>,</mo>
              </mtd>
            </mtr>
            <mtr>
              <mtd>
                <msub>
                  <mi>F</mi>
                  <mrow data-mjx-texclass="ORD">
                    <mi>R</mi>
                    <mo>&#x22A5;</mo>
                  </mrow>
                </msub>
                <mo>=</mo>
                <msup>
                  <mrow data-mjx-texclass="INNER">
                    <mo data-mjx-texclass="OPEN">(</mo>
                    <mstyle displaystyle="true" scriptlevel="0">
                      <mfrac>
                        <mrow>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>1</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>2</mn>
                          </msub>
                          <mo>&#x2212;</mo>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>2</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>1</mn>
                          </msub>
                        </mrow>
                        <mrow>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>1</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>2</mn>
                          </msub>
                          <mo>+</mo>
                          <msub>
                            <mi>&#x3B7;</mi>
                            <mn>2</mn>
                          </msub>
                          <mi>cos</mi>
                          <mo data-mjx-texclass="NONE">&#x2061;</mo>
                          <msub>
                            <mi>&#x3B8;</mi>
                            <mn>1</mn>
                          </msub>
                        </mrow>
                      </mfrac>
                    </mstyle>
                    <mo data-mjx-texclass="CLOSE">)</mo>
                  </mrow>
                  <mn>2</mn>
                </msup>
              </mtd>
            </mtr>
          </mtable>
        </math>

        <p>By taking the average of the two we get the actual ratio of reflected light:</p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <msub>
            <mi>F</mi>
            <mi>R</mi>
          </msub>
          <mo>=</mo>
          <mstyle displaystyle="true" scriptlevel="0">
            <mfrac>
              <mn>1</mn>
              <mn>2</mn>
            </mfrac>
          </mstyle>
          <mo stretchy="false">(</mo>
          <msub>
            <mi>F</mi>
            <mrow data-mjx-texclass="ORD">
              <mi>R</mi>
              <mo>&#x2225;</mo>
            </mrow>
          </msub>
          <mo>+</mo>
          <msub>
            <mi>F</mi>
            <mrow data-mjx-texclass="ORD">
              <mi>R</mi>
              <mo>&#x22A5;</mo>
            </mrow>
          </msub>
          <mo stretchy="false">)</mo>
        </math>

        <p>The terms <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mi>&#x3B7;</mi>
              <mn>1</mn>
            </msub>
          </math>
          , <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mi>&#x3B7;</mi>
              <mn>2</mn>
            </msub>
          </math>
          are the refraction indices of the two mediums. The terms <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>cos</mi>
            <mo data-mjx-texclass="NONE">&#x2061;</mo>
            <msub>
              <mi>&#x3B8;</mi>
              <mn>1</mn>
            </msub>
          </math>
          and <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>cos</mi>
            <mo data-mjx-texclass="NONE">&#x2061;</mo>
            <msub>
              <mi>&#x3B8;</mi>
              <mn>2</mn>
            </msub>
          </math>
          are the angles of incidence and refraction respectively. As mentioned before, due to the conservation of
          energy, the ratio of refracted light can simply be computed as:

        </p>

        <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
          <msub>
            <mi>F</mi>
            <mi>T</mi>
          </msub>
          <mo>=</mo>
          <mn>1</mn>
          <mo>&#x2212;</mo>
          <msub>
            <mi>F</mi>
            <mi>R</mi>
          </msub>
          <mo>.</mo>
        </math>
        <p>Keep in mind that <b>if the light goes from one medium to another medium with a lower refraction index, it
            may be subject to the phenomenon of total internal reflection</b>. This is happening in the case of
          materials such as glass or water, so we need to take this into account.</p>


        <p>We do so by computing as in the case of reflection, the sine of the angle of refraction or <math
            xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mi>&#x3B8;</mi>
              <mn>2</mn>
            </msub>
          </math>
          . If <math xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>sin</mi>
            <mo data-mjx-texclass="NONE">&#x2061;</mo>
            <msub>
              <mi>&#x3B8;</mi>
              <mn>2</mn>
            </msub>
          </math>
          is greater than 1, then we have a case of <b>total reflection</b>. In this particular case, there is no need
          to compute the Fresnels formulas, we can just set
          <math xmlns="http://www.w3.org/1998/Math/MathML">
            <msub>
              <mi>F</mi>
              <mi>R</mi>
            </msub>
          </math> to 1.
        </p>

        <p>As usual, you will need to swap the refraction indices if you find out that the incident ray is inside the
          object with the greatest refraction index. This can be done again by testing the sign of the cosine of the
          angle between the surface normal and the incident ray direction (the sign of <math
            xmlns="http://www.w3.org/1998/Math/MathML">
            <mi>cos</mi>
            <mo data-mjx-texclass="NONE">&#x2061;</mo>
            <msub>
              <mi>&#x3B8;</mi>
              <mn>1</mn>
            </msub>
          </math>
          ).</p>

        <p>The fresnel effect can easily be observed in nature.</p>

        <p>If you look at the picture of the lake with some mountains in the background and pebbles in the foreground,
          you can see that <b>the reflection seems to increase with distance</b>.</p>

        <img class="center" style="width: 90%; height: 90%"
          src="https://www.scratchapixel.com/images/shading-intro/shad-fresnel1.png?" alt="Fresnel 1" />

        <p>Note also that while reflections are strong in the distance, we see more clearly through the water in the
          foreground than we do in the far distance. This is due to fresnel.</p>

        <p>The angle of incidence increases with the distance as shown in the following image:</p>

        <img class="center" style="width: 90%; height: 90%"
          src="https://www.scratchapixel.com/images/shading-intro/shad-fresnel3.png?" alt="Fresnel 2" />

        <p>and we know that the ratio of reflection vs. transmission increases with the angle of incidence. Thus
          naturally as we look in the distance, the water's surface reflects more light.</p>

        <p>Though if we look almost directly down on the water surface, a few meters from where we stand, the angle
          incidence is low and most of the light is transmitted. Thus we see through the water more clearly than when we
          look in the distance.</p>

        <p>You can easily observe this effect on a large variety of objects: the facade of a building made out of glass,
          glass balls that are more reflective on the edges, etc.</p>

        <p>But back to our example, we can see that the <b>CubeCamera</b> is a very useful tool for creating realistic
          reflections and refractions in our scenes.</p>

        <p><b>CubeCamera can take a six-fold picture of a scene from a given point of view and make a cubemap texture
            from those images</b>.
          To use the camera, we have to place it at the location of an object—and make the object invisible so it
          doesn't show up in the pictures.
          Snap the picture, and apply it as an environment map on the object.</p>
        <p>For animated scenes, you have to do this in every frame, and we need to do it for every reflective/refractive
          object in the scene.
          Obviously, this can get very computationally expensive! And the result still isn't perfect. </p>

        <p>There is also the case where we do not want to use the exact environment around our object but instead use a
          different environment.
          This is where we can use a <b>CubeTextureLoader</b> to load a cubemap texture (opposed to capturing our
          current environment with a CubeCamera) from a set of images.</p>

        <h3>Cubemap Textures and Skyboxes</h3>

        <p>We have created and viewed simple scenes, shown on a solid-colored background, but it would be nice to put
          our scenes in an "environment" such as the interior of a building, a nature scene, or a public square.</p>

        <p>It's not practical to build representations of such complex environments out of geometric primitives, but we
          can get a reasonably good effect using textures.</p>

        <p>The technique that is used in Three.js is called a <b>skybox</b>.</p>
        <p>A skybox is a large cube — effectively, infinitely large — where a different texture is applied to each face
          of the cube.
          The textures are images of some environment. For a viewer inside the cube, the six texture images on the cube
          fit together to provide a
          complete view of the environment in every direction.</p>

        <p>The six texture images together make up what is called a <b>cubemap texture</b>. The images must match up
          along the edges of the cube to form a seamless view of the environment.</p>

        <p>A cube map of an actual physical environment can be made by <b>taking six pictures of the environment in six
            directions: left, right, up, down, forward, and back</b>. (More realistically, it is made by taking enough
          photographs to cover all directions, with overlaps, and then using software to "stitch" the images together
          into a complete cube map.) </p>

        <div style="display: flex; justify-content: center;">
          <img class="center" style="width: 50%; height: auto; margin-right: 10px;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/skybox%20layout.png"
            alt="Skybox layout 1" />

          <img class="center" style="width: 50%; height: auto;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/skybox%20layout%202.png"
            alt="Skybox layout 2" />
        </div>

        <p>The six directions are referred to by their relation to the coordinate axes as: <b>positive x, negative x,
            positive y, negative y, positive z, and negative z, and the images must be listed in that order when you
            specify the cube map</b>.</p>

        <p>Here is an example - The first picture shows the six images of a cube map laid out next to each other.</p>

        <p>The positive y image is at the top, the negative y image is at the bottom. In between are the negative x,
          positive
          z, positive x, and negative z images laid out in a row.</p>
        <p>The second picture shows the images used to texture a cube, viewed here from the outside. You can see how the
          images match up along the edges of the cube:</p>

        <img class="center" style="width: 100%; height: 100%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/skybox%20example.png"
          alt="Skybox example" />

        <p>For a skybox, conceptually, a very large cube would be used. The camera, lights, and any objects that are to
          be part of the scene would be inside the cube. It is possible to construct a skybox by hand in just this way.
        </p>

        <p>Let's create a cubemap/skybox contained within a cube:</p>

        <pre><code class="javascript">//creating a cube
const geometry = new THREE.BoxGeometry(8,8,8);
var materials = [
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_PosX.bmp'),
        side : THREE.BackSide,
    }),
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_NegX.bmp'),
        side : THREE.BackSide,
    }),
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_PosY.bmp'),
        side : THREE.BackSide,
    }),
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_NegY.bmp'),
        side : THREE.BackSide,
    }),
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_PosZ.bmp'),
        side : THREE.BackSide,
    }),
    new THREE.MeshBasicMaterial({
        map : new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Daylight%20Box_Pieces/Daylight%20Box_NegZ.bmp'),
        side : THREE.BackSide,
    }),
];
const cube = new THREE.Mesh(geometry,materials);
scene.add(cube);

camera.position.set(0, 0, 15);</code></pre>

        <p>Based on this we should see a cube with the six images of the cube map applied inside it.</p>

        <p>However, Three.js makes it very easy to use a skybox as the background for a scene. It has the class
          THREE.CubeTexture to represent cube maps, and you can enclose your scene in a skybox simply by <b>assigning a
            CubeTexture as the value of the property scene.background</b>. (The value of that property could also be a
          normal
          Texture or a Color.)</p>

        <p>A CubeTexture can be created by a CubeTextureLoader, which can load the six images that make up the cube map.
          The loader has a method named load() that works in the same way as the load() method of a TextureLoader,
          except that the first parameter to the method is an array of six strings giving the URLs of the six images for
          the cube map.</p>

        <p>Now, let's load a texture map for a meadow (this particular cube map, and others like it, are by
          Emil Persson, who has made a large number of cube maps available for download at
          <a href="http://www.humus.name/index.php?page=Textures">http://www.humus.name/index.php?page=Textures</a>
          under a creative commons license or from sources like <a href="http://opengameart.org">opengameart.org</a>
          which is
          where the previous textures came from).
        </p>

        <pre><code class="javascript">const textureURLs = [  // URLs of the six faces of the cube map (in the order: +x, -x, +y, -y, +z, -z)
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/posx.jpg",   
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/negx.jpg",   
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/posy.jpg",   
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/negy.jpg",  
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/posz.jpg",   
    "https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/Meadow/negz.jpg"
];
const cubeLoader = new THREE.CubeTextureLoader();
const cubeTexture = loader.load(textureURLs);
scene.background = cubeTexture;</code></pre>

        <p>Now we should see our environment as our scene is now surrounded by the skybox texture.</p>

        <p>Let's put our reflective sphere into this environment and see how it works:</p>

        <pre><code class="javascript">// Create a cube render target for reflections
const cubeRenderTarget = new THREE.WebGLCubeRenderTarget(128, {
    format: THREE.RGBFormat,
    generateMipmaps: true,
    minFilter: THREE.LinearMipmapLinearFilter
});

const near = 0.1;
const far = 100;
const cubeCamera = new THREE.CubeCamera(near, far, cubeRenderTarget);
scene.add(cubeCamera);

// Sphere for reflection
const sphereReflectionMaterial = new THREE.MeshStandardMaterial({
    metalness: 1,
    roughness: 0,
    envMap: cubeRenderTarget.texture
});
const sphereReflection = new THREE.Mesh(
    new THREE.SphereGeometry(50, 32, 32),
    sphereReflectionMaterial
);
sphereReflection.position.y = 4;
sphereReflection.position.x = -53; // Position it on the left
scene.add(sphereReflection);</code></pre>

        <p>And remember to add the following to our animate() function:</p>

        <pre><code class="javascript">// Update cube camera for reflection
cubeCamera.position.copy(sphereReflection.position);
cubeCamera.update(renderer, scene);</code></pre>

        <p>What effect does the skybox have on the reflective sphere?</p>

        <p>The skybox provides a background environment for the reflective sphere, allowing it to reflect the
          surrounding
          scenery. The reflections on the sphere will now show the meadow environment instead of a solid color.</p>

        <p>Let's add our refractive sphere too:</p>

        <pre><code class="javascript">// Create a cube render target for refractions
const cubeRenderTarget2 = new THREE.WebGLCubeRenderTarget(128, {
  format: THREE.RGBFormat,
  generateMipmaps: true,
  minFilter: THREE.LinearMipmapLinearFilter
});

const cubeCamera2 = new THREE.CubeCamera(near, far, cubeRenderTarget2);
scene.add(cubeCamera2);

// Important to set for refraction
cubeRenderTarget2.texture.mapping = THREE.CubeRefractionMapping;

// Sphere for refraction
const sphereRefractionMaterial = new THREE.MeshPhongMaterial({
    shininess: 100,
    color: 0xffffff,
    specular: 0xffffff,
    envMap: cubeRenderTarget2.texture,
    refractionRatio: 0.5,
    transparent: true,
    side: THREE.BackSide // Render from inside
});
const sphereRefraction = new THREE.Mesh(
    new THREE.SphereGeometry(50, 32, 32),
    sphereRefractionMaterial
);
sphereRefraction.position.y = 4;
sphereRefraction.position.x = 53; // Position it on the right
scene.add(sphereRefraction);</code></pre>

        <p>And once again update in our animate() function:</p>

        <pre><code class="javascript">// Update cube camera for refraction
cubeCamera2.position.copy(sphereRefraction.position);
cubeCamera2.update(renderer, scene);</code></pre>

        <p>Now we should have a sphere for reflecting and a sphere for refracting the cubemap environment around them.
        </p>

        <h3>Additional Ray-Tracing Fundamentals</h3>

        <h3>Ray Casting</h3>

        <p><b>Ray casting</b> (or Object Picking) is the process in a ray-tracing algorithm that <b>shoots one or more
            rays from the camera
            (eye
            position) through each pixel in an image plane, and then tests to see if the rays intersect any primitives
            (triangles) in the scene</b>. If a ray passing through a pixel and out into the 3D scene hits a primitive,
          then
          the distance along the ray from the origin (camera or eye point) to the primitive is determined, and the color
          data from the primitive contributes to the final color of the pixel. The ray may also bounce and hit other
          objects and pick up color and lighting information from them.</p>

        <p>Ray casting is critical for rendering and visualization because it creates realistic lighting. This technique
          simulates how light interacts with objects in the real world in a digital environment, which allows for
          astonishingly realistic characters, objects, and scenes.</p>

        <p>Ray casting is a fundamental technique for modern 3D design, but the technology is decades old. In the 1960s,
          computer scientists explored methods to simulate how light interacts with surfaces for realistic image
          generation. In the 1980s, Turner Whitted wrote a paper on recursive ray tracing that popularized the
          technique. It introduced <b>the concept of handling reflections and refractions by recursively casting
            rays</b> — a
          true game-changer for ray casting.</p>

        <p>John Carmack popularized ray casting in the early 1990s with the game <a
            href="https://youtu.be/eOCQfxRQ2pY?si=XhbADL-3ZOD2iMRI">"Wolfenstein 3D"</a>, which used ray
          casting to create a 3D perspective in a 2D world. In the 2000s, <b>ray casting techniques blended with
            rasterization to support real-time rendering in digital simulations</b>. Today, companies are developing
          hardware-accelerated ray tracing, which pushes the boundaries of what's possible.</p>

        <p>Ray casting has many important applications, such as:</p>
        <ul>
          <li><b>Medical imaging</b>: Ray casting renders volumetric data from CT and MRI scans. This gives medical
            professionals detailed 3D images that improve diagnostic accuracy.</li>
          <li><b>Architectural visualizations</b>: Architects use ray casting to predict how light will interact with
            their spaces. It's especially useful for determining how sunlight will illuminate rooms at different times
            of the day or year.</li>
          <li><b>Virtual reality (VR)</b>: VR uses ray casting for interaction detection, including gaze tracking and
            object selection. This creates a more immersive user experience.</li>
          <li><b>Games</b>: Ray casting is important for video game design because it supports realistic, interactive
            environments. Video games use ray casting for various purposes, including rendering and collision detection.
            Games like Doom Eternal and Cyberpunk 2077 are just a few examples of ray casting at work.</li>
        </ul>

        <h3>Ray Casting in Three.js</h3>

        <p>We can use <b>ray casting as a way to determine where the user's mouse is pointing in a 3D
            scene</b>.
        </p>

        <p>Essentially, a pointer is cast from your mouse or finger (on mobile) through the <b>frustum</b> (the shape of
          the region that can be seen and rendered by a perspective camera) of the scene to the
          object that it intersects.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/raycasting%20in%20three%20js.png"
          alt="Raycasting" />

        <p>In Three.js, ray casting can be achieved using Vector2D and the Raycaster.</p>

        <p>The Raycaster is given the coordinates of the mouse on the screen, and it creates a ray normal to the mouse's
          position.This allows the Raycaster to determine the objects that the ray is intersecting using the
          intersectObjects method. </p>

        <p>A Raycaster takes an initial point and a direction, given as a vector. The point and vector determine a ray,
          that is, a half-infinite line that extends from a starting point, in some direction, to infinity. </p>

        <p>The Raycaster can find all the intersections of the ray with a given set of objects in a three.js scene,
          sorted by order of distance from the rays's starting point.</p>

        <p>The syntax for using the Raycaster is as follows:</p>

        <pre><code class="javascript">const raycaster = new THREE.Raycaster();</code></pre>

        <p>To tell it which ray to use, you can call:</p>

        <pre><code class="javascript">raycaster.set(startingPoint, direction);</code></pre>

        <p>or more specifically set from camera: </p>

        <pre><code class="javascript">raycaster.setFromCamera(mouse, camera);</code></pre>

        <p>Here, along with the Raycaster, we also need to initialize a Vector2 that will hold the coordinates of the
          mouse.</p>

        <p>A Vector2 is a two-dimensional vector that only contains the x and y coordinates. We use this to track the
          movement of the cursor on the screen.</p>

        <p>The syntax for creating a Vector2 is as follows:</p>

        <pre><code class="javascript">const mouse = new THREE.Vector2();</code></pre>

        <p>To set the coordinates of the mouse, we can use the following syntax:</p>

        <pre><code class="javascript">mouse.set(x, y);</code></pre>

        <p>Where x and y are the coordinates of the mouse on the screen.</p>

        <p>To get the coordinates of the mouse, we can use the following syntax:</p>

        <pre><code class="javascript">mouse.x = (event.clientX / width) * 2 - 1;</code></pre>
        <p>And</p>
        <pre><code class="javascript">mouse.y = -(event.clientY / height) * 2 + 1;</code></pre>

        <p>Where event is the event object that is passed to the event handler.</p>

        <p>We multiply by 2 and subtract 1 to get the coordinates in the range of <b>-1 to 1</b>.</p>

        <p>We also need to set the camera to the Raycaster. The camera is the point of view from which we are looking at
          the scene.</p>

        <p>To check if the ray intersects with any objects in the scene, we can use the following syntax:</p>

        <pre><code class="javascript">const intersects = raycaster.intersectObjects(objects);</code></pre>
        <p>Where objects is an array of objects in the scene.</p>

        <p>Let's try an example:</p>

        <pre><code class="javascript">const width = 500;
const height = 400;

const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);

const renderer = new THREE.WebGLRenderer();
renderer.setSize(width, height);
document.body.appendChild(renderer.domElement);

// Position the camera
camera.position.set(0, 0, 10); 

// Lights
const hemisphere = new THREE.HemisphereLight("white", "gray", 1);
scene.add(hemisphere);
const directional = new THREE.DirectionalLight("white", 0.5);
directional.position.set(5, 5, 5);
scene.add(directional);

// Objects array for raycasting
const objects = [];

// Geometry
const geometry = new THREE.SphereGeometry(3, 64, 64);
// Material
const material = new THREE.MeshStandardMaterial({
  color: "red",
  metalness: 0.5,
  roughness: 1
});
// Mesh
const circle = new THREE.Mesh(geometry, material);
scene.add(circle);
objects.push(circle); // Store the circle in the objects array

// Mouse event for color change
document.addEventListener("mousedown", onMouseDown);
function onMouseDown(event) {
  event.preventDefault();
  const mouse = new THREE.Vector2();
  mouse.set((event.clientX / width) * 2 - 1, -(event.clientY / height) * 2 + 1);
  const raycaster = new THREE.Raycaster();
  raycaster.setFromCamera(mouse, camera);
  const intersects = raycaster.intersectObjects(objects);
  if (intersects.length > 0) { // Check if there is an intersection
    // Change the color of the intersected object
    intersects[0].object.material.color.setHex(Math.random() * 0xffffff);
  }
}

// Animation loop
function animate() {
  requestAnimationFrame(animate);

  // Render the scene from the perspective of the camera
  renderer.render(scene, camera);
}

animate();</code></pre>

        <p>In this example, we create a scene with a red sphere. When the user clicks on the sphere, it changes color to
          a random color.</p>

        <p>We use the Raycaster to check if the mouse intersects with the sphere. If it does, we change the color of the
          sphere.</p>

        <p>We can also use the Raycaster to check if the mouse intersects with other objects in the scene.</p>

        <p>For example, we can create a plane and check if the mouse intersects with it:</p>

        <pre><code class="javascript">const planeGeometry = new THREE.PlaneGeometry(20, 20);
const planeMaterial = new THREE.MeshStandardMaterial({ color: "blue", side: THREE.DoubleSide });
const plane = new THREE.Mesh(planeGeometry, planeMaterial);
plane.rotation.x = -Math.PI / 2; // Rotate the plane to be horizontal
plane.position.y = -2.75; // Position the plane below the sphere
scene.add(plane);
objects.push(plane); // Store the plane in the objects array</code></pre>

        <p>Now, when the user clicks on the plane, it will also change color to a random color.</p>

        <p>Let's use the Raycaster to create a scene with more functionality.</p>

        <p>The scene shows a square, green base, and some tapered, yellow
          cylinders standing on the base. Cylinders can intersect; no attempt
          is made to treat them like real solid objects.</p>


        <p>The user can interact with the scene using the mouse.</p>

        <p>Three radio buttons let the user select the action that they want to perform with the mouse.</p>

        <ul>
          <li>When "Add" is selected, a mouse click at a point on the green base will add a new cylinder at that point.
            Clicking a cylinder will have no effect.</li>
          <li>When "Drag" is selected, the mouse can be used to drag the yellow cylinders. The cylinder is constrained
            to stay on the base.
            An interesting point is that as the cylinder is dragged, its new position is determined using the Raycaster.
          </li>
          <li>When "Delete" is selected, clicking on one of the yellow cylinders will remove it from the scene.
            Clicking the ground will have no effect.</li>
        </ul>

        <pre><code class="javascript">const width = 500;
const height = 400;

// Create the scene and camera
const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
camera.position.set(-30, 40, 10);
camera.lookAt(0, 0, 0);

// Create the renderer
const renderer = new THREE.WebGLRenderer();
renderer.setSize(width, height);
document.body.appendChild(renderer.domElement);

// Add lights
const directionalLight = new THREE.DirectionalLight(0xffffff, 1);
directionalLight.position.set(5, 10, 7.5);
scene.add(directionalLight);

const ambientLight = new THREE.AmbientLight(0x404040); // Soft white light
scene.add(ambientLight);

// Create ground
const ground = new THREE.Mesh(
  new THREE.BoxGeometry(40, 1, 40),
  new THREE.MeshLambertMaterial({ color: "green" })
);
ground.position.y = -0.5;
scene.add(ground);

// Create variables for interactivity
const raycaster = new THREE.Raycaster();
const mouse = new THREE.Vector2();
let mouseAction = 1; // Default to ADD = 1
let dragItem = null; // To track the object being dragged

// Functions for interaction
function addCylinder(x, z) {
  const cylinderGeometry = new THREE.CylinderGeometry(1, 2, 6, 16, 1);
  const cylinderMaterial = new THREE.MeshLambertMaterial({ color: "yellow" });
  const cylinder = new THREE.Mesh(cylinderGeometry, cylinderMaterial);
  cylinder.position.set(x, 3, z);
  scene.add(cylinder);
}

function handleMouseDown(event) {
  mouse.x = (event.clientX / width) * 2 - 1;
  mouse.y = -(event.clientY / height) * 2 + 1;
  raycaster.setFromCamera(mouse, camera);

  const intersects = raycaster.intersectObjects(scene.children);
  if (intersects.length > 0) {
    const objectHit = intersects[0].object;

    // Only interact with the ground or cylinders
    if (mouseAction === 1) {
      // ADD
      const coords = intersects[0].point;
      addCylinder(coords.x, coords.z);
    } else if (mouseAction === 2) {
      // DRAG
      // Only start dragging if the item is not the ground
      if (objectHit !== ground) {
        dragItem = objectHit;
      }
    } else if (mouseAction === 3) {
      // DELETE
      // Check if the clicked object is NOT the ground
      if (objectHit !== ground) {
        scene.remove(objectHit);
      }
    }
  }
}

function handleMouseMove(event) {
  if (!dragItem) return; // Only proceed if an item is being dragged
  mouse.x = (event.clientX / width) * 2 - 1;
  mouse.y = -(event.clientY / height) * 2 + 1;
  raycaster.setFromCamera(mouse, camera);

  const intersects = raycaster.intersectObject(ground);
  if (intersects.length > 0) {
    // Use the intersection point to set the position of the dragItem.
    // Ensure dragItem's scale is untouched; it should remain the same as originally created.
    dragItem.position.copy(intersects[0].point);
    dragItem.position.y = 3; // Keep the cylinder at a fixed height
  }
}

function handleMouseUp() {
  dragItem = null; // Release the dragged item
}

function doChangeMouseAction() {
  const checkedAction = document.querySelector(
    'input[name="mouseAction"]:checked'
  );
  if (checkedAction) {
    switch (checkedAction.id) {
      case "mouseAdd":
        mouseAction = 1;
        break;
      case "mouseDrag":
        mouseAction = 2;
        break;
      case "mouseDelete":
        mouseAction = 3;
        break;
    }
  }
}

// Event listeners
window.addEventListener("mousedown", handleMouseDown);
window.addEventListener("mousemove", handleMouseMove);
window.addEventListener("mouseup", handleMouseUp);

// Animation loop
function animate() {
  requestAnimationFrame(animate);
  renderer.render(scene, camera);
}
animate();</code></pre>

        <p>Now we have a scene where we can add, drag, and delete cylinders using the mouse.</p>


        <p>
      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Particle_Effects">
      <br />
      <header><b>Particle Effects</b></header>
      <article>

        <p>Particle effects are a way to simulate and render large numbers of small, simple objects in a scene.</p>

        <p>They are often used to create effects like smoke, fire, rain, snow, and explosions.</p>

        <p>In Three.js, particle effects can be created using the <b>THREE.Points</b> class.</p>

        <p>The <b>THREE.Points</b> class is a type of geometry that represents a collection of points in 3D space.</p>

        <p>Each point can have its own color, size, and other properties.</p>

        <p>To create a particle effect, we can create a geometry and a material, and then use the <b>THREE.Points</b>
          class to create a mesh.</p>

        <p>For example, we can create a particle effect that simulates falling confetti:</p>

        <pre><code class="javascript">const particleCount = 1000;
const particles = new THREE.BufferGeometry();
const positions = new Float32Array(particleCount * 3); // 3 coordinates for each particle
const colors = new Float32Array(particleCount * 3); // 3 color values for each particle

const sizes = new Float32Array(particleCount); // Size for each particle

for (let i = 0; i < particleCount; i++) {
  // Random position
  positions[i * 3] = (Math.random() - 0.5) * 100; // x
  positions[i * 3 + 1] = (Math.random() - 0.5) * 100; // y
  positions[i * 3 + 2] = (Math.random() - 0.5) * 100; // z

  // Random color
  colors[i * 3] = Math.random(); // r
  colors[i * 3 + 1] = Math.random(); // g
  colors[i * 3 + 2] = Math.random(); // b

  // Random size
  sizes[i] = Math.random() * 5; // Size between 0 and 5

}

particles.setAttribute("position", new THREE.BufferAttribute(positions, 3));
particles.setAttribute("color", new THREE.BufferAttribute(colors, 3)); // Set color attribute

particles.setAttribute("size", new THREE.BufferAttribute(sizes, 1)); // Set size attribute

const particleMaterial = new THREE.PointsMaterial({
    size: 1,
    vertexColors: true, // Use vertex colors
    sizeAttenuation: true, // Size decreases with distance
    transparent: true,
    opacity: 0.5,
});


const particleSystem = new THREE.Points(particles, particleMaterial);

scene.add(particleSystem);</code></pre>

        <p>In this example, we create a particle system with 1000 particles.</p>
        <p>Each particle has a random position, color, and size.</p>

        <p>We use the <b>THREE.BufferGeometry</b> class to create a geometry that can hold a large number of points.</p>
        <p>We set the position, color, and size attributes of the geometry using the <b>setAttribute</b> method.</p>

        <p>We create a material using the <b>THREE.PointsMaterial</b> class, which allows us to set the size and color
          of the particles.</p>

        <p>Finally, we create a <b>THREE.Points</b> object using the geometry and material, and add it to the scene.</p>

        <p>We can also animate the particles to create a falling confetti effect:</p>

        <pre><code class="javascript">function animateParticles() {
    const positionArray = particles.attributes.position.array;

    // Update the positions of the particles to create a falling effect
    for (let i = 0; i < particleCount; i++) {
        positionArray[i * 3 + 1] -= 0.1; // Move down
        if (positionArray[i * 3 + 1] < -50) {
            positionArray[i * 3 + 1] = 50; // Reset to the top
        }
    }

    particles.attributes.position.needsUpdate = true; // Mark the position attribute as needing an update
}

function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
    animateParticles(); // Call the particle animation function
}
animate();</code></pre>

        <p>In this example, we update the y-coordinate of each particle to create a falling effect.</p>

        <p>When a particle goes below a certain threshold, we reset its position to the top of the screen.</p>

        <p>We also rotate the particle system to create a nice effect.</p>

        <p>We can also create other particle effects, such as fire, smoke, and explosions, using the same technique.</p>

        <p>For example, we can create a fire effect using a particle system with a red-orange color and a larger size.
        </p>

        <p>We can also use textures to create more complex particle effects.</p>

        <p>For example, we can use a texture of a snowflake to create a snow effect:</p>

        <pre><code class="javascript">const snowflakeTexture = new THREE.TextureLoader().load("https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/snowflake.png");
const particleMaterial = new THREE.PointsMaterial({
  size: 1,
  alphaTest: 0.25, // Discard pixels with an alpha value below this threshold so we don't see the edges of the texture
  map: snowflakeTexture, // Use the snowflake texture
  transparent: true,
  sizeAttenuation: true, // Size decreases with distance
  vertexColors: true, // Use vertex colors
  opacity: 0.5,
});</code></pre>

        <p>Now we should see randomly colored snowflakes falling from the sky.</p>

        <p>To make these snowflakes one color (e.g. white) we can modify our colors to have the same value for each
          particle:</p>

        <pre><code class="javascript">for (let i = 0; i < particleCount; i++) {
  // Random position
  positions[i * 3] = (Math.random() - 0.5) * 100; // x
  positions[i * 3 + 1] = (Math.random() - 0.5) * 100; // y
  positions[i * 3 + 2] = (Math.random() - 0.5) * 100; // z

  // White color
  colors[i * 3] = 1; // r
  colors[i * 3 + 1] = 1; // g
  colors[i * 3 + 2] = 1; // b

  // Random size
  sizes[i] = Math.random() * 5; // Size between 0 and 5
} // Set the color to white for all particles</code></pre>

        <p>Now we should see white snowflakes falling from the sky.</p>

        <p>Let's contain these snowflakes to a certain area, like within a cube:</p>

        <p>First, we have to set our bounding for the snowflakes:</p>

        <pre><code class="javascript">// Size of the cube that contains the snowflakes
const cubeSize = 100; // Size of the cube

for (let i = 0; i < particleCount; i++) {
  // Initialize the snowflakes within the cube
  positions[i * 3] = (Math.random() - 0.5) * cubeSize; // X position
  positions[i * 3 + 1] = Math.random() * cubeSize; // Y position starts from 0 to cubeSize
  positions[i * 3 + 2] = (Math.random() - 0.5) * cubeSize; // Z position

  // Set particle color to white
  colors[i * 3] = 1; // Red
  colors[i * 3 + 1] = 1; // Green
  colors[i * 3 + 2] = 1; // Blue

  sizes[i] = Math.random() * 5;
}</code></pre>

        <p>Then we can create a wireframe cube to house our snowflakes</p>

        <pre><code class="javascript">// Create a cube to contain the snowflakes
const cubeGeometry = new THREE.BoxGeometry(cubeSize, cubeSize, cubeSize);
const cubeMaterial = new THREE.LineBasicMaterial({ color: 0x00ff00 }); // Green color for wireframe
const cubeEdges = new THREE.EdgesGeometry(cubeGeometry); // Creates edges for the cube
const cubeWireframe = new THREE.LineSegments(cubeEdges, cubeMaterial);
scene.add(cubeWireframe);
cubeWireframe.position.y = 50;

camera.position.set(0, 50, 150);</code></pre>

        <p>Let's use what we know about skyboxes to better contain our snow scene:</p>

        <pre><code class="javascript">const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, window.innerWidth / window.innerHeight, 0.1, 1000);
const renderer = new THREE.WebGLRenderer();
renderer.setSize(window.innerWidth, window.innerHeight);
document.body.appendChild(renderer.domElement);
        
// Add Orbit Controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);

const particleCount = 1000;
const particles = new THREE.BufferGeometry();
const positions = new Float32Array(particleCount * 3);
const colors = new Float32Array(particleCount * 3);
const sizes = new Float32Array(particleCount);

// Size of the cube that contains the snowflakes
const cubeSize = 100;

for (let i = 0; i < particleCount; i++) {
    // Initialize the snowflakes within the cube
    positions[i * 3] = (Math.random() - 0.5) * cubeSize; // X position
    positions[i * 3 + 1] = Math.random() * cubeSize; // Y position starts from 0 to cubeSize
    positions[i * 3 + 2] = (Math.random() - 0.5) * cubeSize; // Z position

    // Set particle color to white
    colors[i * 3] = 1; // Red
    colors[i * 3 + 1] = 1; // Green
    colors[i * 3 + 2] = 1; // Blue

    sizes[i] = Math.random() * 5;
}

// Create a cube to contain the snowflakes
const cubeGeometry = new THREE.BoxGeometry(cubeSize, cubeSize, cubeSize);
var materials = [ 
  new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_posX.png'), side: THREE.BackSide, 
  }),
  new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_negX.png'), side: THREE.BackSide, 
  }),
   new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_posY.png'), side: THREE.BackSide, 
  }),
   new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_negY.png'), side: THREE.BackSide, 
  }),
   new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_posZ.png'), side: THREE.BackSide, 
  }),
   new THREE.MeshBasicMaterial({
    map: new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/winter%20world/frost_negZ.png'), side: THREE.BackSide, 
  })
];

const cube = new THREE.Mesh(cubeGeometry,materials);
scene.add(cube);
cube.position.y = 50;

camera.position.set(0, 50, 200);
particles.setAttribute("position", new THREE.BufferAttribute(positions, 3));
particles.setAttribute("color", new THREE.BufferAttribute(colors, 3));
particles.setAttribute("size", new THREE.BufferAttribute(sizes, 1));

const snowflakeTexture = new THREE.TextureLoader().load("https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/snowflake.png");
const particleMaterial = new THREE.PointsMaterial({
    size: 2,
    alphaTest: 0.25,
    map: snowflakeTexture,
    transparent: true,
    sizeAttenuation: true,
    vertexColors: true,
    opacity: 1,
});

const particleSystem = new THREE.Points(particles, particleMaterial);
scene.add(particleSystem);
        
function animateParticles() {
    for (let i = 0; i < particleCount; i++) {
        // Move the snowflakes down
        positions[i * 3 + 1] -= 0.1;

        // Check if the snowflake is out of bounds
        if (positions[i * 3 + 1] < 0) {
          // Reset Y position to the top of the cube
          positions[i * 3 + 1] = cubeSize; 
          // Randomize X and Z positions within the cube bounds
          positions[i * 3] = (Math.random() - 0.5) * cubeSize; // New X position
          positions[i * 3 + 2] = (Math.random() - 0.5) * cubeSize; // New Z position
        }
    }

    particles.attributes.position.array.set(positions);
    particles.attributes.position.needsUpdate = true;
}

function animate() {
    requestAnimationFrame(animate);
    animateParticles();
    renderer.render(scene, camera);
}

animate();</code></pre>

        <p>We can also use particle effects to create explosions and other effects.</p>

        <p>Particle effects are a powerful tool for creating realistic and dynamic effects in Three.js.</p>

        <p>They can be used to create a wide range of effects, from simple particles to complex simulations.</p>

        <p>Let's make a fire using particles, we can achieve this by importing from esm.sh specifically the Three
          Particle Fire
          library.</p>

        <p>We can use the following code to create a fire effect:</p>

        <pre><code class="javascript">import particleFire from "https://esm.sh/three-particle-fire"; // Import the particle fire library

particleFire.install({ THREE: THREE }); // Install the library with THREE 

const width = 500;
const height = 400;

const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);

const renderer = new THREE.WebGLRenderer();
renderer.setSize(width, height);
document.body.appendChild(renderer.domElement);

// Position the camera
camera.position.set(0, 0, 2);

// Add Orbit Controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);

const fireRadius = 0.5; // Radius of the fire
const fireHeight = 3; // Height of the fire
const particleCount = 800; // Number of particles in the fire

const geometry = new particleFire.Geometry(fireRadius, fireHeight, particleCount); // radius, height, number of particles
const material = new particleFire.Material({ color: 0xff2200  }); // scarlet color
material.setPerspective(camera.fov, height); // Set the perspective of the material
const particleFireMesh = new THREE.Points(geometry, material); // Create the particle fire mesh
particleFireMesh.position.y = -0.9;
scene.add(particleFireMesh);


// Animation loop
function animate() {
  requestAnimationFrame(animate);
 
  // Render the scene from the perspective of the camera
  renderer.render(scene, camera);
}

animate();</code></pre>

        <p>In this example, we create a fire effect using the particle fire library.</p>

        <p>We create a geometry and a material for the fire, and then create a mesh using the geometry and material.</p>

        <p>We can also animate the fire to create a flickering effect using what we call <b>delta time</b>.</p>

        <p>As a reminder, delta time is computed by <b>subtracting the current time from the previous time
            when the tick function was last updated</b>.</p>

        <p>In the realm of animation, you can conceptualize delta time as a frame.</p>

        <p>It provides a more controlled and consistent measure, addressing the issue of erratic behavior tied to
          scrolling movements (that often occurs with the use of elapsed time).</p>

        <p>By including delta time we assure that our animation speed remains constant on different machines. In other
          words, our animation
          using delta time is <b>frame-independent</b>, regardless of how fast the computer is, the animation will be
          played at the same speed.</p>

        <p>We can use delta time to add a smooth animation to our particle fire.</p>

        <p>First, we need to create a clock to track the time:</p>

        <pre><code class="javascript">const clock = new THREE.Clock();</code></pre>
        <p>Three.js conveniently provides a getDelta() function, streamlining the process of obtaining the delta time.
          This built-in function simplifies the calculation, making it more accessible and efficient for managing
          time-based updates in our animations.</p>
        <p>We can use the clock to get the delta time:</p>
        <pre><code class="javascript">const delta = clock.getDelta();</code></pre>
        <p>We can use the delta time to update the fire:</p>
        <pre><code class="javascript">particleFireMesh.material.update(delta);</code></pre>
        <p>Now we can put it all together:</p>
        <pre><code class="javascript">// Create a clock
const clock = new THREE.Clock(); // Initialize a clock to track time

// Animation loop
function animate() {
  requestAnimationFrame(animate);

  // Update the particle fire
  var delta = clock.getDelta(); // Get the time elapsed since the last frame

  particleFireMesh.material.update(delta/4); //we divide by 4 to slow the animation down
  // Render the scene from the perspective of the camera
  renderer.render(scene, camera);
  
}

// Start the animation
animate();</code></pre>




      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="Adding_Physics">
      <br />
      <header><b>Adding Physics</b></header>
      <article>
        <p>So far we have made scenes that are static and do not have any physics, meaning that the objects in the scene
          do not interact with each other.</p>

        <p>With the advent of WebGL, the web has been opened up to yet another dimensions of possibilities: <b>3D
            graphics and Interactivity</b>.</p>

        <p>Let's say we want to make a bowling game. We have the ball and the pins arranged. But the problem is we would
          have to code all the steps involved in rolling the ball and the ball knocking down the pins.</p>

        <p>So we set out to code the dynamics involved: how the ball will roll on the floor, its collision with the
          pins, the force of collision, how the pins will collide against each other and so on.</p>

        <p>We would have to code the physics of the game, which is a lot of work.</p>

        <p>There are many ways of adding physics to your project, and it depends on what you want to achieve. You can
          create your own physics with some mathematics and solutions like Raycaster, but if you wish to get realistic
          physics with tension, friction, bouncing, constraints, pivots, etc. and all that in 3D space, you better use a
          <b>physics engine</b>.
        </p>

        <p>A physics engine is a software, a group of codes or a collection
          of objects and algorithms that provides functions for the simulation of a physical world, be it in games or
          other virtual reality application. We don't have to stress yourself coding the movement of a bowling ball or
          how it collides with pins when you can have them represented by physics objects and the actions simulated in a
          realistic way.</p>

        <p>What do you want to make? Is it a catapult game like AngryBirds or a shooting game or you even want to make
          your own soccer game, Physics Engines have you well covered.</p>

        <p>There are many physics engines available for Three.js, but the most popular ones are:</p>

        <ul>
          <li><b>Cannon.js</b>: an open source JavaScript 3D physics engine</li>
          <li><b>Ammo.js</b>: a direct port of the Bullet physics engine to JavaScript, using Emscripten</li>
          <li><b>p2.js</b>: a 2D rigid body physics engine written in JavaScript</li>
          <li><b>Rapier.js</b>: a physics engine that we can use to calculate Rigid body forces, velocities, contacts,
            constraints and more</li>
          <li><b>Matter.js</b>: a JavaScript 2D rigid body physics engine for the web</li>
          <li><b>Oimo.js</b>: a lightweight 3d physics engine for JavaScript. It's a full javascript conversion of
            OimoPhysics.</li>
          <li><b>Physijs</b>: a physics engine built on top of ammo. js (although there is also a cannon. js branch)
          </li>
          <li><b>Box2Djs</b>: a direct port of the Box2D C++ 2D physics engine to JavaScript, using Emscripten.</li>
          <li><b>Planck.js</b>: a JavaScript/TypeScript rewrite of Box2D physics engine for cross-platform HTML5 game
            development.</li>
        </ul>

        <p>A very popular JavaScript physics solution is Ammo.js, which is a direct port of Bullet physics (a C++
          physics engine).
          While directly using Ammo.js is possible, it is not very beginner friendly and has a lot of boilerplate code
          for
          each aspect. Also, as it is not manually written but ported using Emscripten, the code is not easy to
          understand. </p>

        <p>An alternative solution is to use Cannon.js or Physijs.</p>

        <p>We will use Cannon.js for our physics engine.</p>

        <p>Unlike physics engine libraries ported from C++ to JavaScript, Cannon.js is written in JavaScript from the
          start and can take advantage of its features.</p>

        <p>The physics engine implements rigid-body dynamics, discrete collision detection, a Gauss-Seidel constraint
          solver, and can perform cloth simulation.</p>

        <p>First we need to add a script to the HTML to be able to use Cannon.js:</p>

        <pre><code class="html">&lt;script src="https://cdn.rawgit.com/schteppe/cannon.js/master/build/cannon.js"&gt;&lt;/script&gt;</code></pre>

        <p>When implementing physics, not only do we need our scene set up but we also need an equivalent <b>physics
            world</b>.
          It is a separate entity from the Three.js scene.</p>

        <p>The idea is simple. We are going to create a physics world. This physics world is purely theoretical, and we
          cannot see it. But in this world, things fall, collide, rub, slide, etc.</p>

        <p>When we create a Three.js mesh, we will also create a version of that mesh inside the physics world. If we
          make a Box in Three.js, we also create a box in the physics world.</p>

        <p>Then, on each frame, before rendering anything, we tell the physics world to update itself; we take the
          coordinates (position and rotation) of the physics objects and apply them to the corresponding Three.js mesh.
        </p>

        <p>So, after we set up our scene, camera, and renderer (+ orbit controls), we need to create a physics world.
        </p>

        <p>We can do this by creating a new instance of the Cannon.World class:</p>
        <pre><code class="javascript">const world = new CANNON.World();
world.gravity.set(0, -9.81, 0); // Gravity set to -9.81 m/s² (Earth's gravity)</code></pre>

        <p>Next, we need to create a physics body for each object in the scene.</p>

        <p>We can do this by creating a new instance of the Cannon.Body class alsong with each new object:</p>

        <pre><code class="javascript">// Create the plane
const planeGeometry = new THREE.PlaneGeometry(25, 25);
const planeMaterial = new THREE.MeshPhongMaterial({ color: "lightgrey", side: THREE.DoubleSide });
const planeMesh = new THREE.Mesh(planeGeometry, planeMaterial);
planeMesh.rotation.x = -Math.PI / 2; // Rotate the plane to be horizontal
planeMesh.receiveShadow = true;
planeMesh.position.y = -0.5;
scene.add(planeMesh);

// Create the corresponding physics body for the plane
const planeShape = new CANNON.Plane();
const planeBody = new CANNON.Body({ mass: 0 }); // Mass of 0 means it's static
planeBody.quaternion.setFromEuler(-Math.PI / 2, 0, 0); // Rotate plane body to match geometry
planeBody.addShape(planeShape);
world.addBody(planeBody);

// Create the sphere
const sphereGeometry = new THREE.SphereGeometry(1, 32, 32);
const sphereMaterial = new THREE.MeshNormalMaterial();
const sphereMesh = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphereMesh.position.set(0, 5, 0); // Starting position above the plane
sphereMesh.castShadow = true;
scene.add(sphereMesh);

// Create the corresponding physics body for the sphere
const sphereShape = new CANNON.Sphere(0.5);
const sphereBody = new CANNON.Body({ mass: 1 }); // Mass of 1 means it's dynamic, the larger the mass the more force it will take to move it
sphereBody.addShape(sphereShape);
sphereBody.position.set(0, 5, 0); // Ensure it's above the plane
world.addBody(sphereBody); // Add the sphere body to the world</code></pre>

        <p>As you can see in our physics world we introduce <b>quaternion</b>, which is a mathematical representation of
          rotation, as opposed to our standard Euler angles. Here's <a
            href="https://youtu.be/zjMuIxRvygQ?si=yczgsTp0yZf2cSRW">a more detailed explanation.</a></p>

        <p>Quaternions are an extension of imaginary number set, commonely refered to as a hyper-complex number. A
          quaternion can be thought of as a <b>four element vector</b>.</p>

        <p></p>This vector is composed of two distinct components: a <b>scalar</b> and a <b>3 element unit vector</b>.
        </p>
        <p>In mathematical notation, a quaternion is represented as:</p>

        <p class="center" style="font-size: 40px; padding-left: 30%;"><b>q = w + xi + yj + zk</b></p>

        <p>Where <b>w</b> is the scalar part and <b>[x y z]</b> is the vector part.</p>

        <p>The scalar value, <b>w</b>, corresponds to <b>an angle of rotation</b>. The vector term,
          <b>[x y z]</b>, corresponds to <b>an axis of rotation</b>, about which the angle of rotation is performed.
        </p>

        <p>The i, j, and k are unit vectors representing the three Cartesian axes (traditionally x, y, z), <b>the three
            dimensions of
            space</b>.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/quaternion.jpg"
          alt="Quaternion" />

        <p>Compared to rotation matrices, <b>quaternions are more compact, efficient, and numerically stable</b>. </p>
        <p>Compared to Euler angles, they are simpler to compose.</p>
        <p>However, they are not as intuitive and easy to understand and, due to the periodic nature of sine and cosine,
          rotation angles differing precisely by the natural period will be
          encoded into identical quaternions and recovered angles in radians will be limited to [0, 2&pi;]</p>

        <p>We set our planeBody.quaternion.setFromEuler(-Math.PI / 2, 0, 0); in order to rotate the plane body to match
          the
          geometry, since our plane is rotated to be horizontal.</p>

        <p>Lastly, we need to update the physics world in the animation loop.</p>

        <p>We can do this by using delta time and the Cannon.js step function:</p>
        <pre><code class="javascript">const clock = new THREE.Clock(); // Create a clock to track time
function animate() {
  requestAnimationFrame(animate);

  const deltaTime = clock.getDelta(); // Get the time elapsed since the last frame

  world.step(1 / 60, deltaTime, 3); // Step the physics world, parameters: time step, delta time, number of substeps
  
  // Update the Three.js objects to match the physics bodies
  sphereMesh.position.copy(sphereBody.position); // Update the sphere mesh position

  // Render the scene from the perspective of the camera
  renderer.render(scene, camera);

}

animate();</code></pre>

        <p>Now we have a simple physics simulation with a plane and a sphere.</p>

        <p>The sphere will fall under the influence of gravity and collide with the plane.</p>

        <p>We can increase the bounciness of the sphere by setting the <b>restitution property, which is a measure of
            how much energy is conserved during a collision</b>.</p>

        <h3>Coefficient of Restitution</h3>

        <p>When two objects collide with each other, many forces come into play, which also means the application of
          various mathematical equations. Many of these laws were first derived by the same super popular scientist who
          is credited with numerous discoveries and derivations, meaning that he has a number of patents to his name --
          Sir Isaac Newton.</p>

        <p>Pertaining to the collision of two objects, Newton formulated a theory that we now know as Newton's law of
          <b>restitution</b>.
        </p>

        <p>It simply states that when two bodies collide, the speed with which they move <i>after</i> the collision
          depends on the material from which they are made.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/Before-after-collision.png"
          alt="Newton's Law of Restitution" />

        <p>The <b>coefficient of restitution</b> (CoR) is a measure of <b>how elastic a collision is between two
            objects</b>.
        </p>

        <p>It is defined as the ratio of the relative velocity of separation to the relative velocity of approach.</p>
        <p>In simple terms, it tells us <b>how much energy is conserved during a collision</b>.</p>

        <p>In most cases, the coefficient of restitution is <b>between 0 and 1</b>.</p>

        <img class="center" style="width: 80%; height: 80%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/coefficient%20of%20restitution.png"
          alt="Coefficient of Restitution" />

        <p>A <b>coefficient of restitution of 1</b> means that the collision is <b>perfectly elastic</b>, meaning that
          <b>no energy is
            lost during the collision</b>. In other words, <b>the objects will bounce off each other with the same speed
            and
            energy as they had before the collision</b>.
        </p>

        <p>A <b>coefficient of restitution of 0</b> means that the collision is <b>perfectly inelastic</b>, meaning that
          <b>all energy is
            lost during the collision</b> and <b>the objects stick together</b>.
        </p>

        <p>A coefficient of restitution between 0 and 1 means that the collision is <b>partially elastic</b>, meaning
          that <b>some energy is lost during the collision</b>.</p>

        <p>Let's suppose a rubber ball bounces on a flat, hard surface. Obviously, the rubber ball will rebound off the
          surface, but with only a fraction of its original energy, because all real collisions are inelastic. (Note: If
          this collision were elastic, then the ball would have bounced back with the same amount of energy it had
          before striking the surface.)</p>

        <p>When we 'deform' something by colliding it with something else (say, when we bounce a basketball
          on the ground), a fraction of its original energy is lost. That's why the basketball bounces lower with every
          collision -- as its energy gets converted to heat/vibrations.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/Bouncing-basketballs.png"
          alt="Coefficient of Restitution Example" />

        <p>The formula to calculate the coefficient of restitution is rather straightforward. Since it is defined as a
          ratio of the final to the initial relative velocity between two objects after their collision, it can be
          mathematically represented as follows:</p>

        <img class="center" style="width: 80%; height: 80%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/coefficient%20of%20restitution%20formula%201.png"
          alt="Coefficient of Restitution Formula" />

        <p>When considering a one-dimensional collision of two objects, A and B, the coefficient of restitution could be
          calculated by:</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/coefficient%20of%20restitution%20formula%202.png"
          alt="Coefficient of Restitution Formula 2" />

        <p>In the case of a ball bouncing off a flat, stationary surface, the coefficient of restitution turns out to
          be:</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/coefficient%20of%20restitution%20formula%203.png"
          alt="Coefficient of Restitution Formula 3" />

        <p>In the case of our sphere and plane, we can set the restitution property of the sphere to a value between 0
          and 1 to control how bouncy the sphere is.</p>

        <p>In Cannon.js, the restitution property is set on the material of
          the body.</p>

        <p>We can create a new material for the plane and set the restitution property:</p>
        <pre><code class="javascript">// Set restitution for the plane
planeBody.material = new CANNON.Material();
planeBody.material.restitution = 0.6; // Adjust this value for bounciness</code></pre>

        <p>And of the sphere:</p>

        <pre><code class="javascript">// Set restitution for the sphere
sphereBody.material = new CANNON.Material();
sphereBody.material.restitution = 0.9; // Adjust this value for bounciness</code></pre>

        <p>Now the sphere will bounce off the plane when it collides.</p>

        <p>Let's use what we know about raycasting to add the ability to pick up and drop the sphere.</p>

        <p>We can do this by first creating a new raycaster to detect when the mouse is over the sphere and then pick
          it up.</p>

        <pre><code class="javascript"">// Raycaster and mouse
const raycaster = new THREE.Raycaster();
const mouse = new THREE.Vector2();
let isDragging = false; // Flag to check if the sphere is being dragged
</code></pre>

        <p>We can write a function to handle the mouse down event:</p>

        <pre><code class="javascript">function onMouseDown(event) {
  // Calculate mouse position in normalized device coordinates
  mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
  mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;

  // Update the picking ray with the camera and mouse position
  raycaster.setFromCamera(mouse, camera);

  // Calculate objects intersecting the picking ray
  const intersects = raycaster.intersectObject(sphereMesh);
  if (intersects.length > 0) {
    isDragging = true;
    sphereBody.velocity.set(0, 0, 0); // Reset velocity when picked up
  }
}
</code></pre>

        <p>The onMouseDown function will be called when the mouse is clicked.</p>

        <p>We can create another function to handle the mouse up event:</p>

        <pre><code class="javascript">function onMouseUp() {
  if (isDragging) {
    isDragging = false;
    // Release the sphere and apply an initial force downward
    sphereBody.velocity.set(0, -2, 0); // Apply an initial downward velocity to simulate a bounce
  }
}</code></pre>

        <p>The onMouseUp function will be called when the mouse is released.</p>

        <p>And finally, we can create a function to handle the mouse move event:</p>

        <pre><code class="javascript">function onMouseMove(event) {
  if (isDragging) {
    // Calculate mouse position in normalized device coordinates
    mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
    mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;

    // Update the picking ray with the camera and mouse position
    raycaster.setFromCamera(mouse, camera);

    // Get the position in world coordinates
    const intersects = raycaster.intersectObject(planeMesh);
    if (intersects.length > 0) {
      const intersectPoint = intersects[0].point;
      // Set the sphere's position to be above the plane
      sphereBody.position.set(intersectPoint.x, intersectPoint.y + 2, intersectPoint.z); // Keep it above the plane
      sphereMesh.position.copy(sphereBody.position); // Sync visual sphere position
    }
  }
}</code></pre>

        <p>Which will be called when the mouse is moved.</p>

        <p>We can add event listeners to the document to handle the mouse events:</p>

        <pre><code class="javascript">document.addEventListener("mousedown", onMouseDown, false);
document.addEventListener("mouseup", onMouseUp, false);
document.addEventListener("mousemove", onMouseMove, false);</code></pre>

        <p>Now we can pick up and drop the sphere using the mouse.</p>

        <p>We can add more objects to the scene and create more complex physics simulations.</p>

        <h2>Colliding with Other Objects</h2>

        <p>We can also create more complex objects and add them to the scene.</p>

        <p>Let's start by setting up our Physics world:</p>

        <pre><code class="javascript">// Initialize physics world
const world = new CANNON.World();
world.gravity.set(0, -9.81, 0);</pre></code>

        <p>We can also create our clock for delta time and step our physics world in our animate function:</p>

        <pre><code class="javascript">// Animation loop
const clock = new THREE.Clock();

function animate() {
  requestAnimationFrame(animate);
  const delta = clock.getDelta();

  // Step the physics world
  world.step(1 / 60, delta, 3);

  renderer.render(scene, camera);
}

animate();</code></pre>

        <p>Now we can create our plane (in both the scene and our Physics World). However, by default, CANNON.Plane() is
          an infinite plane, but its orientation and the shape's effective size are not directly set by dimensions like
          in THREE.PlaneGeometry. To get a finite plane matching our visual plane size, we will use CANNON.Box:</p>

        <pre><code class="javascript">// Define size for the visual and physics ground plane
const planeSize = 25;

// --- Visual Ground Plane ---
const planeGeo = new THREE.PlaneGeometry(planeSize, planeSize);
const planeMat = new THREE.MeshStandardMaterial({
  color: "lightgrey",
  side: THREE.DoubleSide
});
const planeMesh = new THREE.Mesh(planeGeo, planeMat);
planeMesh.rotation.x = -Math.PI / 2;
planeMesh.receiveShadow = true;
scene.add(planeMesh);

// --- Physics Ground Plane ---
// Using a box shape to match the visual plane's size
const groundShape = new CANNON.Box(
  new CANNON.Vec3(planeSize / 2, 0.1, planeSize / 2)
);
const groundBody = new CANNON.Body({ mass: 0 });
groundBody.addShape(groundShape);
groundBody.position.set(0, 0, 0);
world.addBody(groundBody);</code></pre>

        <p>Now we can start adding in our objects:</p>

        <pre><code class="javascript">// Arrays to hold visual meshes and physics bodies
const objects = [];
const physicsBodies = [];

// Create a wall of cubes
const rows = 7;
const cols = 15;
const cubeSize = 1;
const startX = -(cols / 2) * cubeSize + cubeSize / 2;
const startY = -0.5;
const startZ = 0;

for (let i = 0; i < rows; i++) {
  for (let j = 0; j < cols; j++) {
    const x = startX + j * cubeSize;
    const y = startY + i * cubeSize;
    const z = startZ;

    // Visual cube
    const geo = new THREE.BoxGeometry(cubeSize, cubeSize, cubeSize);
    const mat = new THREE.MeshStandardMaterial({ color: 0x00ff00 });
    const mesh = new THREE.Mesh(geo, mat);
    mesh.castShadow = true;
    scene.add(mesh);
    objects.push(mesh);

    // Physics body
    const shape = new CANNON.Box(
      new CANNON.Vec3(cubeSize / 2, cubeSize / 2, cubeSize / 2)
    );
    const body = new CANNON.Body({ mass: 0.5 });
    body.addShape(shape);
    body.position.set(x, y, z);
    world.addBody(body);
    physicsBodies.push(body);
  }
}</code></pre>

        <p>We will see a wall of cubes with 7 rows and 15 cubes per row once we update our animate function:</p>

        <pre><code class="javascript">for (let i = 0; i < physicsBodies.length; i++) {
    const pBody = physicsBodies[i];
    const mesh = objects[i];
    mesh.position.copy(pBody.position);
    mesh.quaternion.copy(pBody.quaternion);
}</code></pre>

        <p>We want to be able to shoot balls at the wall to knock down the cube, this will require a raycaster:</p>

        <pre><code class="javascript">// For shooting balls
const ballRadius = 0.5;
const ballGeometry = new THREE.SphereGeometry(ballRadius, 16, 16);
const ballMaterial = new THREE.MeshStandardMaterial({ color: 0xff0000 });

const ballMeshes = [];
const ballBodies = [];

// Raycaster for mouse interaction
const raycaster = new THREE.Raycaster();
const mouse = new THREE.Vector2();

function onMouseDown(event) {
  // Convert mouse coords
  mouse.x = (event.clientX / window.innerWidth) * 2 - 1;
  mouse.y = -(event.clientY / window.innerHeight) * 2 + 1;

  // Raycast from camera
  raycaster.setFromCamera(mouse, camera);
  const direction = raycaster.ray.direction.clone();
  const origin = raycaster.ray.origin.clone();

  // Create visual sphere
  const mesh = new THREE.Mesh(ballGeometry, ballMaterial);
  mesh.castShadow = true;
  scene.add(mesh);
  ballMeshes.push(mesh);

  // Create physics body
  const shape = new CANNON.Sphere(ballRadius);
  const body = new CANNON.Body({ mass: 1 });
  body.addShape(shape);

  // Position the ball slightly in front of camera
  const spawnPos = origin.clone().add(direction.clone().multiplyScalar(1));
  body.position.copy(spawnPos);
  // Set initial velocity forward
  const shootSpeed = 50;
  body.velocity.copy(direction.multiplyScalar(shootSpeed));

  world.addBody(body);
  ballBodies.push(body);
}

document.addEventListener("mousedown", onMouseDown);</code></pre>

        <p>Now we should possess the ability to shoot balls at our scene. Let's modify our animate function to reflect
          this:</p>

        <pre><code class="javascript">for (let i = 0; i < ballBodies.length; i++) {
    ballMeshes[i].position.copy(ballBodies[i].position);
    ballMeshes[i].quaternion.copy(ballBodies[i].quaternion);
}</code></pre>

        <p>And now we should be able to knock down the cubes with the spheres!</p>

        <p>We can also use Cannon.js to create more advanced physics simulations, such as ragdoll physics, soft body
          physics, and fluid dynamics.</p>














      </article>
      <br />
    </section>


























    <hr />
    <section class="main-section" id="Cameras_and_Projections">
      <br />
      <header><b>Cameras and Projections</b></header>
      <article>
        <p>So we spoke about the two main cameras in Three.js, the <b>PerspectiveCamera</b> and the
          <b>OrthographicCamera</b>, but what exactly are they?
        </p>

        <p>The PerspectiveCamera, as we know, is the most commonly used camera in Three.js. It mimics the perspective
          projection of real-world cameras, creating a sense of depth and realism in the scene. So that the objects far
          from the
          camera look smaller and those near to the camera look bigger, just like our eyes work.</p>

        <p>Once again, it is defined by a field of view (FOV), aspect ratio, and near and far clipping planes.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/persepective%20camera.png"
          alt="Perspective Camera" />

        <p>Let's set up a scene that uses a PerspectiveCamera:</p>

        <pre><code class="javascript">const scene = new THREE.Scene();
scene.background = new THREE.Color("green"); 

// Set up the Camera (Perspective)
const camera = new THREE.PerspectiveCamera(
  75, // Field of view (in degrees)
  window.innerWidth / window.innerHeight, // Aspect ratio
  0.1, // Near clipping plane
  1000 // Far clipping plane
);

// Position the camera
camera.position.set(3, 5, 7);
camera.lookAt(0, 0, 0); 

const renderer = new THREE.WebGLRenderer({ antialias: true }); // Add antialiasing
renderer.setSize(window.innerWidth, window.innerHeight);
renderer.shadowMap.enabled = true; // Enable shadow maps
renderer.shadowMap.type = THREE.PCFSoftShadowMap; // Softer shadows
document.body.appendChild(renderer.domElement);

// Create the Plane
const planeGeometry = new THREE.PlaneGeometry(20, 20); // Larger plane
const planeMaterial = new THREE.MeshStandardMaterial({
  color: "lightgrey",
  side: THREE.DoubleSide
});
const plane = new THREE.Mesh(planeGeometry, planeMaterial);
plane.rotation.x = Math.PI / 2; // Rotate to lie flat
plane.receiveShadow = true; // Plane can receive shadows
scene.add(plane);

// Create the Cube
const cubeGeometry = new THREE.BoxGeometry(1, 1, 1); // Width, height, depth
const cubeMaterial = new THREE.MeshStandardMaterial({ color: "blue" });
const cube = new THREE.Mesh(cubeGeometry, cubeMaterial);
cube.position.set(-1.5, 0.5, 0); // Position above the plane
cube.castShadow = true; // Cube can cast shadows
scene.add(cube);

// Create the Sphere
const sphereGeometry = new THREE.SphereGeometry(0.75, 32, 16); // Radius, width segments, height segments
const sphereMaterial = new THREE.MeshStandardMaterial({ color: 0xff0000 }); // Red color
const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphere.position.set(1.5, 0.75, 0); // Position above the plane
sphere.castShadow = true; // Sphere can cast shadows
scene.add(sphere);

// Add Lighting
const ambientLight = new THREE.AmbientLight("grey", 0.5); // Color, Intensity
scene.add(ambientLight);

const directionalLight = new THREE.DirectionalLight("white", 0.8); // Color, Intensity
directionalLight.position.set(5, 10, -10); // Position of the light source
directionalLight.castShadow = true; // Enable shadow casting for this light

scene.add(directionalLight);

function animate() {
  requestAnimationFrame(animate);
  renderer.render(scene, camera);
}

animate();</code></pre>

        <p>The OrthographicCamera, on the other hand, provides an orthographic projection, which means that objects
          appear the same size regardless of their distance from the camera. This camera type is often used for 2D games
          or architectural visualizations.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/orthographic%20camera.png"
          alt="Orthographic Camera" />

        <p>To create an OrthographicCamera in Three.js, we can use the following constructor:</p>

        <pre><code class="javascript">const camera = new THREE.OrthographicCamera(left, right, top, bottom, near, far);</code></pre>
        <p>Where:</p>
        <ul>
          <li><b>left</b>: The left plane of the camera frustum</li>
          <li><b>right</b>: The right plane of the camera frustum</li>
          <li><b>top</b>: The top plane of the camera frustum</li>
          <li><b>bottom</b>: The bottom plane of the camera frustum</li>
          <li><b>near</b>: The near clipping plane distance</li>
          <li><b>far</b>: The far clipping plane distance</li>
        </ul>

        <p>We can modify our cube and sphere scene to use an OrthographicCamera instead:</p>

        <pre><code class="javascript">//Set up the Camera (Orthographic)
const aspect = window.innerWidth / window.innerHeight;
const frustumSize = 15; // Controls the 'zoom' or size of the view
const camera = new THREE.OrthographicCamera(
  (frustumSize * aspect) / -2, // left
  (frustumSize * aspect) / 2, // right
  frustumSize / 2, // top
  frustumSize / -2, // bottom
  -100, // Near clipping plane
  1000 // Far clipping plane
);
</code></pre>

        <p>Orthographic projection is just one of the types of parallel projection.</p>

        <p><b>Representing an n-dimensional object into an n-1 dimension is known as projection</b>.</p>

        <p>It is process of converting a 3D object into 2D object, where we represent a 3D object on a 2D plane
          {(x,y,z)->(x,y)}. It is also defined as mapping or transforming of the object in projection plane or view
          plane.</p>

        <p>When geometric objects are formed by the intersection of lines with a plane, the plane is called the
          projection plane and the lines are called projections.</p>

        <h2>Types of Projections:</h2>

        <ul>
          <li><b>Parallel</b> Projection (which includes Orthographic)</li>
          <li><b>Perspective</b> Projection</li>
        </ul>

        <img class="center" style="width: 75%; height: 75%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/projection%20types.png"
          alt="Projection Types 1" />

        <h2>Center of Projection</h2>

        <p>The center of projection (CoP) is an arbitrary point from where the lines are drawn on each point of an
          object.</p>

        <ul>
          <li>In parallel projection, <b>the CoP is at infinity</b>.</li>
          <li>In perspective projection, <b>the CoP is at a finite distance</b>.</li>
        </ul>

        <h2>Parallel Projection</h2>

        <p>A parallel projection is formed by extending parallel lines from each vertex of object until they intersect
          plane of screen.</p>

        <p>Parallel projection transforms object to the view plane along parallel lines. A projection is said to be
          parallel, if center of projection is at an infinite distance from the projected plane.</p>

        <p>A parallel projection preserves relative proportion of objects, accurate views of the various sides of an
          object are obtained with a parallel projection. The projection lines are parallel to each other and extended
          from the object and intersect the view plane. </p>

        <p>It preserves relative propositions of objects, and it is used in drafting to produce scale drawings of 3D
          objects. This is not a realistic representation, the point of intersection is the projection of the vertex.
        </p>

        <img class="center" style="width: 75%; height: 75%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/parallel%20projection.png"
          alt="Parallel Projection" />

        <p>Parallel projection is divided into two parts and these two parts sub divided into many:</p>

        <ul>
          <li>Orthographic Projection</li>
          <li>Oblique Projection</li>
        </ul>

        <h3>Orthographic Projection</h3>

        <p>In orthographic projection the direction of projection is normal to the projection of the plane. In
          orthographic lines are parallel to each other making an angle 90 with view plane. Orthographic parallel
          projections are done by <b>projecting points along parallel lines that are perpendicular to the projection
            line</b>.
        </p>

        <p>Orthographic projections are most often used to procedure the front, side, and top views of an object and are
          called <b>evaluations</b>.</p>

        <p>Engineering and architectural drawings commonly employ these orthographic projections.</p>

        <p>Transformation equations for an orthographic parallel projection as straight forward. Some special
          orthographic parallel projections involve plan view, side elevations. We can also perform orthographic
          projections that <b>display more than one phase of an object</b>, such views are called <b>monometric
            orthographic
            projections</b>.</p>

        <h3>Orthographic Projection has two categories:</h3>

        <ul>
          <li>Multiview Projection</li>
          <li>Axonometric Projection</li>
        </ul>

        <h3>Multiview Projection</h3>

        <p>Multiview Projection is further divided into three categories:</p>

        <ol>
          <li><b>Top-View (or First Angle Projection)</b>: In this projection, the rays that emerge from the top of the
            polygon
            surface are observed.</li>
          <img class="center" style="width: 70%; height: 70%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/top%20view.png"
            alt="Top View" />
          <li><b>Front-View (or Third Angle Projection)</b>: In this orthographic projection front face view of the
            object is
            observed.</li>
          <img class="center" style="width: 70%; height: 70%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/front%20view.png"
            alt="Front View" />
          <li><b>Side-View</b>: It is another type of projection orthographic projection where the side view of the
            polygon
            surface is observed.</li>
          <img class="center" style="width: 35%; height: 35%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/side%20view.png"
            alt="Side View" />
        </ol>

        <h3>Axonometric Projection</h3>

        <p>Axonometric projection is an orthographic projection, where the projection lines are perpendicular to the
          plane of projection, and the object is rotated around one or more of its axes to show multiple sides.</p>

        <p>It is further divided into three categories:</p>

        <ol>
          <li><b>Isometric Projection</b>: It is a method for visually representing three-dimensional objects in
            two-dimensional display in technical and engineering drawings. Here in this projection, the three coordinate
            axes appear equally foreshortened and the angle between any two of them is 120 degrees.</li>
          <img class="center" style="width: 80%; height: 80%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/isometric%20projection.png"
            alt="Isometric Projection" />
          <li><b>Dimetric Projection</b>: It is a kind of orthographic projection where the visualized object appears to
            have
            only two adjacent sides and angles are equal.</li>
          <img class="center" style="width: 55%; height: 55%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/dimetric%20projection.png"
            alt="Dimetric Projection" />
          <li><b>Trimetric Projection</b>: It is a kind of orthographic projection where the visualized object appears
            to have
            all the adjacent sides and angles unequal.</li>
          <img class="center" style="width: 55%; height: 55%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/trimetric%20projection.png"
            alt="Trimetric Projection" />
        </ol>

        <h3>Oblique Projection</h3>

        <p>Oblique projection is a kind of parallel projection where projecting rays emerges parallel from the surface
          of the polygon and incident at an angle other than 90 degrees on the plane.</p>
        <p>It is further divided into two categories:</p>

        <ol>
          <li><b>Cavalier Projection</b>: is a kind of oblique projection where the projecting lines emerge parallel
            from the
            object surface and incident at 45′ rather than 90′ at the projecting plane. In this projection, the length
            of the reading axis is larger than the cabinet projection.</li>
          <img class="center" style="width: 75%; height: 75%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/cavalier%20projection.png"
            alt="Cavalier Projection" />
          <li><b>Cabinet Projection</b>: It is similar to that cavalier projection but here the length of reading axes
            just
            half than the cavalier projection and the incident angle at the projecting plane is 63.4′ rather 45′.</li>
          <img class="center" style="width: 75%; height: 75%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/cabinet%20projection.png"
            alt="Cabinet Projection" />
        </ol>

        <h2>Perspective Projection</h2>

        <p>In Perspective Projection the <b>center of projection (CoP) is at finite distance from projection plane</b>.
        </p>

        <p>The perspective projection can be easily described by the following figure:</p>

        <img class="center" style="width: 60%; height: 60%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/perspective%20projection%202.png"
          alt="Perspective Projection" />

        <ul>
          <li><b>Center of Projection</b> – It is a point where lines or projection that are not parallel to projection
            plane appear to meet.</li>
          <li><b>View Plane or Projection Plane</b> – The view plane is determined by :</li>
          <ul>
            <li>View reference point <b>R0(x0, y0, z0)</b></li>
            <li>View plane normal</li>
          </ul>
          <li><b>Location of an Object</b> – It is specified by a point P that is located in world coordinates at (x, y,
            z)
            location. The objective of perspective projection is to determine the image point P’ whose coordinates are
            (x’, y’, z’)</li>
        </ul>

        <p>This projection produces realistic views but <b>does not preserve relative proportions of an object
            dimensions</b>.
          Projections of distant object are smaller than projections of objects of same size that are closer to
          projection plane.</p>

        <p>In perspective projection, <b>the lines of projection are not parallel</b>. Instead, they all converge at a
          single point called the center of projection or projection reference point.</p>

        <p>The object positions are transformed to the view plane along these converged projection lines and the
          projected view of an object is determined by calculating the intersection of the converged projection lines
          with the view plane, as shown in the below figure:</p>

        <img class="center" style="width: 75%; height: 75%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/perspective%20projection%203.png"
          alt="Perspective Projection 3" />

        <h3>Types of Perspective Projection</h3>

        <p>Classification of perspective projection is on basis of <b>vanishing points</b> (It is a point in image where
          a
          parallel line through center of projection intersects view plane.).</p>

        <p>The classification is as follows:</p>

        <img class="center" style="width: 55%; height: 55%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/types%20of%20perspective%20projection.png"
          alt="Types of Perspective Projection" />

        <ul>
          <li><b>One Point Perspective Projection</b>: this projection occurs when any of principal
            axes intersects with projection plane or we can say when projection plane is perpendicular to principal
            axis. In the image below, the z axis intersects projection plane whereas x and y axis remain parallel to
            projection plane.</li>
          <br />
          <img class="center" style="width: 50%; height: 50%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/one%20point%20perspective%20projection.png"
            alt="One Point Perspective Projection" />
          <br />
          <li><b>Two Point Perspective Projection</b>: in this projection, the projection plane intersects two of
            principal axis. This is used for objects that are at an angle to the viewer. In the image below, the
            projection plane intersects x and y axis whereas z axis remains parallel to projection plane.</li>
          <br />
          <img class="center" style="width: 75%; height: 75%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/two%20point%20perspective%20projection.png"
            alt="Two Point Perspective Projection" />
          <br />
          <li><b>Three Point Perspective Projection</b>: this projection occurs when all three axis intersects with
            projection plane. There is not any principal axis which is parallel to projection plane.</li>
          <br />
          <img class="center" style="width: 40%; height: 40%;"
            src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/three%20point%20perspective%20projection.png"
            alt="Three Point Perspective Projection" />
          <br />
        </ul>

        <h2>Additional Cameras</h2>

        <p>There are also other types of cameras in Three.js, such as the <b>CubeCamera</b>, which is used for rendering
          cube maps, and the <b>StereoCamera</b>, which is used for rendering stereo images for VR applications.</p>

        <p>As we have already seen, the <b>CubeCamera</b> is used for capturing a panoramic view of the scene from a
          specific position.
          It <b>renders the scene six times, each time capturing the view along one of the cube's faces</b>. This camera
          type is commonly used
          for creating <b>reflections or environment maps</b>.</p>

        <p>Here’s an example of creating and updating a CubeCamera:</p>

        <pre><code class="javascript">const cubeCamera = new THREE.CubeCamera(near, far, resolution);</code></pre>

        <p>Where:</p>
        <ul>
          <li><b>near</b>: The near clipping plane distance</li>
          <li><b>far</b>: The far clipping plane distance</li>
          <li><b>resolution</b>: The resolution of the cube map (e.g., 256)</li>
        </ul>

        <p>To use the CubeCamera, you need to add it to the scene and update it in your render loop:</p>
        <pre><code class="javascript">// Add the CubeCamera to the scene
scene.add(cubeCamera);

// Update the CubeCamera in the render loop
cubeCamera.update(renderer, scene);</code></pre>

        <p>CubeCamera is useful for creating realistic reflections and environment maps in your 3D scenes.</p>

        <p>The <b>ArrayCamera</b> allows you to create an array of cameras and switch between them to achieve different
          perspectives or views. It can be useful for <b>creating multi-camera setups or implementing camera
            transitions</b>.
        </p>

        <p>To create an ArrayCamera, you can use the following constructor:</p>
        <pre><code class="javascript">const cameras = [
  new THREE.PerspectiveCamera(fov, aspect, near, far),
  new THREE.PerspectiveCamera(fov, aspect, near, far)
];
const arrayCamera = new THREE.ArrayCamera(cameras);</code></pre>

        <p>Where:</p>
        <ul>
          <li><b>cameras</b>: An array of cameras to be used in the ArrayCamera</li>
        </ul>

        <p>To use the ArrayCamera, you can set it as the active camera in your render loop:</p>
        <pre><code class="javascript">// Set the ArrayCamera as the active camera
renderer.render(scene, arrayCamera);</code></pre>

        <p>ArrayCamera is useful for creating multi-camera setups or implementing camera transitions in your 3D scenes.
        </p>

        <p>The <b>StereoCamera</b> is used to create stereoscopic 3D effects, providing a <b>sense of depth perception
            by
            rendering separate views for the left and right eye</b>. This camera type is commonly used for <b>virtual
            reality
            (VR) or augmented reality (AR) applications</b>.

        <p>It is used to render the scene through two cameras that mimic the eyes in order to create what we call a
          <b>parallax effect</b> that will lure your brain into thinking that there is depth. You <b>must have the
            adequate
            equipment like a VR headset or red and blue glasses to see the result</b>.
        </p>

        <img class="center" style="width: 70%; height: 70%;"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/images/stereoCamera%20example.png"
          alt="Stereo Camera" />

        <p>Here’s an example of creating a StereoCamera:</p>

        <pre><code class="javascript">const camera = new THREE.StereoCamera();

// Set up left and right cameras
const leftCamera = camera.cameraL;
const rightCamera = camera.cameraR;

// Position the cameras
leftCamera.position.set(x, y, z);
rightCamera.position.set(x, y, z);

// Set the focal length for both cameras
leftCamera.focalLength = 10;
rightCamera.focalLength = 10;</code></pre>

        <p>To render the stereo view, you need to <b>update the camera and render the scene twice, once for the left eye
            and once for the right eye</b>.</p>

        <p>Here’s an example of rendering with a StereoCamera, we would put the following in our animate() function:</p>

        <pre><code class="javascript">// Update the stereo camera
camera.update(renderer, scene);

// Set the active renderer to the left eye
renderer.setRenderTarget(renderer.getRenderTarget().left);
renderer.render(scene, leftCamera);

// Set the active renderer to the right eye
renderer.setRenderTarget(renderer.getRenderTarget().right);
renderer.render(scene, rightCamera);

// Reset the active renderer to the default
renderer.setRenderTarget(null);</code></pre>

        <p>Understanding the different types of cameras available in Three.js is crucial for controlling the view and
          perspective of our 3D scenes.</p>

        <p>Whether we need to create a realistic perspective, an orthographic projection, or capture panoramic views,
          Three.js offers a variety of camera options to suit our needs. By utilising these cameras, we can enhance
          the interactivity and immersion of our web-based 3D applications.</p>

        <p>Remember to experiment with camera settings, position, and movement to achieve the desired effects and create
          captivating user experiences.</p>




      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Shaders">
      <br />
      <header><b>Shaders</b></header>
      <article>
        <p>So we have created objects and added textures to them, but how do we create our own custom
          shaders?</p>

        <p><b>Shaders</b> are small programs that run on the GPU (Graphics Processing Unit) and are used to control the
          rendering
          process in Three.js. They allow us to <b>create custom visual effects, manipulate vertex positions, and
            control how
            materials are rendered</b>.</p>

        <p>These programs are run for each specific section of the graphics pipeline. In a basic sense, shaders are
          nothing more than <b>programs transforming inputs to outputs</b>. Shaders are also very isolated programs in
          that
          they're not allowed to communicate with each other; the only communication they have is via their inputs and
          outputs.</p>

        <h2>GLSL</h2>

        <p>Shaders are written in the language <b>GLSL</b>.</p>

        <p>GLSL stands for <b>OpenGL Shading Language</b>, a high-level shading language used for programming graphics
          shaders
          within the OpenGL API. It's a <b>C-style language that allows developers to control the rendering pipeline on
            the
            GPU</b>, enabling the creation of complex visual effects and graphics. GLSL shaders are executed on the GPU
          and
          are crucial for rendering 2D and 3D graphics in applications and games</p>

        <p>GLSL is tailored for use with graphics and contains useful features specifically targeted at vector and
          matrix manipulation.</p>

        <p>Shaders always begin with a version declaration, followed by a list of input and output variables, uniforms
          and its main function. Each shader's entry point is at its main function where we process any input variables
          and output the results in its output variables. Don't worry if you don't know what uniforms are, we'll get to
          those shortly.</p>

        <h3>Variables and Constants</h3>

        <p>Same as the other programming languages, GLSL utilizes variables for storing information.</p>

        <p>However, it's essential to <b>declare the data type of the variable before use</b>, unlike JavaScript, where
          types are inferred. This explicit declaration ensures strict type handling.</p>

        <pre><code class="GLSL">int a = 77;</code></pre>

        <p>In addition to variables, we have constants, which are variables that must be initialized upon declaration
          and cannot be updated afterward.</p>

        <p>The main difference between constants in GLSL and JavaScript lies in the requirement to specify the data type
          in GLSL. Otherwise, their behavior is similar.</p>

        <pre><code class="GLSL">const int b = 50.0;</code></pre>

        <p>GLSL supports a variety of data types, including:</p>

        <ul>
          <li><b>int</b>: Integer values</li>
          <li><b>float</b>: Floating-point values</li>
          <li><b>bool</b>: Boolean values (true or false)</li>
        </ul>

        <p>Unlike some other languages, <b>GLSL does not include character or string types</b>.</p>

        <pre><code class="GLSL">int a = 77;
const float b = 50.0;
bool c = true;</code></pre>

        <p>Once a variable is assigned a data type, it cannot be reassigned a value of a different type. For instance,
          it's not possible to assign a float value to an int variable or vice versa, ensuring strict adherence to data
          types.</p>

        <pre><code class="GLSL">// These couple lines generate error messages.
int a = 20.0;
float b = 4;</code></pre>

        <p>The only way to assign the value of a variable to another one with a different type is by using a <b>type
            conversion function</b>.</p>

        <p>For instance, you can convert a float value to an integer using the int() function.</p>

        <pre><code class="GLSL">float x = 4.0;
int y = 20;
bool z = false;

// a = 4
int a = int(x);
// b = 20.0
float b = float(y);
// c = 0.0
float c = float(z);</code></pre>

        <p>In addition to variables and type conversions, we have a variety of operators for performing different
          operations such as addition (+), subtraction (-), division (/), and more.</p>

        <pre><code class="GLSL">// a = 4
int a = 2 + 2;
// a = 5
a++;</code></pre>

        <h3>Vectors and Matrices</h3>

        <p>GLSL also supports vectors and matrices, which are essential for representing points, colors, and other
          attributes in 3D space. hese types are analogous to objects in object-oriented languages, as they are composed
          of basic types and allow for more complex operations.</p>


        <p>Vectors are categorized into three types:</p>

        <ul>
          <li>float vectors <b>vec</b></li>
          <li>integer vectors <b>ivec</b></li>
          <li>boolean vectors <b>bvec</b></li>
        </ul>

        <p>Each type is designed to handle specific kinds of data.</p>

        <p>In addition to specifying the type, we need to use a digit to indicate the <b>number of components</b> in a
          vector.</p>

        <p>A vector can have <b>a minimum of 2 components and a maximum of 4 components</b>.</p>

        <pre><code class="GLSL">vec2 vectA = vec2(1.0, 5.0);
ivec3 vectB = ivec3(7, 10, 1);
bvec4 vectC = bvec4(true, true, false, false);</code></pre>

        <p>In the above example, we have a 2D vector (vec2), a 3D vector (ivec3), and a 4D vector (bvec4).</p>

        <p>Another way to create a vector, when all its components have the same value, is by specifying that value only
          once.</p>

        <pre><code class="GLSL">// Same as: vec3 vectD = vec3(0.0, 0.0, 0.0);
vec3 vectD = vec3(0.0);</code></pre>

        <p>We can also initialize a vector by using the values from another vector.</p>

        <pre><code class="GLSL">vec2 vectE = vec2(3.0, 9.0);
// Same as: vec4 vectF = vec4(3.0, 9.0, 0.0, 10.0);
vec4 vectF = vec4(vectE, 0.0, 10.0);
// Same as: vec2 vectG = vec3(3.0, 9.0);
vec2 vectG = vec2(vectF);</code></pre>

        <p>Furthermore, we can combine values from multiple vectors to create a new vector.</p>

        <pre><code class="GLSL">bvec2 vectH = bvec2(true);
bvec2 vectI = bvec2(false);
// Same as: bvec4 vectJ = bvec4(true, true, false, false);
bvec4 vectJ = bvec4(vectH, vectI);</code></pre>

        <p>To access the first component of a vector, <b>specify the variable name followed by a period and then x, r,
            or s</b>.</p>

        <pre><code class="GLSL">vec4 v = vec4(2.0, 3.0, 7.0, 5.0);
// a == b == c == 2.0
float a = v.x;
float b = v.r;
float c = v.s;</code></pre>

        <p>To access the second component of a vector, use the variable name followed by a period and then y, g, or t.
        </p>

        <pre><code class="GLSL">vec4 v = vec4(2.0, 3.0, 7.0, 5.0);
// a == b == c == 3.0
float a = v.y;
float b = v.g;
float c = v.t;</code></pre>

        <p>To access the third component of a vector, use the variable name followed by a period and then z, b, or p.
        </p>

        <pre><code class="GLSL">vec4 v = vec4(2.0, 3.0, 7.0, 5.0);
// a == b == c == 7.0
float a = v.z;
float b = v.b;
float c = v.p;</code></pre>

        <p>To access the fourth component of a vector, use the variable name followed by a period and then w, a, or q.
        </p>

        <pre><code class="GLSL">vec4 v = vec4(2.0, 3.0, 7.0, 5.0);
// a == b == c == 5.0
float a = v.w;
float b = v.a;
float c = v.q;</code></pre>

        <p>As you can see, there are various notations for accessing vector components, each suited for different
          contexts.</p>

        <p>For example:</p>

        <ul>
          <li>When dealing with <b>vertices' positions</b>, it's more intuitive to use <b>x, y, and z</b>.</li>
          <li>For <b>colors, r, g, and b</b> are commonly used.</li>
          <li>While <b>s, t, and p</b> are preferred for <b>textures</b>.</li>
        </ul>

        <p>It's important to note that <b>we can access multiple components of a vector simultaneously</b>. For example,
          we can create a new vector whose components are taken from the 1st and 3rd components of another vector.</p>

        <pre><code class="GLSL">vec4 vectA = vec4(2.0, 3.0, 7.0, 5.0);
// Same as: vec2 vectB = vec2(2.0, 7.0);
vec2 vectB = vectA.xz;</code></pre>

        <p>Also we have the flexibility to repeat values or rearrange them in a different order within vectors.</p>

        <pre><code class="GLSL">vec4 vectA = vec4(2.0, 3.0, 7.0, 5.0);
// Same as: vec3 vectB = vec3(2.0, 2.0, 2.0);
vec3 vectB = vectA.rrr;
// Same as: vec2 vectC = vec2(7.0, 2.0);
vec3 vectC = vectA.br;</code></pre>

        <p>Quite similar to vectors, a matrix is composed of a certain number of floats. So we have mat2, mat3, and mat4
          matrices.</p>

        <p>Unlike vectors, <b>matrices can contain different types of values simultaneously</b>. For example, a mat2
          could
          theoretically have 2 integers and 2 booleans. However, these values will be converted automatically into
          floats.</p>

        <pre><code class="GLSL">// Same as: mat2 mat = mat2(1.0, 1.0, 0.0, 0.0);
mat2 mat = mat2(1, 1, false, false);</code></pre>

        <p>The elements of a matrix are set in a column-major order. This means that when creating a matrix with code,
          the elements must be typed in column order.</p>

        <p>To access the components of a matrix, we use bracket notation. For example, m[0] accesses the first column of
          a matrix m and <b>returns it as a vector</b>.</p>

        <pre><code class="GLSL">mat3 m = mat3(7.0, 4.0, 5.0, 0.0, 2.0, 0.5, 1.0, 3.0, 7.0);
// Same as: vec3 v = vec3(7.0, 4.0, 5.0);
vec3 v = m[0];</code></pre>

        <p>To set the last value of a matrix m to 100, you can use double brackets m[2][2].</p>

        <pre><code class="GLSL">mat3 m = mat3(7.0, 4.0, 5.0, 0.0, 2.0, 0.5, 1.0, 3.0, 7.0);
m[2][2] = 100.0;</code></pre>

        <p>Furthermore, we can combine the dot notation used to get and set vector values with bracket notation for
          matrices.</p>

        <pre><code class="GLSL">mat3 m = mat3(7.0, 4.0, 5.0, 0.0, 2.0, 0.5, 1.0, 3.0, 7.0);
// f value is 4.0
// Same as: float f = m[0][1];
float f = m[0].y;</code></pre>

        <p>Of course, we can perform a variety of operations on vectors and matrices just as we do with other types.
          This includes addition, subtraction, multiplication, and more. For instance, you can add two vectors, multiply
          a matrix by a vector, or even multiply two matrices together.</p>

        <h3>Samplers</h3>

        <p>Another type of variables we have is sampler. Actually we have a couple of them: <b>sampler2D</b>, which you
          will
          use most often for 2D textures, and <b>samplerCube</b>, which is used for cube map textures.</p>

        <p>In short, a sampler is a type of variable used to store image data in GLSL.</p>

        <h3>Arrays</h3>

        <p>Similar to other programming languages, GLSL provides arrays to store collections of data.</p>

        <p>To create an array, we need to specify the type and the number of elements in advance.</p>

        <p>In the example below, the number 7 does not initialize the array with the value 7; instead, it indicates that
          <b>the array can hold up to 7 elements</b>.
        </p>

        <pre><code class="GLSL">float arr[7];</code></pre>

        <p>To access an element of an array, we use bracket notation in the same way as in C or JavaScript.</p>

        <pre><code class="GLSL">float arr[7];
// Sets the value of the first element
arr[0] = 20.0;
// Gets the value of the last element
float f = arr[6];</code></pre>

        <h3>Structures</h3>

        <p>In addition to the different types of variables we've seen, GLSL also allows us to create our own custom
          types.</p>

        <p>To create custom types, we use the keyword <b>struct</b>, followed by a name and a pair of opening and
          closing curly braces.
          Within these braces, we specify the components of our structure, defining our custom type.</p>

        <pre><code class="GLSL">struct myType {
    int c1;
    vec3 c2;
};</code></pre>

        <p>Once we've defined the structure of our custom type, creating a variable of that type is done in the same way
          as with other variable types.</p>

        <pre><code class="GLSL">struct myType {
    int c1;
    vec3 c2;
};

myType a;</code></pre>

        <p>To access the components of a variable of a custom type, whether to get or set their values, we use the dot
          notation, similar to how we access components of vectors.</p>

        <pre><code class="GLSL">struct myType {
    int c1;
    vec3 c2;
};

myType a;

// Sets the value of c1
a.c1 = 10;
// Gets the value of c2
vec3 v = a.c2;</code></pre>

        <h3>Control Flow Statements</h3>

        <p>Again similar to other programming languages, we have control flow statements such as if-else statements,
          switch cases, for loops, and while loops.</p>

        <pre><code class="GLSL">if(condition1) {
    // Do something
} else if(condition2) {
    // Do something else
} else {
    // Do something else
}</code></pre>

        <pre><code class="GLSL">for(int i = 0; i < 10; i++) {
    // Do something
}</code></pre>

        <h3>Functions</h3>

        <p>A function in GLSL, like a variable, must have a type. If a function returns a value, its type must match the
          type of the returned value. However, if a function doesn't return anything, its type must be set to void.</p>

        <pre><code class="GLSL">void funcA() {
    // Stuff to do
}

float funcB() {
    // Stuff to do
    return 1.0;
}</code></pre>

        <p>It is also necessary to specify the types of parameters for a function.</p>

        <pre><code class="GLSL">float func(vec3 v) {
    return v.x;
}</code></pre>

        <p>It's crucial to note that function definitions must be placed outside the main function. More on the main
          function later on.</p>

        <pre><code class="GLSL">float func(vec3 v) {
    return v.x;
}

void main() {
    // Stuff to do
}</code></pre>

        <p>In GLSL, the order of function definition and calling matters. Functions need to be defined before they are
          called.</p>

        <pre><code class="GLSL">// This works
int func() {
    return 5;
}

void main() {
    int five = func();
}</code></pre>

        <pre><code class="GLSL">// This won't work!
void main() {
    int five = func();
}

int func() {
    return 5;
}</code></pre>

        <p>That said, we can actually do something to bypass this rule.</p>

        <p>To do that, we can set what is called the prototype of the function at the top of the code. A prototype is
          essentially the definition of the function without its body. We can then call the function and provide its
          full definition at the bottom of the code base.</p>

        <pre><code class="GLSL">// This is the prototype of the function
vec2 func(float x, float y);

void main() {

    vec2 v = func(2.0, 3.0);

}

vec2 func(float x, float y) {
    return vec2(x, y);
}</code></pre>

        <p>GLSL provides a wealth of built-in functions. For comprehensive documentation on these functions, be sure to
          visit shaderific.com. There, you'll find brief documentation for any built-in function you might need.</p>

        <h3>Storage Qualifiers</h3>

        <p>There are four storage qualifiers. We have already encountered one of them: <b>const</b>. The other three are
          <b>attribute</b>, <b>uniform</b>, and <b>varying</b>.
        </p>

        <p>Attribute and uniform variables, are variables that <b>receive data from the outside of the GLSL code</b>,
          from the
          Javascript side of the application code more precisely.</p>

        <img class="center" style="width: 50%; height: 50%;" src="images/js%20vs%20glsl.png"
          alt="Attribute and Uniform" />

        <p>The difference between the two is that an attribute variable holds data that varies from one vertex to
          another.</p>

        <p>In contrast, time should be passed as a uniform variable because all vertices share the same time value.</p>

        <img class="center" style="width: 50%; height: 50%;" src="images/uniform%20variables.png"
          alt="Uniform Variables" />

        <p>Another difference between attributes and uniforms is that the number of attribute variables allowed is less
          than the number of uniforms.</p>

        <p>Additionally, <b>attribute variables can only be used in the vertex shader</b>, while <b>uniforms are allowed
            in both the vertex and fragment shaders</b>.</p>

        <p>Varying variables, on the other hand, are used to transfer data from the vertex shader to the fragment
          shader. That's it.</p>

        <img class="center" style="width: 50%; height: 50%;" src="images/vertex%20and%20fragment%20shaders.png"
          alt="Varying Variables" />

        <h3>Precision Qualifiers</h3>

        <p>Precision qualifiers are used to optimize resource consumption, specifically memory usage.</p>

        <p>There are three precision qualifiers: lowp, mediump, and highp, with lowp being the least resource-intensive.
          However, it's important to carefully choose the right precision, as a lower precision can sometimes yield
          incorrect results.</p>

        <p>To set the precision of a variable, simply begin its declaration with the desired precision qualifier.</p>

        <pre><code class="GLSL">mediump float f = 5.0;</code></pre>

        <p>Alternatively, you can set a precision for an entire type of variables by using the keyword precision,
          followed by the precision qualifier, and then the type.</p>

        <pre><code class="GLSL">precision highp int;</code></pre>

        <p>WebGL does not offer the use of the Fixed Pipeline, which is a shorthand way of saying that it doesn't give
          you any means of rendering your stuff out of the box.</p>

        <p>What it does offer, however, is the <b>Programmable Pipeline</b>, which is <b>more powerful but which can
            also be more challenging to understand and use</b>.</p>

        <p>In the programmable pipeline, you have to write your own shaders to do the rendering. This is where the
          <b>vertex and fragment shaders</b> come into play.
        </p>

        <p>What you should know about them is that <b>they both run entirely on your graphics card's GPU</b>. This means
          that
          we want to offload all that we can to them, leaving our CPU to do other work. A modern GPU is heavily
          optimised for the functions that shaders require so it's great to be able to use it.</p>

        <h3>Vertex Shaders</h3>

        <p>Take a standard primitive shape, like a sphere. It's made up of vertices, right?</p>

        <p>Every object in 3D, whether it's a point, text, shape, or a 3D model, is composed of a number of vertices.
          The role of a vertex shader is <b>to handle the positioning of each vertex comprising that mesh in the
            scene</b>.</p>

        <p>In and of itself that's quite an interesting process, because we're actually talking about getting a 3D
          position (a vertex with x,y,z) onto, or projected, to a 2D screen. Thankfully for us if we're using something
          like Three.js we will have a shorthand way of setting the gl_Position without things getting too tricky.</p>

        <p>The code of a vertex shader must be enclosed within a function named main. This function will be executed
          once for each vertex that composes the mesh.</p>

        <pre><code class="GLSL">void main() {
    // Vertex shader logic
}</code></pre>

        <p>For example, if we create an object with 20 vertices, the main function of the vertex shader will be executed
          20 times to position each vertex correctly.</p>

        <img class="center" style="width: 50%; height: 50%;"
          src="images/vertex%20shader%20example%201.png"
          alt="Vertex Shader" />

        <p>Within the main function of the vertex shader, we must assign a value to the built-in variable gl_Position,
          which indicates the coordinates of each vertex.</p>

        <pre><code class="GLSL">void main() {
    // Stuff to do
    gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
}</code></pre>

        <p>The value of gl_Position here depends on the 3D library you're using.</p>

        <p>In Three.js the gl_Position value depends on projectionMatrix, modelViewMatrix, and position which are
          Three.js built-in variables.</p>

        <p>The first two variables, projectionMatrix and modelViewMatrix, are related to the camera view and contribute
          to the calculation of the final value passed to gl_Position.</p>

        <p>The third variable, position, stores the initial coordinates of a vertex. It's important to note that this
          may vary if you're working with a different library, such as Babylon.js or Pixi.js.</p>


        <h3>Fragment Shaders</h3>

        <p>Essentially, the role of a fragment shader is to apply colors to vertices and the meshes they form after they
          have been positioned by the vertex shader.</p>

        <img class="center" style="width: 50%; height: 50%;" src="images/fragment%20shader%20example%201.png" alt="Fragment Shader" />

        <p>The colorization process involves several steps, but as a beginner, you just need to know that the fragment
          shader breaks down the mesh, created from the vertices positioned by the vertex shader, into small fragments
          and then colorizes them.</p>

        <p>Similar to the vertex shader, a fragment shader's code must be enclosed within the body of a main function.
          Within this function, we also need to specify a special built-in variable called gl_FragColor.</p>

        <pre><code class="GLSL">void main() {
    // Stuff to do
    gl_FragColor = vec4(1.0, 1.0, 1.0, 1.0);
}</code></pre>

        <p>As its name suggests, <b>gl_FragColor is where the color of one fragment is stored</b>. This vec4 variable
          has four
          components, each representing a color channel: <b>the first value represents the red channel, the second the
            green channel, the third the blue channel, and the fourth the alpha channel</b>.</p>

        <p>Bear in mind that the values of these components range from 0 to 1. Any negative value is treated as 0, and
          any value exceeding 1 is equivalent to 1.</p>

        <h3>Using Shaders in a Three.js Application</h3>

        <p>First, we need a basic Three.js project to implement what we've learned so far. To make things easier, here's
          a basic Three.js scene.</p>

        <p>Every object in a Three.js scene has its own vertex and fragment shader. In fact, even the scene itself has a
          vertex and fragment shader. To apply an effect using shaders, we will replace the existing shaders with the
          custom ones we write.</p>

        <p><b>Shaders are passed to meshes as custom materials</b>. So to apply a specific effect to a mesh, you need to
          create
          a new material with a vertex and fragment shader, then combine this material with the geometry to create the
          mesh.</p>

        <p>To create the custom material, We'll instantiate a new instance of the <b>ShaderMaterial</b> class.</p>

        <p>The ShaderMaterial constructor takes an object as an argument, where we specify the code for our vertex and
          fragment shaders, along with other configurations such as wireframe mode.</p>

        <p>Let's create a plane to do this:</p>

        <pre><code class="JavaScript">const planeGeometry = new THREE.PlaneGeometry(10, 10, 30, 30);
const planeCustomMaterial = new THREE.ShaderMaterial({
    vertexShader: `// Vertex shader code here`,
    fragmentShader: `// Fragment shader code here`,
    wireframe: true
});
const planeMesh = new THREE.Mesh(
    planeGeometry,
    planeCustomMaterial
);
scene.add(planeMesh);</code></pre>

        <p>A good practice is to separate your GLSL logic from your JavaScript code for clarity and organization. You
          can create separate files for the vertex and fragment shaders, or you can place them in the HTML as script
          tags. We'll use the script tag option.</p>

        <p>Let's add our vertex and fragment shaders in our HTML:</p>

        <pre><code class="html">&lt;script id="vertexshader" type="vertex"&gt;
  void main() {
      gl_Position = projectionMatrix * modelViewMatrix * vec4(position, 1.0);
  }
&lt;/script&gt;

&lt;script id="fragmentshader" type="fragment"&gt;
  void main() {
      gl_FragColor = vec4(1.0);
  }
&lt;/script&gt;</code></pre>

        <p>Now we'll pass the values of these script tags to the vertex and fragment shaders properties using their ids.
        </p>

        <pre><code class="JavaScript">const planeCustomMaterial = new THREE.ShaderMaterial({
    vertexShader: document.getElementById('vertexshader').textContent,
    fragmentShader: document.getElementById('fragmentshader').textContent,
    side: THREE.DoubleSide,
    wireframe: true
});</code></pre>

        <p>Now we should see a white wireframe plane in the scene. The vertex shader is responsible for positioning the
          vertices of the
          plane, while the fragment shader colors them.</p>

        <h3>Example 2</h2>

          <p>Let's use the vertex shader to reposition the vertices that form our plane.</p>

          <p>To do this, we simply need to update the value of the position variable, which is of type vec since it
            contains the x, y, and z positions of a vertex. We'll apply the sin() math function here.</p>

          <pre><code class="html">&lt;script id="vertexshader" type="vertex"&gt;
  void main() {
      // sin() is applied to the x, y, z components of the position vector
      gl_Position = projectionMatrix * modelViewMatrix * vec4(sin(position), 1.0);
  }
&lt;/script&gt;</code></pre>

          <p>Let's apply another math function. This time, we'll use tan(), which stands for tangent.</p>

          <pre><code class="html">&lt;script id="vertexshader" type="vertex"&gt;
  void main() {
      gl_Position = projectionMatrix * modelViewMatrix * vec4(tan(position), 1.0);
  }
&lt;/script&gt;</code></pre>

          <p>As you can see, the plane is now distorted. The vertex shader is responsible for this distortion, as it
            modifies the position of each vertex based on the math function applied to it.</p>

          <h3>Example 2.5</h3>

          <p>Let's turn off the wireframe mode.</p>

          <pre><code class="JavaScript">const planeCustomMaterial = new THREE.ShaderMaterial({
    vertexShader: document.getElementById('vertexshader').textContent,
    fragmentShader: document.getElementById('fragmentshader').textContent,
    side: THREE.DoubleSide,
    wireframe: false
});</code></pre>

          <p>With that done, let's change the color of the plane using the fragment shader.</p>

          <p>To do this, we need to set the gl_FragColor variable to a new color.</p>

          <p>the gl_FragColor value is a vec4 whose components represent the RGBA values of one fragment's color.
            Therefore, altering these values will result in a color change. For example, we can set it to red.</p>

          <pre><code class="html">&lt;script id="fragmentshader" type="fragment"&gt;
  void main() {
      gl_FragColor = vec4(1.0, 0.0, 0.0, 1.0);
  }
&lt;/script&gt;</code></pre>

          <h3>Example 3</h2>

            <p>We can also animate the shape by changing the coordinates of the vertices over time. However, unlike
              JavaScript, <b>GLSL does not have a Date class to get the current time</b>.</p>

            <p>So, what we're going to do is pass the time from JavaScript to the vertex shader using a uniform
              variable,
              which is one of the four storage qualifiers I explained earlier, if you remember.

            <p>So, in the JavaScript code, let's create an object where we will set every uniform variable that we'll
              use
              in the vertex and fragment shaders.</p>

            <p>Each property of this object represents a uniform variable. Therefore, each property must include a value
              property that contains the value we want to pass to the shader.</p>

            <p>Next, we need to pass the uniforms object to the uniforms property in the shaderMaterial configuration
              object. Here, we can use the ES6 shorthand since our object has the same name as the property key.</p>

            <pre><code class="JavaScript">const uniforms = {
    u_time: {value: 0.0}
}

const planeGeometry = new THREE.PlaneGeometry(10, 10, 30, 30);
const planeCustomMaterial = new THREE.ShaderMaterial({
    vertexShader: document.getElementById('vertexshader').textContent,
    fragmentShader: document.getElementById('fragmentshader').textContent,
    side: THREE.DoubleSide,
    wireframe: true,
    uniforms
});</code></pre>

            <p>Then, we need to update the value with the elapsed time from the moment the page was loaded in the
              browser.
            </p>

            <p>To do that, we'll create a clock and then update u_time with the value returned by getElapsedTime() from
              the Clock instance.</p>

            <pre><code class="JavaScript">const clock = new THREE.Clock();
function animate() {
    uniforms.u_time.value = clock.getElapsedTime();
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}

animate();</code></pre>

            <p>Keep in mind though, that unlike delta time in JavaScript, getElapsedTime() returns the time in seconds.
              This also means that unlike delta time, <b>elasped time is not frame rate independent</b>.</p>

            <p>Now, in the vertex shader, we need to create the uniform variable whose name must be the same as the
              property name in the uniforms object.</p>

            <pre><code class="html">&lt;script id="vertexshader" type="vertex"&gt;
  uniform float u_time;
  void main() {
      gl_Position = projectionMatrix * modelViewMatrix * vec4(sin(position), 1.0);
  }
&lt;/script&gt;</code></pre>

            <p>Finally, we can use the u_time variable to animate the plane. For example, we can apply the sin()
              function
              to it.</p>

            <pre><code class="html">&lt;script id="vertexshader" type="vertex"&gt;
  uniform float u_time;
  void main() {
      // sin() is applied to the x, y, z components of the position vector
      gl_Position = projectionMatrix * modelViewMatrix * vec4(sin(position + u_time), 1.0);
  }
&lt;/script&gt;</code></pre>

            <p>Now, the plane will be animated based on the time passed to the vertex shader.</p>

            <p>In the fragment shader, we can also use the u_time variable to animate the color of the plane.</p>

            <pre><code class="html">&lt;script id="fragmentshader" type="fragment"&gt;
  uniform float u_time;
  void main() {
      // sin() is applied to the x, y, z components of the position vector
      gl_FragColor = vec4(sin(u_time), 1.0, 1.0, 1.0);
  }
&lt;/script&gt;</code></pre>

            <p>Now, the color of the plane will be animated based on the time passed to the fragment shader.</p>

            <p>We can also change the shape.</p>

            <h3>Example 4</h3>

            <p>We can use the u_time variable to change the x position of the vertices, for example.</p>

            <p>Let's create a variable whose value is calculated using the time, the x position, and the y position of
              each plane vertex. Then, we can create another vec3 variable that represents the new coordinates of a
              vertex.</p>

            <p>Essentially, we're changing the x position while preserving the y and z positions.

            <p>Finally, we can replace the initial position with the new position.</p>

            <pre><code class="html">&lt;script id="fragmentshader" type="fragment"&gt;
  uniform float u_time;
  void main() {
      float newX = sin(position.x * u_time) * sin(position.y * u_time);
      vec3 newPosition = vec3(newX, position.y, position.z);
      gl_Position = projectionMatrix * modelViewMatrix * vec4(newPosition, 1.0);
  }
&lt;/script&gt;</code></pre>

            <p>And there we go! You can see how applying a short, random math equation to the x position of a plane's
              vertices made such a big change to its shape.</p>




    </section>








    </article>
    <br />
    </section>













    <hr />
    <section class="main-section" id="Reference">
      <br />
      <header><b>Reference</b></header>
      <article>
        <p>Documentation on this page is taken from the following:</p>
        <ul>
          <li>
            Eck, D. J. (2023). Introduction to computer graphics (Version 1.4). Hobart and William Smith Colleges.
            Retrieved from <a href="https://math.hws.edu/graphicsbook/">https://math.hws.edu/graphicsbook/</a>
          </li>
          <li>
            <a href="https://www.tutorialspoint.com/computer_graphics/index.htm" target="_blank">Tutorials Point</a>
          </li>
          <li>
            <a href="https://learn.leighcotnoir.com/artspeak/elements-color/hue-value-saturation/"
              target="_blank">Learn. (Leigh Cotnoir's art and design courses)</a>
          </li>
          <br />

        </ul>
      </article>
    </section>
  </main>
  <script>
    var dropdown = document.getElementsByClassName("dropdown-btn");
    var i;

    for (i = 0; i < dropdown.length; i++) {
      dropdown[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var dropdownContent = this.nextElementSibling;
        if (dropdownContent.style.display === "block") {
          dropdownContent.style.display = "none";
        } else {
          dropdownContent.style.display = "block";
        }
      });
    }
  </script>
  <script>
    // Function to close the dropdown menu
    function closeDropdown() {
      var dropdown = document.querySelector(".dropdown-container");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 2 dropdown menu
    function closeDropdown2() {
      var dropdown = document.querySelector(".container2");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 3 dropdown menu
    function closeDropdown3() {
      var dropdown = document.querySelector(".container3");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown5() {
      var dropdown = document.querySelector(".container5");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown7() {
      var dropdown = document.querySelector(".container7");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 9 dropdown menu
    function closeDropdown9() {
      var dropdown = document.querySelector(".container9");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 4 and chapter 6 dropdown menu
    function closeDropdown46() {
      var dropdown = document.querySelector(".container46");
      dropdown.style.display = "none";
    }

    // Function to close the chapter 8 dropdown menu
    function closeDropdown8() {
      var dropdown = document.querySelector(".container8");
      dropdown.style.display = "none";
    }

    // Function to close the Number Systems dropdown menu
    function closeDropdownNS() {
      var dropdown = document.querySelector(".containerNS");
      dropdown.style.display = "none";
    }
  </script>
  <script>
    // Check for saved dark mode preference
    if (localStorage.getItem('darkMode') === 'enabled') {
      document.body.classList.add('dark-mode');
      document.querySelector('.change').textContent = 'ON';
    }

    // Dark mode toggle functionality
    document.querySelector('.mode').addEventListener('click', function () {
      document.body.classList.toggle('dark-mode');

      // Get the change element
      const changeModeText = document.querySelector('.change');

      // Update the text based on dark mode status
      if (document.body.classList.contains('dark-mode')) {
        changeModeText.textContent = 'ON'; // Change the text to 'ON'
        localStorage.setItem('darkMode', 'enabled'); // Save preference
      } else {
        changeModeText.textContent = 'OFF'; // Change the text to 'OFF'
        localStorage.setItem('darkMode', 'disabled'); // Save preference
      }
    });

  </script>
  <script>
    function toggleMode() {
      const lightModeLink = document.getElementById('light-mode');
      const darkModeLink = document.getElementById('dark-mode');
      const modeText = document.querySelector('.change');

      if (lightModeLink.disabled) {
        lightModeLink.disabled = false;
        darkModeLink.disabled = true;
        modeText.textContent = "OFF"; // Update the mode text to show Dark mode is off
      } else {
        lightModeLink.disabled = true;
        darkModeLink.disabled = false;
        modeText.textContent = "ON"; // Update the mode text to show Dark mode is on
      }
    }

    // Call highlight.js to apply syntax highlighting
    document.addEventListener('DOMContentLoaded', () => {
      hljs.highlightAll();
    });
  </script>
</body>

</html>
