<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />

  <!-- Default CSS style for dark mode -->
  <link id="dark-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/base16/bright.css" />

  <!-- CSS style for light mode -->
  <link id="light-mode" rel="stylesheet"
    href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/felipec.css" disabled />

  <link rel="stylesheet" href="../3620style.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

  <!-- and it's easy to individually load additional languages -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/go.min.js"></script>

  <script>
    hljs.highlightAll();
  </script>
  <title>CISC 3620</title>
</head>

<body>
  <nav id="navbar">
    <header class="header-link">
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" style="color: inherit; text-decoration: none;"
        class="header-text">CISC 3620</a>
    </header>
    <div class="mode">
      Dark mode:
      <span class="change" onclick="toggleMode()">OFF</span>
    </div>
    <hr />

    <ul>
      <!--Chapter 1 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">Introduction</a>
        <div class="dropdown-container">
          <a href="#What_is_Computer_Graphics?" onclick="closeDropdown()">What is Computer Graphics?</a>
          <a href="#Types_of_Computer_Graphics" onclick="closeDropdown()">Types of Computer Graphics</a>
          <a href="#Applications_of_Computer_Graphics" onclick="closeDropdown()">Applications of Computer Graphics</a>
          <a href="#The_Computer_Graphics_System" onclick="closeDropdown()">The Computer Graphics System</a>
        </div>
      </li>
      <!--Chapter 2 Dropdown-->
      <li>
        <a class="dropdown-btn" style="font-size: 20px">2D Computer Graphics</a>
        <div class="dropdown-container container2">
          <a href="#Types_of_2D_Graphics" onclick="closeDropdown2()">Types of 2D Graphics</a>
          <a href="#Pixels_and_Coordinate_Systems" onclick="closeDropdown2()">Pixels and Coordinate Systems</a>
          <a href="#Color_Models" onclick="closeDropdown2()">Color Models</a>
          <a href="#Shapes" onclick="closeDropdown2()">Shapes</a>
          <a href="#JavaScript" onclick="closeDropdown2()">Basic JavaScript</a>
          <a href="#Intro_to_HTML5_Canvas" onclick="closeDropdown2()">Intro to HTML5 Canvas</a>
          <a href="#Polygons_and_Curves" onclick="closeDropdown2()">Polygons and Curves</a>
          <a href="#Mouse_Events" onclick="closeDropdown2()">Mouse Events</a>
          <a href="#Additional_Events" onclick="closeDropdown2()">Additional Events</a>
          <a href="#Transforms" onclick="closeDropdown2()">Transforms</a>
        </div>
      </li>

      <li>
        <a class="dropdown-btn" style="font-size: 20px">3D Computer Graphics</a>
        <div class="dropdown-container container2">
          <a href="#Linear_Algebra" onclick="closeDropdown3()">Linear Algebra</a>
          <a href="#Intro_to_3D_Graphics" onclick="closeDropdown3()">Intro to 3D Graphics</a>
          <a href="#Graphic_APIs" onclick="closeDropdown3()">Graphic APIs</a>
          <a href="#Intro_to_Three.js" onclick="closeDropdown3()">Intro to Three.js</a>
          <a href="#Lights_and_Interactivity" onclick="closeDropdown3()">Lights and Interactivity</a>
          <a href="#Shadows" onclick="closeDropdown3()">Shadows</a>
          <a href="#Textures" onclick="closeDropdown3()">Textures</a>
          <a href="#A_More_Complex_Scene" onclick="closeDropdown3()">A More Complex Scene</a>
          <!--<a href="#Linear_Algebra" onclick="closeDropdown2()">Linear Algebra</a>
          <a href="#Variable_Scope" onclick="closeDropdown2()">Variable Scope</a>
          <a href="#Global_Variables" onclick="closeDropdown2()">Global Variables</a>
          <a href="#Operators" onclick="closeDropdown2()">Operators</a>
          <a href="#Floating-Point_Numbers" onclick="closeDropdown2()">Floating-Point Numbers</a>
          <a href="#Math_Methods" onclick="closeDropdown2()">Math Methods</a>
          <a href="#Error_Types" onclick="closeDropdown2()">Error Types</a>-->
        </div>
      </li>

      <!--Chapter 3 Dropdown-->
      <!--<li>
        <a class="dropdown-btn" href="#Input_and_Output" style="font-size: 20px">Input and Output</a>
        <div class="dropdown-container container3">
          <a href="#System_Class" onclick="closeDropdown3()">System Class</a>
          <a href="#Data_Types" onclick="closeDropdown3()">Data Types</a>
          <a href="#Reading_Input" onclick="closeDropdown3()">Reading Input</a>
          <a href="#Literals_and_Constants" onclick="closeDropdown3()">Literals and Constants</a>
          <a href="#Putting_it_all_Together" onclick="closeDropdown3()">Putting it all Together</a>
          <a href="#Program_Structure" onclick="closeDropdown3()">Program Structure</a>
          <a href="#Using_Files" onclick="closeDropdown3()">Using Files</a>
        </div>
      </li>-->

      <li><a href="#Reference" style="font-size: 20px">Reference</a></li>
      <!-- add more links here -->
      <a href="https://amaraauguste.github.io/courses/cisc3620.html" class="previous backbutton"
        style="font-size: 20px;">&laquo; Back</a>
      <br /><br /><br /><br /><br /><br /><br /><br /><br /><br />
    </ul>
  </nav>

  <main id="main-doc">
    <!-- WEEK 1 NOTES -->
    <!-- WEEK 1 DAY 1-->


    <section class="main-section" id="What_is_Computer_Graphics?">
      <br />
      <header><b>What is Computer Graphics?</b></header>
      <article>
        <p>The term "Computer Graphics" is concerned with all aspects of producing pictures or images using a computer.
        </p>
        <p>It encompasses the <b>creation</b>, <b>manipulation</b>, and <b>representation of images and animations</b>
          on computers.</p>
      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Types_of_Computer_Graphics">
      <br />
      <header><b>Types of Computer Graphics</b></header>
      <article>
        <p>Computer graphics can be broadly classified into two types: two-dimensional (2D) and three-dimensional (3D)
          graphics.</p>
        <p><b>2D Graphics:</b> are digital images that are computer-based. </p>
        <p>They include 2D geometric models, such as image compositions,
          pixel art, digital art, photographs, and text.</p>
        <p>2D graphics or computer generated images are used everyday on traditional printing and drawing. </p>
        <p><b>3D Graphics:</b> are graphics that use 3D representation of geometric data.</p>
        <p>This geometric data is then manipulated by computers via 3D computer graphics software in order to customize
          their display,
          movements, and appearance.</p>
        <p>3D computer graphics are often referred to as 3d models. A 3d model is a mathematical representation of
          geometric data that is contained in a data file. 3D models, can be used for real-time 3D viewing in
          animations, videos,
          movies, training, simulations, architectural visualizations or for display as 2D rendered images (2D renders)
        </p>
      </article>
      <br />
    </section>
    <section class="main-section" id="Applications_of_Computer_Graphics">
      <br />
      <header><b>Applications of Computer Graphics</b></header>
      <article>
        <p>The development of computer graphics has been driven both by the needs of the user
          community and by advances in hardware and software. The applications of computer
          graphics are many and varied; we can, however, divide them into <b>four</b> major areas:</p>
        <ol>
          <li>Display of information</li>
          <li>Design</li>
          <li>Simulation and animation</li>
          <li>User interfaces</li>
        </ol>
        <p>Although many applications span two or more of these areas, the development of the
          field was based on separate work in each.</p>
        <h2>1. Display of Information:</h2>
        <p>One of the most common uses of computer graphics is to display information in a
          pictorial or graphical form. This includes the generation of charts, graphs, and maps,
          as well as the visualization of scientific data. For example, medical imaging techniques
          such as MRI and CT scans use computer graphics to create detailed images of the human
          body.</p>
        <h2>2. Design:</h2>
        <p>Computer graphics is widely used in design and modeling applications, such as
          computer-aided design (CAD) for engineering and architectural design. It allows
          designers to create and manipulate 3D models of objects and structures, visualize
          designs from different angles, and simulate how they will look and function in the real
          world.</p>
        <h2>3. Simulation and Animation:</h2>
        <p>Computer graphics is also used to create realistic simulations and animations for
          various purposes, including entertainment, training, and scientific visualization. This
          includes the creation of 3D animations for movies and video games, as well as
          simulations for training pilots, surgeons, and other professionals.</p>
        <h2>4. User Interfaces:</h2>
        <p>Computer graphics plays a crucial role in the design of user interfaces for
          software applications. It allows developers to create visually appealing and intuitive
          interfaces that enhance the user experience. This includes the design of icons, buttons,
          menus, and other graphical elements that users interact with.</p>


      </article>
      <br />
    </section>
    <section class="main-section" id="The_Computer_Graphics_System">
      <br />
      <header><b>The Computer Graphics System</b></header>
      <article>
        <p>A computer graphics system is a computer system; as such, it must have all the
          components of a general-purpose computer system. There are six major elements in our system:</p>
        <ol>
          <li>Input devices</li>
          <li>Central Processing Unit</li>
          <li>Graphics Processing Unit</li>
          <li>Memory</li>
          <li>Frame buffer</li>
          <li>Output devices</li>
        </ol>

        <p>These components are shown in the figure below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20computer%20graphics%20system.png?raw=true"
          alt="The computer graphics system">

        <p>This model is general enough to include workstations and personal computers,
          interactive game systems, mobile phones, GPS systems, and sophisticated imagegeneration systems.
          Although most of the components are present in a standard computer, it is the way each element is specialized
          for
          computer graphics that characterizes this diagram as a portrait of a graphics system.</p>

        <h2>Input Devices</h2>

        <p>Input devices are used to capture data (and images) from the real world and convert them into a form that can
          be processed by the computer.</p>

        <p>Most graphics systems provide a keyboard and at least one other input device. The
          most common input devices are the mouse, the joystick, and the data tablet. Each
          provides positional information to the system, and each usually is equipped with one
          or more buttons to provide signals to the processor. Often called pointing devices,
          these devices allow a user to indicate a particular location on the display.</p>

        <p>Modern systems, such as game consoles, provide a much richer set of input
          devices, with new devices appearing almost weekly. In addition, there are devices
          which provide three- (and more) dimensional input. Consequently, we want to provide a
          flexible model for incorporating the input from such devices into our graphics
          programs</p>

        <p>We can think about input devices in two distinct ways. The obvious one is to look
          at them as <b>physical devices</b>, such as a keyboard or a mouse, and to discuss how they
          work. Certainly, we need to know something about the physical properties of our input devices,
          so such a discussion is necessary if we are to obtain a full understanding
          of input. However, from the perspective of an application programmer, we should not
          need to know the details of a particular physical device to write an application program.</p>

        <p>Rather, we prefer to treat input devices as <b>logical devices</b> whose properties are
          specified in terms of what they do from the perspective of the application program. A
          logical device is characterized by its high-level interface with the user program rather
          than by its physical characteristics. </p>

        <p>Logical devices are familiar to all writers of highlevel programs.
          For example, data input and output in Java are done through classes
          such as System.out for output, PrintWriter for writing to files, and Scanner for input, whose methods use the
          standard Java data types. When we output a string using System.out.println or PrintWriter.println, the
          physical device
          on which the output appears could be a printer, a terminal, or a disk file. This output could even be the
          input to another
          program. The details of the format required by the destination device are of minor
          concern to the writer of the application program.</p>

        <p>In computer graphics, the use of logical devices is slightly more complex because
          the forms that input can take are more varied than the strings of bits or characters
          to which we are usually restricted in nongraphical applications. For example, we can
          use the mouse—a physical device—either to select a location on the screen of our
          CRT or to indicate which item in a menu we wish to select. In the first case, an x, y
          pair (in some coordinate system) is returned to the user program; in the second, the
          application program may receive an integer as the identifier of an entry in the menu.
          The separation of physical from logical devices allows us to use the same physical
          devices in multiple markedly different logical ways. It also allows the same program
          to work, without modification, if the mouse is replaced by another physical device,
          such as a data tablet or trackball.</p>

        <h3>Physical Input Devices</h3>

        <p>From the physical perspective, each input device has properties that make it more
          suitable for certain tasks than for others. We take the view used in most of the workstation
          literature that there are <b>two</b> primary types of physical devices: <b>pointing devices</b>
          and <b>keyboard devices</b></p>

        <p>The pointing device allows the user to indicate a position on
          the screen and almost always incorporates one or more buttons to allow the user to
          send signals or interrupts to the computer.</p>

        <p>The keyboard device is almost always a physical keyboard but can be generalized to include any device that
          returns character
          codes. We use the American Standard Code for Information Interchange (ASCII) in
          our examples. ASCII assigns a single unsigned byte to each character. Nothing we do
          restricts us to this particular choice, other than that ASCII is the prevailing code used.
          Note, however, that other codes, especially those used for Internet applications, use
          multiple bytes for each character, thus allowing for a much richer set of supported
          characters.</p>

        <p>The mouse and trackball are similar in use and often
          in construction as well. A typical mechanical mouse when turned over looks like a
          trackball. In both devices, the motion of the ball is converted to signals sent back to
          the computer by pairs of encoders inside the device that are turned by the motion of
          the ball. The encoders measure motion in two orthogonal directions</p>

        <p>There are many variants of these devices. Some use optical detectors rather than
          mechanical detectors to measure motion. Small trackballs are popular with portable
          computers because they can be incorporated directly into the keyboard. There are
          also various pressure-sensitive devices used in keyboards that perform similar functions
          to the mouse and trackball but that do not move; their encoders measure the
          pressure exerted on a small knob that often is located between two keys in the middle
          of the keyboard</p>

        <p>We can view the output of the mouse or trackball as two independent values
          provided by the device. These values can be considered as positions and converted—
          either within the graphics system or by the user program—to a two-dimensional
          location in a convenient coordinate system. If it is configured in this manner, we can
          use the device to position a marker (cursor) automatically on the display; however,
          we rarely use these devices in this direct manner.</p>

        <p>It is not necessary that the output of the mouse or trackball encoders be interpreted as a position.
          Instead, either the device driver or a user program can interpret
          the information from the encoder as two independent velocities. The computer can
          then integrate these values to obtain a two-dimensional position.</p>

        <p>Thus, as a mouse moves across a surface, the integrals of the velocities yield x, y
          values that can be converted to indicate the position for a cursor on the screen, as shown below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/cursor%20positioning.png?raw=true"
          alt="cursor positioning">

        <p>By interpreting the distance traveled by the ball as a velocity, we can use the device
          as a variable-sensitivity input device. Small deviations from rest cause slow or small
          changes; large deviations cause rapid large changes.</p>

        <p>With either device, if the ball does not rotate, then there is no change in the integrals and a cursor
          tracking the position of the mouse will not move. </p>

        <p>In this mode, these devices are <b>relative-positioning</b> devices because changes in the position of
          the ball yield a position in the user program; the absolute location of the ball (or the mouse) is not used by
          the application
          program.</p>

        <p>Relative positioning, as provided by a mouse or trackball, is <b>not always desirable</b>.</p>

        <p>In particular, these devices are not suitable for an operation such as tracing a diagram.
          If, while the user is attempting to follow a curve on the screen with a mouse, she
          lifts and moves the mouse, the absolute position on the curve being traced is lost.
        </p>

        <p><b>Data tablets</b> provide <b>absolute positioning</b>. A typical data tablet has rows
          and columns of wires embedded under its surface. The position of the stylus is
          determined through electromagnetic interactions between signals traveling through
          the wires and sensors in the stylus. Touch-sensitive transparent screens that can be
          placed over the face of a CRT have many of the same properties as the data tablet.
          Small, rectangular, pressure-sensitive touchpads are embedded in the keyboards of
          many portable computers. These touchpads can be configured as either relative- or
          absolute-positioning devices.</p>

        <h3>Logical Devices</h3>

        <p>Two major characteristics describe the logical behavior
          of an input device: (1) the measurements that the device returns to the user program
          and (2) the time when the device returns those measurements.</p>

        <p>The logical <b>string</b> device in Java is similar to using character input through Scanner or
          BufferedReader.
          A physical keyboard will return a string of characters to an application program; the same string might be
          provided from a file, or the user may see a virtual keyboard displayed on the output and use a pointing device
          to generate the
          string of characters. Logically, all three methods are examples of a string device, and application code for
          using such input can be
          the same regardless of which physical device is used.</p>

        <p>The physical pointing device can be used in a variety of logical ways. As a <b>locator</b>
          it can provide a position to the application in either a device-independent coordinate
          system, such as world coordinates, as in OpenGL, or in screen coordinates, which the
          application can then transform to another coordinate system. A logical <b>pick</b> device
          returns the identifier of an object on the display to the application program. It is
          usually implemented with the same physical device as a locator but has a separate
          software interface to the user program.</p>

        <p>A <b>widget</b> is a graphical interactive device, provided by either the window system
          or a toolkit. Typical widgets include menus, scrollbars, and graphical buttons. Most
          widgets are implemented as special types of windows. Widgets can be used to provide
          additional types of logical devices. For example, a menu provides one of a number of
          <b>choices</b> as may a row of graphical buttons. A logical <b>valuator</b> provides analog input
          to the user program, usually through a widget such as a slidebar, although the same
          logical input could be provided by a user typing numbers into a physical keyboard.
        </p>

        <h2>The CPU and The GPU</h2>
        <p>In a simple system, there may be only one processor, the central processing unit
          (CPU) of the system, which must do both the normal processing and the graphical processing.
          The main graphical function of the processor is to take specifications of graphical primitives (such as lines,
          circles, and polygons)
          generated by application programs and to assign values to the pixels in the frame buffer that best represent
          these entities.</p>

        <p> For example, a triangle is specified by its three vertices, but to display
          its outline by the three line segments connecting the vertices, the graphics system
          must generate a set of pixels that appear as line segments to the viewer.
          The conversion of geometric entities to pixel colors and locations in the frame buffer is known
          as <b>rasterization</b>, or <b>scan conversion</b>.</p>

        <p> In early graphics systems, the frame buffer was
          part of the standard memory that could be directly addressed by the CPU. Today,
          virtually all graphics systems are characterized by special-purpose <b>graphics processing
            units (GPUs)</b>, custom-tailored to carry out specific graphics functions. The GPU can
          be either on the mother board of the system or on a graphics card. The frame buffer
          is accessed through the graphics processing unit and usually is on the same circuit
          board as the GPU.</p>

        <p>GPUs have evolved to where they are as complex or even more complex than
          CPUs. They are characterized by both special-purpose modules geared toward graphical operations
          and a high degree of parallelism—recent GPUs contain over 100 processing units, each of which is user
          programmable. GPUs are so powerful that they can often be used as mini supercomputers for general purpose
          computing.</p>


        <h2>Output Devices</h2>
        <p>Until recently, the dominant type of display (or monitor) was the <b>cathode-ray tube
            (CRT)</b>. A simplified picture of a CRT is shown below:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20cathode%20ray%20tube.png?raw=true"
          alt="the cathode-ray tube">

        <p>When electrons strike the phosphor coating on the tube, light is emitted. The direction of the beam is
          controlled
          by two pairs of deflection plates. The output of the computer is converted, by digitalto-analog converters,
          to voltages across the x and y deflection plates. Light appears
          on the surface of the CRT when a sufficiently intense beam of electrons is directed at
          the phosphor.</p>

        <p>If the voltages steering the beam change at a constant rate, the beam will trace
          a straight line, visible to a viewer. Such a device is known as the <b>random-scan</b>,
          <b>calligraphic</b>, or <b>vector</b> CRT, because the beam can be moved directly from any
          position to any other position. If intensity of the beam is turned off, the beam can
          be moved to a new position without changing any visible display. This configuration
          was the basis of early graphics systems that predated the present raster technology
        </p>

        <p>A typical CRT will emit light for only a short time—usually, a few milliseconds—
          after the phosphor is excited by the electron beam. For a human to see a steady,
          flicker-free image on most CRT displays, the same path must be retraced, or <b>refreshed</b>,
          by the beam at a sufficiently high rate, the <b>refresh rate</b>. In older systems,
          the refresh rate is determined by the frequency of the power system, 60 cycles per
          second or 60 Hertz (Hz) in the United States and 50 Hz in much of the rest of the world.
          Modern displays are no longer coupled to these low frequencies and operate at rates
          up to about 85 Hz.</p>

        <p>In a raster system, the graphics system takes pixels from the frame buffer and
          displays them as points on the surface of the display in one of two fundamental
          ways.</p>

        <p>In a <b>noninterlaced</b> system, the pixels are displayed row by row, or scan line
          by scan line, at the refresh rate.</p>

        <p>In an <b>interlaced</b> display, odd rows and even rows are refreshed alternately.
          Interlaced displays are used in commercial television. In an interlaced display operating at 60 Hz,
          the screen is redrawn in its entirety only 30 times per second, although the visual system is tricked
          into thinking the refresh rate is 60 Hz rather than 30 Hz. Viewers located near the screen, however, can tell
          the difference between the interlaced and noninterlaced displays. Noninterlaced displays
          are becoming more widespread, even though these displays process pixels at twice the
          rate of the interlaced display</p>

        <p>Color CRTs have three different colored phosphors (red, green, and blue), arranged in small groups.
          One common style arranges the phosphors in triangular groups called <b>triads</b>, each triad consisting of
          three phosphors,
          one of each primary. Most color CRTs have three electron beams, corresponding to the three types of phosphors.
        </p>
        <p>In the shadow-mask CRT, a metal screen with small holes—the
          shadow mask—ensures that an electron beam excites only phosphors of the proper
          color:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shadowmask%20CRT.png?raw=true"
          alt="Shadowmask CRT">

        <p>Although CRTs are still common display devices, they are rapidly being replaced
          by flat-screen technologies. Flat-panel monitors are inherently raster based. Although
          there are multiple technologies available, including light-emitting diodes (LEDs),
          liquid-crystal displays (LCDs), and plasma panels, all use a two-dimensional grid
          to address individual light-emitting elements.</p>

        <p>The following shows a generic flat-panel monitor:</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/flat%20panel%20display.png?raw=true"
          alt="Flat panel display">

        <p>The two outside plates each contain parallel grids of wires that are oriented
          perpendicular to each other. By sending electrical signals to the proper wire in each
          grid, the electrical field at a location, determined by the intersection of two wires, can
          be made strong enough to control the corresponding element in the middle plate.
          The middle plate in an LED panel contains light-emitting diodes that can be turned
          on and off by the electrical signals sent to the grid. In an LCD display, the electrical
          field controls the polarization of the liquid crystals in the middle panel, thus turning
          on and off the light passing through the panel. A plasma panel uses the voltages on the
          grids to energize gases embedded between the glass panels holding the grids. The
          energized gas becomes a glowing plasma.</p>

        <p>Most projection systems are also raster devices. These systems use a variety of
          technologies, including CRTs and digital light projection (DLP). From a user perspective,
          they act as standard monitors with similar resolutions and precisions. Hard-copy
          devices, such as printers and plotters, are also raster based but cannot be refreshed.</p>


        <h2></h2>

      </article>
      <br />
    </section>

    <section class="main-section" id="Types_of_2D_Graphics">
      <br />
      <header><b>Types of 2D Graphics</b></header>
      <article>

        <p>There are two kinds of 2D computer graphics: raster graphics and vector graphics.</p>

        <h2>1. Raster Graphics</h2>

        <p><b>Raster graphics</b>, also known as <b>bitmap graphics</b>, are images that are made up of a grid of
          <b>pixels</b>.
        </p>

        <p>The pixels are small enough that they are not easy to see individually. In fact, for many very
          high-resolution displays, they
          become essentially invisible. Each pixel in the grid has a specific color value, and together they form the
          complete image.</p>

        <p>Modern screens typically use <b>24-bit color</b>, where each color is defined by three 8-bit numbers
          representing the levels of red, green, and blue. These three primary colors combine to create any color
          displayed on the
          screen. Such systems are known as <b>true-color</b>, <b>RGB-color</b>, or <b>full-color systems</b> because
          each
          pixel's color is determined by the combination of red, green, and blue values.</p>

        <p> Other formats are possible, such as <b>grayscale</b>, where each pixel is some shade of gray and the pixel
          color is given by one number that specifies the level of gray on a black-to-white scale. Typically, 256 shades
          of gray are used.</p>

        <p>Early computer screens used <b>indexed color</b>, where only a small set of colors, usually 16 or
          256, could be displayed. For an indexed color display, there is a numbered list of possible colors,
          and the color of a pixel is specified by an integer giving the position of the color in the list.</p>

        <p>In any case, the color values for all the pixels on the screen are stored in a large block of
          memory known as a <b>frame buffer</b>. Changing the image on the screen requires changing color
          values that are stored in the frame buffer. The screen is redrawn many times per second, so
          that almost immediately after the color values are changed in the frame buffer, the colors of
          the pixels on the screen will be changed to match, and the displayed image will change.</p>

        <p>In a very simple system, the frame buffer holds only the colored pixels that are
          displayed on the screen. In most systems, the frame buffer holds far more information,
          such as depth information needed for creating images from three-dimensional
          data. In these systems, the frame buffer comprises multiple buffers, one or more of
          which are color buffers that hold the colored pixels that are displayed. For now, we
          can use the terms frame buffer and color buffer synonymously without confusion.</p>

        <p>A computer screen used in this way is the basic model of <b>raster graphics</b>. The term
          "raster" technically refers to the mechanism used on older vacuum tube computer monitors:
          An electron beam would move along the rows of pixels, making them glow. The beam was
          moved across the screen by powerful magnets that would deflect the path of the electrons. The
          stronger the beam, the brighter the glow of the pixel, so the brightness of the pixels could be
          controlled by modulating the intensity of the electron beam. The color values stored in the
          frame buffer were used to determine the intensity of the electron beam. (For a color screen,
          each pixel had a red dot, a green dot, and a blue dot, which were separately illuminated by the
          beam.)</p>

        <p>Virtually all modern graphics systems are raster based. The image we see on the output device is an array—the
          <b>raster</b>—of
          picture elements, or pixels, produced by the graphics system.
        </p>

        <p>Raster graphics are best suited for representing complex images with many colors and gradients, such as
          photographs and detailed
          illustrations.</p>

        <h2>2. Vector Graphics</h2>

        <p>Although images on the computer screen are represented using pixels, specifying individual
          pixel colors is not always the best way to create an image. Another way is to specify the basic
          geometric objects that it contains, shapes such as lines, circles, triangles, and rectangles. This
          is the idea that defines <b>vector graphics</b>: Represent an image as a list of the geometric shapes
          that it contains.</p>

        <p>To make things more interesting, the shapes can have attributes, such as
          the thickness of a line or the color that fills a rectangle. Of course, not every image can be
          composed from simple geometric shapes. This approach certainly wouldn't work for a picture
          of a beautiful sunset (or for most any other photographic image). However, it works well for
          many types of images, such as architectural blueprints and scientific illustrations.</p>

        <p>In fact, early in the history of computing, vector graphics was even used directly on computer
          screens. When the first graphical computer displays were developed, raster displays were too
          slow and expensive to be practical. Fortunately, it was possible to use vacuum tube technology
          in another way: The electron beam could be made to directly draw a line on the screen, simply
          by sweeping the beam along that line. A vector graphics display would store a display list
          of lines that should appear on the screen. Since a point on the screen would glow only very
          briefly after being illuminated by the electron beam, the graphics display would go through the
          display list over and over, continually redrawing all the lines on the list. To change the image,
          it would only be necessary to change the contents of the display list. Of course, if the display
          list became too long, the image would start to flicker because a line would have a chance to
          visibly fade before its next turn to be redrawn</p>

        <p>But here is the point: For an image that can be specified as a reasonably small number of
          geometric shapes, the amount of information needed to represent the image is much smaller
          using a vector representation than using a raster representation. Consider an image made up
          of one thousand line segments. For a vector representation of the image, We only need to store
          the coordinates of two thousand points, the endpoints of the lines. <b>This would take up only a
            few kilobytes of memory. To store the image in a frame buffer for a raster display would require
            much more memory.</b> Similarly, a vector display could draw the lines on the screen more quickly
          than a raster display could copy the same image from the frame buffer to the screen. (As soon
          as raster displays became fast and inexpensive, however, they quickly displaced vector displays
          because of their ability to display all types of images reasonably well.)
        </p>

        <p>Unlike raster graphics, vector graphics are resolution-independent, meaning that they can be scaled to any
          size without losing quality.
          This is because instead of pixels, vector graphics use points, lines, and curves to represent elements.
          This allows for scalable graphics that can be resized without loss of quality. </p>

        <p>Vector graphics are best suited for representing simple images with solid colors and sharp edges, such as
          logos and icons, and
          widely used in graphic design, architectural design, and illustration industries.</p>

        <p>In summary, raster graphics are made up of pixels and are best suited for complex images with many colors,
          while vector graphics
          are made up of lines and curves and are best suited for simple images with solid colors.</p>

        <h2>So What's The Difference?</h2>

        <p>The divide between raster graphics and vector graphics persists in several areas of computer
          graphics.</p>

        <p> For example, it can be seen in a division between two categories of programs that
          can be used to create images: <b>painting programs</b> and <b>drawing programs</b></p>

        <h2>Painting Programs</h2>

        <p>In a painting program, the image is represented as a grid of pixels, and the user creates an image by
          assigning colors to pixels. This might be done by using a "drawing tool" that acts like a painter's brush,
          or even by tools that draw geometric shapes such as lines or rectangles. But the point in a
          painting program is to color the individual pixels, and it is only the pixel colors that are saved.
          To make this clearer, suppose that We use a painting program to draw a house, then draw a
          tree in front of the house. If We then erase the tree, We'll only reveal a blank background, not
          a house. In fact, the image never really contained a "house" at all—only individually colored
          pixels that the viewer might perceive as making up a picture of a house</p>

        <h2>Drawing Programs</h2>

        <p>In a drawing program, the user creates an image by adding geometric shapes, and the image
          is represented as a list of those shapes. If We place a house shape (or collection of shapes making
          up a house) in the image, and We then place a tree shape on top of the house, the house is
          still there, since it is stored in the list of shapes that the image contains. If We delete the tree,
          the house will still be in the image, just as it was before We added the tree. Furthermore, We
          should be able to select one of the shapes in the image and move it or change its size, so drawing
          programs offer a rich set of editing operations that are not possible in painting programs. (The
          reverse, however, is also true.)</p>

        <p>A practical program for image creation and editing might <b>combine elements of painting and
            drawing</b>, although one or the other is usually dominant.</p>

        <p>For example, a drawing program might allow the user to include a raster-type image, treating it as one shape.
          A painting program
          might let the user create “layers,” which are separate images that can be layered one on top of
          another to create the final image. The layers can then be manipulated much like the shapes in
          a drawing program (so that We could keep both our house and our tree in separate layers,
          even if in the image of the house is in back of the tree).</p>

        <p>Two well-known graphics programs are <b>Adobe Photoshop</b> and <b>Adobe Illustrator</b>. Photoshop
          is in the category of painting programs, while Illustrator is more of a drawing program. In
          the world of free software, the GNU image-processing program, Gimp, is a good alternative to
          Photoshop, while Inkscape is a reasonably capable free drawing program</p>

        <h2>File Formats</h2>

        <p>The divide between raster and vector graphics also appears in the field of graphics file
          formats. There are many ways to represent an image as data stored in a file. If the original
          image is to be recovered from the bits stored in the file, the representation must follow some
          exact, known specification.</p>
        <p>Such a specification is called a <b>graphics file format</b>.</p>
        <p>Some popular graphics file formats include GIF, PNG, JPEG, WebP, and SVG. Most images used on the
          Web are GIF, PNG, or JPEG, but most browsers also have support for SVG images and for
          the newer WebP format</p>

        <img class="center" style="width: 65%; height: 65%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/raster%20vs%20vector.png?raw=true"
          alt="raster vs vector">

        <p><b>GIF, PNG, JPEG, and WebP are raster graphics formats; an image is specified
            by storing a color value for each pixel.</b></p>

        <p><b>JPEG (Joint Photographic Experts Group)</b> allows <b>up to 16 million colors</b> and is <b>best for
            images with many
            colors or color gradations</b>, especially photographs. JPEG is a <b>"lossy" format, meaning each time the
            image is
            saved and compressed, some image information is lost, degrading quality</b>. JPEG images allow for various
          levels of compression.</p>
        <p>Low compression means high image quality, but large file size.
          High compression means lower image quality, but smaller file size.</p>

        <p><b>GIF (Graphics Interchange Format)</b> is a <b>"lossless"</b> format, meaning <b>image quality is not
            degraded through
            compression</b>. However, GIFs are <b>limited to a 256-color palette</b>, making them suitable for
          <b>simpler graphics with
            fewer colors</b>. GIFs also <b>support transparent backgrounds and simple animations</b>.
        </p>

        <p><b>PNG (Portable Network Graphics)</b> combines features of <b>both JPEG and GIF</b>. PNG <b>supports
            millions of colors and
            transparent backgrounds</b>. It uses <b>lossless compression, ensuring no quality loss</b>. However, PNGs
          may not be
          supported by older web browsers.</p>

        <p>WebP is a modern format that supports both lossless and lossy compression, providing a balance between image
          quality and file size.</p>

        <p>The amount of data necessary to represent a raster image can be quite large. However, the data usually
          contains a lot of redundancy and can be compressed to reduce its size. GIF and PNG use lossless compression,
          meaning the original image can be perfectly recovered. JPEG uses lossy compression, which allows for greater
          reduction in file size but at the cost of some image quality. WebP supports both types of compression.</p>


        <p><b>SVG, on the other hand, is fundamentally a vector graphics format (although SVG images
            can include raster images).</b> SVG is actually an XML-based language for describing twodimensional vector
          graphics images.</p>
        <p>"SVG" stands for "Scalable Vector Graphics" and the term "scalable" indicates one of the advantages of vector
          graphics: There is no loss of quality when the size of the image is increased. A line between two points can
          be
          represented at any scale, and it is still the same perfect geometric line. If We try to greatly increase the
          size of
          a raster image, on the other hand, We will find that We don't have enough color values for
          all the pixels in the new image; each pixel from the original image will be expanded to cover a
          rectangle of pixels in the scaled image, and We will get multi-pixel blocks of uniform color. The
          scalable nature of SVG images make them a good choice for web browsers and for graphical
          elements on our computer's desktop. And indeed, some desktop environments are now using
          SVG images for their desktop icons.
        </p>

        <p>A digital image, no matter what its format, is specified using a coordinate system. A
          coordinate system sets up a correspondence between numbers and geometric points. In two
          dimensions, each point is assigned a pair of numbers, which are called the coordinates of the
          point. The two coordinates of a point are often called its x -coordinate and y-coordinate,
          although the names "x" and "y" are arbitrary.</p>

        <p>A raster image is a two-dimensional grid of pixels arranged into rows and columns. As
          such, it has a natural coordinate system in which each pixel corresponds to a pair of integers
          giving the number of the row and the number of the column that contain the pixel. (Even in
          this simple case, there is some disagreement as to whether the rows should be numbered from
          top-to-bottom or from bottom-to-top.)</p>

        <p>For a vector image, it is natural to use real-number coordinates. The coordinate system for
          an image is arbitrary to some degree; that is, the same image can be specified using different
          coordinate systems.</p>

      </article>
      <br />
    </section>

    <section class="main-section" id="Pixels_and_Coordinate_Systems">
      <br />
      <header><b>Pixels and Coordinate Systems</b></header>
      <article>

        <p>As previously mentioned, most images viewed online are raster-based. Raster images are created with
          pixel-based
          software or captured with a camera or scanner. They are more common in general such as jpg, gif, png, and are
          widely
          used on the web.</p>

        <p>To create these two-dimensional images, each point in the image is assigned a color.</p>

        <p> A point in 2D can be identified by a pair of numerical coordinates. Colors can also be specified
          numerically. </p>

        <p>However, the assignment of numbers to points or colors is somewhat arbitrary.
          So we need to spend some time studying coordinate systems, which associate numbers to
          points, and color models, which associate numbers to colors.</p>

        <p>A digital image is made up of rows and columns of pixels. A pixel in such an image can be
          specified by saying which column and which row contains it. In terms of coordinates, a pixel
          can be identified by a pair of integers giving the column number and the row number.</p>

        <p> For example, the pixel with coordinates (3,5) would lie in column number 3 and row number 5.</p>

        <p>Conventionally, columns are numbered from left to right, starting with zero. Most graphics
          systems (like HTML Canvas), number rows from top to bottom starting from zero. </p>

        <p>Some, including OpenGL, number the rows from bottom to top instead.</p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pixel%20grids.png?raw=true"
          alt="Pixel grids">

        <p>Note in particular that the pixel that is identified by a pair of coordinates (x,y) depends on the
          choice of coordinate system. We always need to know what coordinate system is in use before
          We know what point We are talking about.</p>

        <p>Row and column numbers identify a pixel, not a point. A pixel contains many points;
          mathematically, it contains an infinite number of points. The goal of computer graphics is not
          really to color pixels—it is to create and manipulate images. In some ideal sense, an image
          should be defined by specifying a color for each point, not just for each pixel. Pixels are an
          approximation. If we imagine that there is a true, ideal image that we want to display, then
          any image that we display by coloring pixels is an approximation. This has many implications.</p>

        <p>Suppose, for example, that we want to draw a line segment. A mathematical line has no
          thickness and would be invisible. So we really want to draw a thick line segment, with some
          specified width.</p>
        <p>Let's say that the line should be one pixel wide.</p>
        <p>The problem is that, <b>unless
            the line is horizontal or vertical, we can't actually draw the line by coloring pixels</b>. A diagonal
          geometric line will cover some pixels only partially. It is not possible to make part of a pixel
          black and part of it white. When We try to draw a line with black and white pixels only,
          the result is a jagged staircase effect.</p>

        <p>This effect is an example of something called <b>"aliasing"</b>.</p>
        <p>Aliasing can also be seen in the outlines of characters drawn on the screen and in diagonal or
          curved boundaries between any two regions of different color. (The term aliasing likely comes
          from the fact that ideal images are naturally described in real-number coordinates. When We
          try to represent the image using pixels, many real-number coordinates will map to the same
          integer pixel coordinates; they can all be considered as different names or "aliases" for the same
          pixel.)
        </p>

        <h3>Anti-Aliasing</h3>

        <p>Anti-aliasing is a fundamental technique employed in graphics production that allows for smoother and more
          realistic images.
          This technology is used to reduce the jagged edges or "jaggies" that are commonly seen in computer-generated
          images,
          allowing them to appear as they would in real life.</p>
        <p>It was presented by the Architecture Machine Group team, which later became known as the Media Lab,
          a laboratory engaged in research and development in the field of technology, science, art, design, and
          medicine,
          in 1972 at the Massachusetts Institute of Technology.</p>
        <p>The idea is that when a pixel is only partially covered by a shape, the color of the pixel should be
          a mixture of the color of the shape and the color of the background. When drawing a black line
          on a white background, the color of a partially covered pixel would be gray, with the shade of
          gray depending on the fraction of the pixel that is covered by the line. (In practice, calculating
          this area exactly for each pixel would be too difficult, so some approximate method is used.)
        </p>
        <p>At its core, <b>anti-aliasing (also known as AA) is a method of manipulating pixels so that they appear
            smoother
            than they actually are</b>.
          To achieve this effect, the software or hardware being used will <b>sample adjacent pixels and create an
            average
            color value
            between them</b>. This helps the image appear more natural and realistic since it blends together sharp
          pixel
          lines into one
          continuous line instead of several distinct pixelated lines.</p>

        <p>So why does the "jagged" effect occur? Modern monitors and screens of mobile devices consist of quadrangular
          elements - pixels.
          This means that, in fact, only horizontal or vertical lines can be displayed in straight lines with clear
          boundaries.
          Angled curves are displayed as "steps". For example, the line in the picture below appears straight, but as
          We zoom in,
          it becomes clear that it is not.</p>

        <p>Here, for example, is a geometric line, shown on the left, along with two approximations of that
          line made by coloring pixels. The lines are greatly magnified so that We can see the individual
          pixels. The line on the right is drawn using anti-aliasing, while the one in the middle is not:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/antialiasing%201.png?raw=true"
          alt="Antialiasing 1">

        <p>Note that anti-aliasing does not give a perfect image, but it can reduce the "jaggies" that are
          caused by aliasing (at least when it is viewed on a normal scale).</p>

        <p>Anyone who has played older games is familiar with the distinctive pixelated and blocky aesthetic.
          "Jaggedness" occurs due to the lack of smooth transitions between colors, and anti-aliasing helps to mitigate
          this issue.</p>

        <p>Jagged edges, or aliasing, occur when real-world objects with smooth, continuous curves are rasterized using
          pixels.
          This problem arises from <b>undersampling</b>, which happens when the sampling frequency is lower than the
          <a
            href="https://www.gatan.com/nyquist-frequency#:~:text=The%20Nyquist%2DShannon%20sampling%20theorem,shown%20in%20the%20figures%20below.">Nyquist
            Sampling Frequency</a>,
          leading to a loss of information about the image.
        </p>

        <p>Anti-aliasing works by sampling multiple points within and around each pixel, then calculating an average
          color value.
          This process effectively blurs the edges of objects, creating the illusion of smoother lines and reducing
          visible pixelation.</p>

        <p>While anti-aliasing improves image quality, it also increases the load on the processor and graphics card,
          as they need to render additional shades and expend more power resources.</p>

        <p>One way to reduce jagged edges is to increase the resolution, as higher resolution images have smaller
          pixels,
          making the blocky appearance less noticeable. However, resolution alone is not always sufficient,
          and software developers use various anti-aliasing techniques to further improve image quality.</p>

        <h3>Methods of Anti-Aliasing (AA)</h3>

        <p>There are essentially four methods of Anti-Aliasing:</p>
        <ol>
          <li>High-Resolution Display</li>
          <li>Post-Filtering (Supersampling)</li>
          <li>Pre-Filtering (Area Sampling)</li>
          <li>Pixel Phasing</li>
        </ol>

        <h4>High-Resolution Display</h4>

        <p>Using a high-resolution display is one of the simplest methods of anti-aliasing.
          By increasing the resolution, more pixels can be used to represent the image, reducing the appearance of
          jagged edges.
          However, this method is limited by the physical resolution of the display and may not be practical for all
          applications.</p>

        <h4>Post-Filtering (Supersampling)</h4>

        <p>Post-filtering, also known as supersampling, involves treating the screen as if it has a finer grid,
          effectively reducing the pixel size.
          The average intensity of each pixel is calculated from the intensities of subpixels, and the image is
          displayed at the screen resolution.
          This method is called post-filtering because it is done after generating the rasterized image.</p>

        <h4>Pre-Filtering (Area Sampling)</h4>

        <p>Pre-filtering, or area sampling, calculates pixel intensities based on the areas of overlap between each
          pixel and the objects to be displayed.
          The final pixel color is an average of the colors of the overlapping areas. This method is called
          pre-filtering because it is done before generating the rasterized image.</p>

        <h4>Pixel Phasing</h4>

        <p>Pixel phasing involves shifting pixel positions to approximate the positions near object geometry.
          Some systems allow the size of individual pixels to be adjusted to distribute intensities, which helps in
          pixel phasing.</p>

        <h3>Types of Anti-Aliasing (AA)</h3>

        <p>Generally all anti-aliasing methods can be classified into two classifications:</p>
        <ol>
          <li>Spatial Anti-Aliasing</li>
          <li>Post Process Anti-Aliasing</li>
        </ol>

        <h4>1. Spatial Anti-Aliasing</h4>

        <p>Spatial anti-aliasing techniques work by sampling multiple points within each pixel and averaging the colors
          to reduce jagged edges.</p>

        <h4>Supersampling Anti-Aliasing (SSAA)</h4>

        <p><b>Supersampling Anti-Aliasing (SSAA)</b>, also called full-scene anti-aliasing (FSAA), works by <b>rendering
            the
            image at a higher resolution and then downsampling it to the display resolution</b>.
          This method reduces jagged edges by averaging colors near the edges.</p>
        <p> In this approach, a 512x512 image is first computed at higher resolution, such as 2048x2048, for example.
          It is then reduced through averaging or filtering to produce a 512x512 image.</p>
        <p>While effective, SSAA is <b>computationally
            intensive and can heavily load the GPU</b>.</p>

        <h4>Multi-Sample Anti-Aliasing (MSAA)</h4>

        <p><b>Multi-Sample Anti-Aliasing (MSAA)</b> improves performance compared to SSAA by <b>sampling multiple points
            within
            each pixel only at the edges of polygons</b>.</p>
        <p> Images are computed for 4 (or 8) subpixel sample points, followed by averaging. It is slow, since the frame
          rate is
          reduced by a factor of 4 (or 8). It works well for horizontal and vertical triangle edges.
          For other edge angles, the gaps between subpixels can cause narrow face breakups.</p>
        <p>This method <b>reduces the computational load while still providing good anti-aliasing quality</b>.</p>

        <h4>Coverage Sampling Anti-Aliasing (CSAA)</h4>

        <p><b>Coverage Sampling Anti-Aliasing (CSAA)</b> is an Nvidia-specific technique that improves upon MSAA by
          increasing
          the number of coverage samples
          without significantly increasing the number of color/depth samples. This method <b>provides better edge
            quality
            with less performance impact</b>.</p>

        <h4>2. Post-Processing Anti-Aliasing</h4>

        <p>Post-processing anti-aliasing techniques are applied after the image has been rendered to smooth out jagged
          edges.</p>

        <h4>Fast Approximate Anti-Aliasing (FXAA)</h4>

        <p>Fast Approximate Anti-Aliasing (FXAA) is a post-processing technique, created by Timothy Lottes at Nvidia,
          that smooths edges by <b>analyzing the final image and blending colors at the edges</b>.</p>
        <p>This is the <b>cheapest and simplest smoothing algorithm</b>.</p>
        <p>In layman's terms, FXAA is applied to our final rendered image and works based on pixel data, not geometry.
          GPU's are particularly fast at executing these shader algorithms in parallel, thus it's very quick to render.
        </p>
        <p>FXAA is less computationally intensive than SSAA and MSAA, making it <b>suitable for real-time applications
            like
            video games</b>.</p>

        <h4>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</h4>

        <p><b>Enhanced Subpixel Morphological Anti-Aliasing (SMAA)</b> is a logical development of the FXAA algorithm.
          This post effect is used in post-processing the final image that combines edge
          detection and blending to reduce aliasing.
          SMAA provides <b>high-quality anti-aliasing with a lower performance cost compared to SSAA and MSAA</b>.</p>

        <h4>Temporal Anti-Aliasing (TAA) </h4>

        <p>Temporal anti-aliasing techniques use information from previous frames to reduce aliasing in the current
          frame.</p>

        <p><b>Temporal Anti-Aliasing (TAA)</b> reduces aliasing by <b>using information from previous frames to smooth
            edges in
            the current frame.</b>
          TAA is effective at reducing flickering and shimmering in moving images, but it <b>can introduce ghosting
            artifacts</b> (visual distortions that appear in images due to a variety of factors, including movement,
          refraction, and sampling errors)
          if not implemented correctly.</p>






        <p>There are other issues involved in mapping real-number coordinates to pixels.</p>
        <p>For example,
          which point in a pixel should correspond to integer-valued coordinates such as (3,5)? The center
          of the pixel? One of the corners of the pixel? In general, we think of the numbers as referring
          to the top-left corner of the pixel.</p>
        <p>Another way of thinking about this is to say that integer coordinates refer to the lines between pixels,
          rather than to the pixels themselves. But that still doesn't determine exactly which pixels are affected when
          a geometric shape is drawn.</p>

        <p>For example, here are two lines drawn using HTML canvas graphics, shown greatly magnified. The
          lines were specified to be colored black with a one-pixel line width:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/antialiasing%202.png?raw=true"
          alt="Antialiasing 2">

        <p>The top line was drawn from the point (100,100) to the point (120,100).</p>

        <p>In canvas graphics, integer coordinates correspond to the lines between pixels, but when a one-pixel line is
          drawn,
          it extends one-half pixel on either side of the infinitely thin geometric line.</p>

        <p>So for the top line, the line as it is drawn lies half in one row of pixels and half in another row. The
          graphics
          system, which uses anti-aliasing, rendered the line by coloring both rows of pixels gray.</p>

        <p>The bottom line was drawn from the point (100.5,100.5) to (120.5,100.5). In this case, the line lies
          exactly along one line of pixels, which gets colored black. The gray pixels at the ends of the
          bottom line have to do with the fact that the line only extends halfway into the pixels at its
          endpoints. Other graphics systems might render the same lines differently</p>

        <p>All this is complicated further by the fact that pixels aren't what they used to be. Pixels
          today are smaller!</p>

        <h3>Understanding Resolution</h3>

        <p>The <b>resolution</b> of a display device can be measured in terms of the number
          of pixels per inch on the display, a quantity referred to as <b>PPI (pixels per inch)</b> or sometimes
          <b>DPI (dots per inch)</b>.
        </p>

        <h4>PPI vs DPI</h4>

        <p>While PPI (Pixels Per Inch) and DPI (Dots Per Inch) are often used interchangeably, they refer to different
          concepts and are used in different contexts.</p>

        <h4>Pixels Per Inch (PPI)</h4>

        <p>PPI is a measure of the <b>pixel density of a digital display</b>, such as a computer monitor, smartphone
          screen, or
          television. It indicates the number of pixels present in one inch of the display. <b>Higher PPI values mean
            more
            pixels are packed into each inch, resulting in sharper and more detailed images</b>.</p>

        <p>For example, a display with a resolution of 1920x1080 pixels and a diagonal size of 15.6 inches has a PPI of
          approximately 141. This means there are 141 pixels in each inch of the display.</p>

        <h4>Dots Per Inch (DPI)</h4>

        <p>DPI is a measure of the <b>resolution of a printed image</b>, indicating the number of individual dots of ink
          or
          toner that a printer can produce within one inch. <b>Higher DPI values result in finer detail and smoother
            gradients in printed images</b>.</p>

        <p>For example, a printer with a resolution of 300 DPI can produce 300 dots of ink per inch, resulting in
          high-quality prints suitable for photographs and detailed graphics.</p>

        <p>Both measures are important for ensuring high-quality visuals, but they apply to different
          mediums.</p>

        <p>Early screens tended to have resolutions of somewhere close to 72 PPI.
          At that resolution, and at a typical viewing distance, individual pixels are clearly visible. For a
          while, it seemed like most displays had about 100 pixels per inch, but high resolution displays
          today can have 200, 300 or even 400 pixels per inch. At the highest resolutions, individual
          pixels can no longer be distinguished.
        </p>

        <p>The fact that pixels come in such a range of sizes is a problem if we use coordinate systems
          based on pixels. An image created assuming that there are 100 pixels per inch will look tiny on a
          400 PPI display. A one-pixel-wide line looks good at 100 PPI, but at 400 PPI, a one-pixel-wide
          line is probably too thin</p>

        <p>In fact, in many graphics systems, "pixel" doesn't really refer to the size of a physical
          pixel. Instead, it is just <b>another unit of measure</b>, which is set by the system to be something
          appropriate. (On a desktop system, a pixel is usually about one one-hundredth of an inch. On
          a smart phone, which is usually viewed from a closer distance, the value might be closer to
          1/160 inch. Furthermore, the meaning of a pixel as a unit of measure can change when, for
          example, the user applies a magnification to a web page.)
        </p>

        <p>Pixels cause problems that have not been completely solved. Fortunately, they are less of a
          problem for vector graphics.</p>
        <p>For vector graphics, pixels only become an issue during rasterization, the step in which a vector image is
          converted into pixels for display. The vector image itself can be created using any convenient coordinate
          system. It represents an idealized, resolution-independent image.</p>
        <p>A rasterized image is an approximation of that ideal image, but how to do the approximation can be left to
          the display
          hardware.
        </p>

        <h2>Real-number Coordinate Systems</h2>

        <p>When doing 2D graphics, We are given a rectangle in which We want to draw some graphics
          primitives. Primitives are specified using some coordinate system on the rectangle. It should
          be possible to select a coordinate system that is appropriate for the application. For example, if
          the rectangle represents a floor plan for a 15 foot by 12 foot room, then We might want to use
          a coordinate system in which the unit of measure is one foot and the coordinates range from 0
          to 15 in the horizontal direction and 0 to 12 in the vertical direction. The unit of measure in
          this case is feet rather than pixels, and one foot can correspond to many pixels in the image.
          The coordinates for a pixel will, in general, be real numbers rather than integers. In fact, it's
          better to forget about pixels and just think about points in the image. A point will have a pair
          of coordinates given by real numbers.</p>

        <p>To specify the coordinate system on a rectangle, We just have to specify the horizontal
          coordinates for the left and right edges of the rectangle and the vertical coordinates for the top
          and bottom. Let's call these values left, right, top, and bottom. Often, they are thought of as
          xmin, xmax, ymin, and ymax, but there is no reason to assume that, for example, top is less
          than bottom. We might want a coordinate system in which the vertical coordinate increases
          from bottom to top instead of from top to bottom. In that case, top will correspond to the
          maximum y-value instead of the minimum value.</p>

        <p>To allow programmers to specify the coordinate system that they would like to use, it would
          be good to have a subroutine such as:</p>

        <p class="center"><b>setCoordinateSystem(left,right,bottom,top)</b></p>

        <p>The graphics system would then be responsible for automatically transforming the coordinates
          from the specified coordinate system into pixel coordinates. Such a subroutine might not be
          available, so it's useful to see how the transformation is done by hand. Let's consider the general
          case. Given coordinates for a point in one coordinate system, we want to find the coordinates
          for the same point in a second coordinate system. (Remember that a coordinate system is just
          a way of assigning numbers to points. It's the points that are real!)</p>

        <p>Suppose that the horizontal and vertical limits are oldLeft, oldRight, oldTop, and oldBottom for the first
          coordinate system,
          and are newLeft, newRight, newTop, and newBottom for the second. Suppose that a point
          has coordinates (oldX,oldY ) in the first coordinate system. We want to find the coordinates
          (newX,newY ) of the point in the second coordinate system</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinates.png?raw=true"
          alt="coordinates">

        <p>Formulas for newX and newY are then given by: </p>

        <p class="center"><b>newX = newLeft + ((oldX - oldLeft) / (oldRight - oldLeft)) * (newRight - newLeft)</b></p>
        <p class="center"><b>newY = newTop + ((oldY - oldTop) / (oldBottom - oldTop)) * (newBottom - newTop)</b></p>

        <p>The logic here is that oldX is located at a certain fraction of the distance from oldLeft to
          oldRight. That fraction is given by:</p>

        <p class="center"><b>(oldX - oldLeft) / (oldRight - oldLeft)</b></p>

        <p>The formula for newX just says that newX should lie at the same fraction of the distance from
          newLeft to newRight. We can also check the formulas by testing that they work when oldX is
          equal to oldLeft or to oldRight, and when oldY is equal to oldBottom or to oldTop.</p>

        <p>As an example, suppose that we want to transform some real-number coordinate system
          with limits left, right, top, and bottom into pixel coordinates that range from 0 at left to 800 at
          the right and from 0 at the top 600 at the bottom. In that case, newLeft and newTop are zero,
          and the formulas become simply:</p>

        <p class="center"><b>newX = ((oldX - left) / (right - left)) * 800</b></p>
        <p class="center"><b>newY = ((oldY - top) / (bottom - top)) * 600</b></p>

        <p>Of course, this gives newX and newY as real numbers, and they will have to be rounded
          or truncated to integer values if we need integer coordinates for pixels. The reverse
          transformation—going from pixel coordinates to real number coordinates—is also useful.</p>

        <p>For example, if the image is displayed on a computer screen, and We want to react to mouse clicks
          on the image, We will probably get the mouse coordinates in terms of integer pixel coordinates,
          but We will want to transform those pixel coordinates into our own chosen coordinate system.</p>

        <p>In practice, though, We won't usually have to do the transformations Werself, since most
          graphics APIs provide some higher level way to specify transforms.</p>

        <h2>Aspect Ratio</h2>

        <p>The <b>aspect ratio</b> of a rectangle is the ratio of its width to its height. For example an aspect
          ratio of 2:1 means that a rectangle is twice as wide as it is tall, and an aspect ratio of 4:3 means
          that the width is 4/3 times the height. Although aspect ratios are often written in the form
          width:height, I will use the term to refer to the fraction width/height. A square has aspect ratio
          equal to 1. A rectangle with aspect ratio 5/4 and height 600 has a width equal to 600*(5/4),
          or 750.</p>

        <p>A coordinate system also has an aspect ratio. If the horizontal and vertical limits for the
          coordinate system are left, right, bottom, and top, as above, then the aspect ratio is the absolute
          value of:</p>

        <p class="center"><b>(right - left) / (top - bottom)</b></p>

        <p>If the coordinate system is used on a rectangle with the same aspect ratio, then when viewed in
          that rectangle, one unit in the horizontal direction will have the same apparent length as a unit
          in the vertical direction. If the aspect ratios don't match, then there will be some distortion.</p>

        <p>For example, the shape defined by the equation x^2 + y^2 = 9 should be a circle, but that will
          only be true if the aspect ratio of the (x,y) coordinate system matches the aspect ratio of the
          drawing area.</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/aspect%20ratio%201.png?raw=true"
          alt="aspect ratio 1">

        <p>It is not always a bad thing to use different units of length in the vertical and horizontal
          directions. However, suppose that We want to use coordinates with limits left, right, bottom,
          and top, and that We do want to preserve the aspect ratio.</p>

        <p>In that case, depending on the shape of the display rectangle, We might have to adjust the values either of
          left and right or
          of bottom and top to make the aspect ratios match:</p>

        <img class="center" style="width: 100%; height: 100%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/aspect%20ratio%202.png?raw=true"
          alt="aspect ratio 2">

      </article>
      <br />
    </section>

    <section class="main-section" id="Color_Models">
      <br />
      <header><b>Color Models</b></header>
      <article>

        <p>We are talking about the most basic foundations of computer graphics. One of those is
          coordinate systems. The other is color.</p>

        <p>Red, Yellow, and Blue — Primary colors. Or at least, that's what we have been told since kindergarten, isn't
          it?
          But there is more to it.</p>

        <p>The colors on a computer screen are produced as combinations of <b>red, green, and blue light</b>.</p>

        <p>Now the question is — if RYB is the primary color set then why do computers use RGB instead?</p>

        <p>Going deep into the line, we first need to understand the color theory.</p>
        <p>There are two different theories: </p>
        <ol>
          <li>Additive</li>
          <li>Subtractive</li>
        </ol>

        <h2>Additive</h2>

        <p>Different colors are produced by varying the intensity of each type of light. A color can be
          specified by three numbers giving the intensity of red, green, and blue in the color. Intensity
          can be specified as a number in the range zero, for minimum intensity, to one, for maximum
          intensity.</p>

        <p>The additive is the case of the projection of one or more colored lights (wavelengths). These are the colors
          when mixed produce more light.</p>

        <p>This method of specifying color is called the <b>RGB color model</b>, where RGB stands
          for Red/Green/Blue.</p>


        <p>The red, green, and blue values for a color are called the color components of
          that color in the RGB color model and when mixed produces lighter colors, resulting in white light at the end.
          That's how our computer, TV, and other light-emitting screen works.</p>

        <p>Each parameter (red, green, and blue) defines the intensity of the color with a value <b>between 0 and
            255</b>.</p>

        <p>This means that there are 256 x 256 x 256 = 16777216 possible colors!</p>

        <p>For example, rgb(255, 0, 0) is displayed as red, because red is set to its highest value (255), and the other
          two (green and blue) are set to 0.</p>

        <p>Another example, rgb(0, 255, 0) is displayed as green, because green is set to its highest value (255), and
          the other two (red and blue) are set to 0.</p>

        <p>To display black, set all color parameters to 0, like this: rgb(0, 0, 0).</p>

        <p>To display white, set all color parameters to 255, like this: rgb(255, 255, 255).</p>

        <h3>Shades of Gray</h3>

        <p>Shades of gray are often defined using equal values for all three parameters:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shades%20of%20gray.png?raw=true"
          alt="shades of gray">

        <p>Light is made up of waves with a variety of wavelengths. A pure color is one for which
          all the light has the same wavelength, but in general, a color can contain many wavelengths—
          mathematically, an infinite number. How then can we represent all colors by combining just
          red, green, and blue light? In fact, we can't quite do that.</p>

        <p>We might have heard that combinations of the three basic, or "primary" colors are sufficient
          to represent all colors, because the human eye has three kinds of color sensors that detect red,
          green, and blue light. However, that is only an approximation. The eye does contain three
          kinds of color sensors. The sensors are called "cone cells."</p>

        <p>However, cone cells do not respond exclusively to red, green, and blue light. Each kind of cone cell
          responds, to a varying degree,
          to wavelengths of light in a wide range. A given mix of wavelengths will stimulate each type
          of cell to a certain degree, and the intensity of stimulation determines the color that we see. A
          different mixture of wavelengths that stimulates each type of cone cell to the same extent will
          be perceived as the same color.</p>

        <p>So a perceived color can, in fact, be specified by three numbers
          giving the intensity of stimulation of the three types of cone cell. However, it is not possible
          to produce all possible patterns of stimulation by combining just three basic colors, no matter how those
          colors are chosen.
          This is just a fact about the way our eyes actually work; it might have been different. </p>

        <p>Three basic colors can produce a reasonably large fraction of the set of
          perceivable colors, but there are colors that We can see in the world that We will never see on
          our computer screen. (This whole discussion only applies to people who actually have three
          kinds of cone cell. Color blindness, where someone is missing one or more kinds of cone cell, is
          surprisingly common.)
        </p>

        <p>The range of colors that can be produced by a device such as a computer screen is called
          the <b>color gamut</b> of that device. Different computer screens can have different color gamuts,
          and the same RGB values can produce somewhat different colors on different screens. The color
          gamut of a color printer is noticeably different—and probably smaller—than the color gamut
          of a screen, which explains why a printed image probably doesn't look exactly the same as it
          did on the screen.
        </p>

        <h2>Subtractive</h2>

        <p>When we mix paints or inks, subtractive mixing results. Paints or inks are non-emissive objects here. They
          reflect when light falls on them.
          Molecules of paint absorb some of the wavelengths of light and reflect rest. That's how we see such objects.
        </p>

        <p>Printers, by the way, make colors differently from the way a screen does it.
          Whereas a screen combines light to make a color, a printer combines inks or dyes. Because of
          this difference, colors meant for printers are often expressed using a different set of basic colors.</p>

        <p>The primary colors of the subtractive mix are CMYK — Cyan, Magenta, Yellow, and K which stands for black ( To
          distinguish it from B for Blue.
          Just a convention.)</p>

        <p>When the CMY (not K) gets mixed, it produces brownish color — a bit muddy. To get the more blackish color,
          the additional K for black is used.
          CMYK — the model used by printers & publishing houses.</p>

        <img class="center" style="width: 75%; height: 75%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/additive%20and%20subtractive.png?raw=true"
          alt="additive and subtractive color">

        <p>In any case, the most common color model for computer graphics is RGB. RGB colors are
          most often represented using 8 bits per color component, a total of 24 bits to represent a color.
          This representation is sometimes called "24-bit color."" An 8-bit number can represent 28, or
          256, different values, which we can take to be the positive integers from 0 to 255. A color is
          then specified as a triple of integers (r,g,b) in that range.</p>

        <p>This representation works well because 256 shades of red, green, and blue are about as many
          as the eye can distinguish. In applications where images are processed by computing with color
          components, it is common to use additional bits per color component to avoid visual effects
          that might occur due to rounding errors in the computations. Such applications might use a
          16-bit integer or even a 32-bit floating point value for each color component. On the other
          hand, sometimes fewer bits are used.</p>

        <p> For example, one common color scheme uses 5 bits for
          the red and blue components and 6 bits for the green component, for a total of 16 bits for a
          color. (Green gets an extra bit because the eye is more sensitive to green light than to red or
          blue.) This “16-bit color” saves memory compared to 24-bit color and was more common when
          memory was more expensive.
        </p>

        <p>There are many other color models besides RGB. RGB is sometimes criticized as being
          unintuitive. For example, it's not obvious to most people that yellow is made of a combination
          of red and green.</p>

        <h2>Hues, Saturation, and Values (Lightness)</h2>

        <p> The closely related color models <b>HSV</b> and <b>HSL</b> describe the same set of
          colors as RGB, but attempt to do it in a more intuitive way. (HSV is sometimes called HSB,
          with the "B" standing for "brightness" HSV and HSB are exactly the same model.)
        </p>

        <p>The "H" in these models stands for <b>"hue", a basic spectral color</b>. As H increases, the color
          changes from red to yellow to green to cyan to blue to magenta, and then back to red. The
          value of H is often taken to range from 0 to 360, since the colors can be thought of as arranged
          around a circle with red at both 0 and 360 degrees.</p>

        <p>The "S" in HSV and HSL stands for <b>"saturation"</b> and is taken to <b>range from 0 to 1</b>. A
          saturation of 0 gives a shade of gray (the shade depending on the value of V or L). A saturation
          of 1 gives a "pure color" and decreasing the saturation is like adding more gray to the color.</p>

        <p>"V" stands for <b>"value"</b> and "L" stands for <b>"lightness"</b>. They determine how bright or dark the
          color is. The main difference is that in the HSV model, the pure spectral colors occur for V=1,
          while in HSL, they occur for L=0.5.
        </p>

        <img class="center" style="width: 50%; height: 50%; top: 50%; left: 50%;"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/HSV.png?raw=true"
          alt="HSV explained">

        <p>Let's look at some colors in the HSV color model. The illustration below shows colors with
          a full range of H-values, for S and V equal to 1 and to 0.5. Note that for S=V=1, We get
          bright, pure colors. S=0.5 gives paler, less saturated colors. V=0.5 gives darker colors.
        </p>

        <img style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/HSV%20color%20model.png?raw=true"
          alt="HSV color model">

        <p> In the simple scale diagrams below, the first model indicates amount of black, white, or grey pigment added
          to the hue.
          The second model illustrates the same scale but explains the phenomenon based on light [spectral] properties.
        </p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pigment%20and%20light%20scale.png?raw=true"
          alt="pigment and light scale">

        <p>Regardless of the two Additive and Subtractive color models, all color is a result of how our eyes physically
          process light waves.
          So let's start with the light Additive model to see how it filters into the Subtractive model and to see how
          hues,
          values and saturation interact to produce unique colors.</p>

        <h3>Hues</h3>

        <p>The three primary hues in light are red, green, and blue. Thus, that is why televisions, computer monitors,
          and other full-range,
          electronic color visual displays use a triad of red, green, and blue phosphors to produce all electronically
          communicated color.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%201.png?raw=true"
          alt="hues 1">

        <p>As we mentioned before, in light, all three of these wavelengths added together at full strength produces
          pure white light.
          The absence of all three of these colors produces complete darkness, or black.</p>

        <h3>Mixing Adjacent Primaries = Secondary Hues</h3>

        <h4>Making Cyan, Magenta, and Yellow</h4>

        <p>Although additive and subtractive color models are considered their own unique entities for screen vs. print
          purposes,
          the hues CMY do not exist in a vacuum.</p>

        <p>They are produced as secondary colors when RGB light hues are mixed, as follows:</p>

        <ul>
          <li>Green + Red light → Yellow</li>
          <li>Red + Blue light → Magenta</li>
          <li>Blue + Green light → Cyan</li>
        </ul>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%202.png?raw=true"
          alt="hues 2">

        <h3>Overview of Hues</h3>

        <p>The colors on the outermost perimeter of the color circle are the "hues", which are colors in their purest
          form. This process can continue
          filling in colors around the wheel. The next level colors, the tertiary colors, are those colors between the
          secondary and primary colors.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/hues%203.png?raw=true"
          alt="hues 3">

        <h3>Saturation</h3>

        <p>Saturation is also referred to as "intensity" and "chroma". It refers to the dominance of hue in the color.
          On the outer edge of the hue wheel are the 'pure' hues.
          As We move into the center of the wheel, the hue we are using to describe the color dominates less and less.
          When We reach the center of the wheel, no hue dominates. These colors directly on the central axis are
          considered <b>desaturated</b>.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/saturation%201.png?raw=true"
          alt="saturation 1">

        <p>Naturally, the opposite of the image above is to saturate color.</p>
        <p>The first example below describes the general direction color must
          move on the color circle to become more saturated (towards the outside). The second example depicts how a
          single color looks completely
          saturated, having no other hues present in the color.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/saturation%2002.png?raw=true"
          alt="saturation 2">


        <h3>Value</h3>

        <p>Now let's add "value" to the HSV scale. Value is the dimension of lightness/darkness. In terms of a spectral
          definition of color,
          value describes the overall intensity or strength of the light. If hue can be thought of as a dimension going
          around a wheel,
          then value is a linear axis running through the middle of the wheel, as seen below:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%201.png?raw=true"
          alt="value 1">

        <p>To better visualize even more, look at the example below showing a full color range for a single hue:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%202.png?raw=true"
          alt="value 2">

        <p>Now, if We imagine that each hue was also represented as a slice like the one above, we would have a solid,
          upside-down cone of colors.
          The example above can be considered a slice of the cone. Notice how the right-most edge of this cone slice
          shows the greatest amount of the
          dominant red hue (least amount of other competing hues), and how as We go down vertically, it gets darker in
          "value".</p>
        <p>Also notice that as we travel from right to left in the cone, the hue becomes less dominant and eventually
          becomes completely desaturated
          along the vertical center of the cone. This vertical center axis of complete desaturation is referred to as
          <b>grayscale</b>.
        </p>
        <p>See how this slice below translates into some isolated color swatches:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/value%203.png?raw=true"
          alt="value 3">


        <p>
          Often, a fourth component is added to color models. The fourth component is called <b>alpha</b>,
          and color models that use it are referred to by names such as <b>RGBA</b> and <b>HSLA</b>. Alpha is not a
          color as such. It is usually used to represent transparency.</p>

        <p>A color with maximal alpha value is
          fully opaque; that is, it is not at all transparent. A color with alpha equal to zero is completely
          transparent and therefore invisible. Intermediate values give translucent, or partly transparent, colors.
        </p>

        <p>Transparency determines what happens when We draw with one color (the foreground
          color) on top of another color (the background color). If the foreground color is fully opaque,
          it simply replaces the background color. If the foreground color is partly transparent, then it
          is blended with the background color.</p>

        <p>Assuming that the alpha component ranges from 0 to 1,
          the color that We get can be computed as: </p>

        <p class="center"><b>new_color = (alpha)*(foreground_color) + (1 - alpha)*(background_color)</b></p>

        <p>This computation is done separately for the red, blue, and green color components. This is
          called <b>alpha blending</b>. The effect is like viewing the background through colored glass; the
          color of the glass adds a tint to the background color. This type of blending is not the only
          possible use of the alpha component, but it is the most common.</p>

        <p>An RGBA color model with 8 bits per component uses a total of 32 bits to represent a color.
          This is a convenient number because integer values are often represented using 32-bit values. A
          32-bit integer value can be interpreted as a 32-bit RGBA color.</p>

        <p>How the color components are arranged within a 32-bit integer is somewhat arbitrary.</p>

        <p>The most common layout is to store the alpha component in the eight high-order bits,
          followed by red, green, and blue. (This should probably be called ARGB color.) However, other layouts are also
          in use.</p>

      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="Shapes">
      <br />
      <header><b>Shapes</b></header>
      <article>

        <p>We have been talking about low-level graphics concepts like pixels and coordinates, but
          fortunately we don't usually have to work on the lowest levels. Most graphics systems let us
          work with higher-level shapes, such as triangles and circles, rather than individual pixels.</p>

        <p>In a graphics API, there will be certain basic shapes that can be drawn with one command,
          whereas more complex shapes will require multiple commands. Exactly what qualifies as a
          basic shape varies from one API to another.</p>

        <p>For example, the HTML5 canvas API provides commands to draw rectangles, circles, and
          lines, but not triangles. To draw a triangle, We have to draw three lines.</p>

        <p>By "line", we really mean line segment, that is a straight line segment connecting two given
          points in the plane. <b>A simple one-pixel-wide line segment, without anti-aliasing, is the most
            basic shape</b>. It can be drawn by coloring pixels that lie along the infinitely thin geometric line
          segment.</p>

        <p>An algorithm for drawing the line has to decide exactly which pixels to color. One of
          the first computer graphics algorithms, Bresenham's algorithm for line drawing, implements
          a very efficient procedure for doing so.</p>

        <p>In any case, lines are typically more complicated. Anti-aliasing is one
          complication. Line width is another. A wide line might actually be drawn as a rectangle.</p>

        <p>Lines can have other attributes, or properties, that affect their appearance. One question
          is, what should happen at the end of a wide line?</p>

        <p>Appearance might be improved by adding
          a rounded "cap" on the ends of the line. A square cap—that is, extending the line by half of
          the line width—might also make sense.</p>

        <p>Another question is, when two lines meet as part of a
          larger shape, how should the lines be joined? And many graphics systems support lines that
          are patterns of dashes and dots.</p>

        <p>This illustration shows some of the possibilities:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20lines.png?raw=true"
          alt="Types of Lines">

        <p>On the left are three wide lines with no cap, a round cap, and a square cap. The geometric line
          segment is shown as a dotted line. (The no-cap style is called “butt.”) To the right are four
          lines with different patterns of dots and dashes. In the middle are three different styles of line
          joins: mitered, rounded, and beveled.</p>

        <p>The basic rectangular shape has sides that are vertical and horizontal. (A tilted rectangle
          generally has to be made by applying a rotation.) Such a rectangle can be specified with two
          points, (x1,y1) and (x2,y2), that give the endpoints of one of the diagonals of the rectangle.
          Alternatively, the width and the height can be given, along with a single base point, (x,y). In
          that case, the width and height have to be positive, or the rectangle is empty. The base point
          (x,y) will be the upper left corner of the rectangle if y increases from top to bottom, and it will
          be the lower left corner of the rectangle if y increases from bottom to top.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20rectangles.png?raw=true"
          alt="Types of Rectangles">

        <p>Suppose that We are given points (x1,y1) and (x2,y2), and that We want to draw the rectangle
          that they determine. And suppose that the only rectangle-drawing command that We have
          available is one that requires a point (x,y), a width, and a height. For that command, x must
          be the smaller of x1 and x2, and the width can be computed as the absolute value of x1 minus
          x2. And similarly for y and the height.</p>

        <p>In pseudocode,</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rectangle%20pseudocode.png?raw=true"
          alt="Rectangle Pseudocode">

        <p>A common variation on rectangles is to allow rounded corners. For a “round rect,” the
          corners are replaced by elliptical arcs. The degree of rounding can be specified by giving the
          horizontal radius and vertical radius of the ellipse.</p>

        <p> Here are some examples of round rects. For
          the shape at the right, the two radii of the ellipse are shown:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rounded%20rectangles.png?raw=true"
          alt="Rounded Rectangles">

        <p>Our final basic shape is the oval. (An oval is also called an ellipse.) An oval is a closed curve
          that has two radii. For a basic oval, we assume that the radii are vertical and horizontal. An
          oval with this property can be specified by giving the rectangle that just contains it. Or it can
          be specified by giving its center point and the lengths of its vertical radius and its horizontal
          radius. </p>

        <p>In this illustration, the oval on the left is shown with its containing rectangle and with
          its center point and radii:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/types%20of%20ovals.png?raw=true"
          alt="Types of Ovals">

        <p>The oval on the right is a circle. A circle is just an oval in which the two radii have the same
          length.</p>

        <p>If ovals are not available as basic shapes, they can be approximated by drawing a large
          number of line segments. The number of lines that is needed for a good approximation depends
          on the size of the oval. It's useful to know how to do this. Suppose that an oval has center
          point (x,y), horizontal radius r1, and vertical radius r2. Mathematically, the points on the oval
          are given by:</p>

        <text class="center"><b>( x + r1*cos(angle), y + r2*sin(angle) )</b></text>

        <p>where angle takes on values from 0 to 360 if angles are measured in degrees or from 0 to 2π if
          they are measured in radians. Here sin and cos are the standard sine and cosine functions. To
          get an approximation for an oval, we can use this formula to generate some number of points
          and then connect those points with line segments.</p>

        <p>In pseudocode, assuming that angles are
          measured in radians and that pi represents the mathematical constant π,</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/oval%20pseudocode.png?raw=true"
          alt="Oval Pseudocode">

        <p>For a circle, of course, We would just have r1 = r2. This is the first time we have used the
          sine and cosine functions, but it won't be the last. These functions play an important role in
          computer graphics because of their association with circles, circular motion, and rotation. We
          will meet them again when we talk about transforms later.</p>

        <h2>Stroke and Fill</h2>

        <p>There are two ways to make a shape visible in a drawing.</p>

        <p>We can <b>stroke</b> it. Or, if it is a closed
          shape such as a rectangle or an oval, We can <b>fill</b> it.</p>

        <p>Stroking a line is like dragging a pen along
          the line. Stroking a rectangle or oval is like dragging a pen along its boundary.</p>

        <p>Filling a shape
          means coloring all the points that are contained inside that shape.</p>

        <p>It's possible to both stroke
          and fill the same shape; in that case, the interior of the shape and the outline of the shape can
          have a different appearance.
        </p>

        <p>When a shape intersects itself, like the two shapes in the illustration below, it's not entirely
          clear what should count as the interior of the shape. In fact, there are at least two different
          rules for filling such a shape.</p>

        <p> In fact, there are at least two different
          rules for filling such a shape. Both are based on something called the <b>winding number</b>. The
          winding number of a shape about a point is, roughly, <b>how many times the shape winds around
            the point in the positive direction</b>, which we'll take here to be counterclockwise.</p>

        <p>Winding number
          can be negative when the winding is in the opposite direction.</p>

        <p>In the illustration, the shapes on
          the left are traced in the direction shown, and the winding number about each region is shown
          as a number inside the region.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/understanding%20winding%20number.png?raw=true"
          alt="Understanding Winding Number">

        <p>The shapes are also shown filled using the two fill rules.</p>

        <p>For the shapes in the center, the fill
          rule is to color any region that has a non-zero winding number.</p>

        <p>For the shapes shown on the
          right, the rule is to color any region whose winding number is odd; regions with even winding
          number are not filled.
        </p>

        <p>There is still the question of what a shape should be filled with. Of course, it can be filled
          with a color, but other types of fill are possible, including <b>patterns</b> and <b>gradients</b>.</p>

        <p> A pattern
          is an image, usually a small image. When used to fill a shape, a pattern can be repeated
          horizontally and vertically as necessary to cover the entire shape.</p>

        <p>A gradient is similar in that
          it is a way for color to vary from point to point, but instead of taking the colors from an image,
          they are computed. There are a lot of variations to the basic idea, but there is always a line
          segment along which the color varies. The color is specified at the endpoints of the line segment,
          and possibly at additional points; between those points, the color is interpolated. The color
          can also be extrapolated to other points on the line that contains the line segment but lying
          outside the line segment; this can be done either by repeating the pattern from the line segment
          or by simply extending the color from the nearest endpoint.</p>

        <p>For a <b>linear gradient</b>, the color
          is constant along lines perpendicular to the basic line segment, so we get lines of solid color
          going in that direction.</p>

        <p>In a radial gradient, the color is constant along circles centered at
          one of the endpoints of the line segment.</p>

        <p>And that doesn't exhaust the possibilities. To give
          we an idea what patterns and gradients can look like, here is a shape, filled with two gradients
          and two patterns:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/patterns%20and%20gradients.png?raw=true"
          alt="Patterns and Gradients">

        <p>The first shape is filled with a simple linear gradient defined by just two colors, while the second
          shape uses a radial gradient.</p>

        <p>Patterns and gradients are not necessarily restricted to filling shapes. Stroking a shape is,
          after all, the same as filling a band of pixels along the boundary of the shape, and that can be
          done with a gradient or a pattern, instead of with a solid color.</p>

        <p>Finally, a string of text can be considered to be a shape for the purpose
          of drawing it. The boundary of the shape is the outline of the characters. The text is drawn
          by filling that shape. </p>

        <p>In some graphics systems, it is also possible to stroke the outline of the
          shape that defines the text.</p>

        <p>In the following illustration, the string "Graphics" is shown, on
          top, filled with a pattern and, below that, filled with a gradient and stroked with solid black:</p>


        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/stroke%20and%20fill%20text.png?raw=true"
          alt="Stroke and Fill Text">







      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="JavaScript">
      <br />
      <header><b>JavaScript</b></header>
      <article>
        <p>JavaScript is a dynamic programming language that's used for web development, in web applications, for game
          development, and lots more. It allows we to implement dynamic features on web pages that cannot be done with
          only HTML and CSS.</p>

        <p>Many browsers use JavaScript as a scripting language for doing dynamic things on the web. Any time we see a
          click-to-show dropdown menu, extra content added to a page, and dynamically changing element colors on a page,
          to name a few features, we're seeing the effects of JavaScript.</p>

        <h2>How JavaScript Makes Things Dynamic</h2>
        <p>HTML defines the structure of our web document and the content therein. CSS declares various styles for the
          contents provided on the web document.</p>

        <p>HTML and CSS are often called markup languages rather than programming languages, because they, at their
          core, provide markups for documents with very little dynamism.</p>

        <p>JavaScript, on the other hand, is a dynamic programming language that supports Math calculations, allows we
          to dynamically add HTML contents to the DOM, creates dynamic style declarations, fetches contents from another
          website, and lots more.</p>

        <p>Here's a basic breakdown of JavaScript fundamentals:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/javascript%20cheat%20sheet.png?raw=true"
          alt="JavaScript Cheat Sheet" />


      </article>
      <br />
    </section>

    <hr />




    <section class="main-section" id="Intro_to_HTML5_Canvas">
      <br />
      <header><b>HTML5 Canvas</b></header>
      <article>
        <p>HTML5 (Hypertext Markup Language 5) is a markup language used for structuring and presenting hypertext
          documents on
          the World Wide Web. It was the <b>fifth and final major HTML version</b> that is now a retired World Wide Web
          Consortium (W3C)
          recommendation.
          The current specification is known as the <b>HTML Living Standard</b>.</p>

        <p>Canvas is a new element in HTML5, which provides APIs used to draw raster graphics on a web application.
          The presence of the Canvas API for HTML5, strengthens the HTML5 platform by providing two drawing contexts: 2D
          and 3D.
          These capabilities are supported on most modern operating systems and browsers.</p>

        <p>However, we will begin with 2D graphics.</p>

        <p>The <b>HTML5 canvas element</b> is used to draw graphics, on the fly, via JavaScript. The canvas element is
          only a container
          for graphics. We must use a <b>script</b> to actually draw the graphics.</p>

        <p>Canvas has several methods for drawing paths, boxes, circles, text, and adding images.</p>

        <p>HTML Canvas can:</p>

        <ul>
          <li><b>draw colorful text</b>, with or without animation</li>
          <li><b>draw graphics</b> using great features for graphical data presentation with an imagery of graphs and
            charts
          </li>
          <li><b>be animated</b> - everything is possible: from simple bouncing balls to complex animations</li>
          <li><b>be interactive</b> - canvas can respond to JavaScript events to any user action (key clicks, mouse
            clicks,
            button clicks, finger movement)</li>
          <li><b>be used in games</b> - canvas' methods for animations, offer a lot of possibilities for HTML gaming
            applications</li>
        </ul>

        <p>Here is an example of a canvas element:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;&lt;/canvas&gt;</code></pre>

        <ul>
          <li>The <b>id attribute is required</b> (so it can be referred to by JavaScript)</li>
          <li>The width and height attribute defines the size of the canvas (the default size of the canvas is 300px
            (width) x 150px (height))</li>
          <li>The canvas element requires the closing tag</li>
          <p>Unlike the &lt;img&gt; element, The &lt;canvas&gt; element requires the closing tag &lt;/canvas&gt;. Any
            content between the
            opening and closing tags is fallback content that will display only if the browser doesn't support the
            &lt;canvas&gt; element.</p>
          <p>For example:</p>
          <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400"&gt;The browser doesn't support the canvas element&lt;/canvas&gt;</code></pre>
          <p>However, nowadays, most modern web browsers support the &lt;canvas&gt; element.</p>
          <li>We can have multiple &lt;canvas&gt; elements on one HTML page.</li>
          <li>By default, the &lt;canvas&gt; element has no border and no content.</li>
        </ul>

        <p>Dimensions of canvas element can either be set statically in HTML, or dynamically using JavaScript, or a
          combination of both.</p>

        <p>To add a border, use a style attribute:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <p>Canvas consists of a drawable region defined in HTML code with height and width attributes.
          JavaScript code may access the area through a full set of drawing functions, allowing for dynamically
          generated graphics.</p>

        <p>The drawing on the canvas is done with JavaScript.</p>

        <p>The canvas is initially blank. To display something, a script is needed to access the rendering context and
          draw on it.</p>

        <p>The following example draws a red rectangle on the canvas, from position (0,0) with a width of 150 and a
          height of 75:</p>

        <h3>Step 1: Find the Canvas Element</h3>

        <p>Initially, the canvas is blank. To draw something, We need to access the rendering context and use it to
          draw on the canvas.</p>

        <p>First, we need to find the &lt;canvas&gt; element.</p>

        <p>We access a &lt;canvas&gt; element with the HTML DOM method getElementById():</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");</code></pre>

        <p>The getElementById() method of the Document interface returns an Element object representing the element
          whose id property matches the specified string.</p>

        <p>To set the dimensions dynamically with JavaScript, we can access the width and height as follows: </p>

        <pre><code class="javascript">//To set width and height of current viewport
canvas.width = window.innerWidth; //or to set a specific width i.e 200
canvas.height = window.innerHeight; //or to set a specific height i.e 300</code></pre>

        <h3>Step 2: Create a Drawing Object</h3>

        <p>Secondly, we need a drawing object for the canvas.</p>

        <p>The getContext() method returns an object with tools (properties and methods) for drawing:</p>

        <pre><code class="javascript">const ctx = canvas.getContext("2d");</code></pre>

        <h3>Step 3: Draw on the Canvas</h3>

        <p>Finally, we can draw on the canvas.</p>

        <p>Set the fill-color to red with the fillStyle property:</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)";</code></pre>

        <p>The fillStyle property can be a color, a gradient, or a pattern. The default fillStyle is black.</p>

        <p>The fillRect(x, y, width, height) method draws the rectangle, filled with the fill style color, on the
          canvas:</p>

        <pre><code class="javascript">ctx.fillRect(0, 0, 150, 75);</code></pre>

        <h2>Canvas Fill and Stroke</h2>

        <p>To define fill-color and outline-color for shapes/objects in canvas, we use the following properties:</p>
        <ul>
          <li>fillStyle - Defines the color, gradient, or pattern used to fill shapes</li>
          <li>strokeStyle - Defines the color, gradient, or pattern used for strokes</li>
        </ul>

        <h3>The fillStyle Property</h3>

        <p>The fillStyle property defines the fill-color of the object.</p>
        <p>The fillStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
          
//How can we set the fill-color to blue? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>The strokeStyle Property</h3>

        <p>The strokeStyle property defines the color of the outline.</p>

        <p>The strokeStyle property value can be a color (colorname, RGB, HEX, HSL), a gradient or a pattern.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
                      
//How can we set the stroke-color to yellow? 
ctx.fillRect(10,10, 100,100);</code></pre>

        <h3>Combining fillStyle and strokeStyle</h3>

        <p>It is perfectly legal to combine the previous two rectangles: </p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// the filled rectangle
ctx.fillStyle = "rgb(0 0 255)";
ctx.fillRect(10,10, 100,100);

// the outline rectangle
ctx.strokeStyle = "rgb(255 255 0)";
ctx.strokeRect(10,10, 100,100);</code></pre>

        <h3>Gradients</h3>

        <p>Gradients let us display smooth transitions between two or more specified colors.</p>

        <p>Gradients can be used to fill rectangles, circles, lines, text, etc.</p>

        <p>There are two methods used for creating gradients:</p>

        <ul>
          <li>createLinearGradient() - creates a linear gradient</li>
          <li>createRadialGradient() - creates a radial/circular gradient</li>
        </ul>

        <h3>Linear Gradient</h3>

        <p>The createLinearGradient() method is used to define a linear gradient.</p>

        <p>A linear gradient changes color along a linear pattern (horizontally/vertically/diagonally).</p>

        <p>The createLinearGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting point</li>
          <li>y0 - The y-coordinate of the starting point</li>
          <li>x1 - The x-coordinate of the ending point</li>
          <li>y1 - The y-coordinate of the ending point</li>
        </ul>

        <p>The gradient object requires two or more color stops.</p>

        <p>The addColorStop() method specifies the color stops, and its position along the gradient. The positions can
          be anywhere between 0 and 1.</p>

        <p>To use the gradient, assign it to the fillStyle or strokeStyle property, then draw the shape (rectangle,
          circle, shape, or text).</p>

        <p>Here is an example of a linear gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create gradient
const grd = ctx.createLinearGradient(0, 0, 200, 0);
grd.addColorStop(0, "red");
grd.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd;
ctx.fillRect(10, 10, 150, 80);</code></pre>

        <h3>Radial Gradient</h3>

        <p>The createRadialGradient() method is used to create a radial/circular gradient.</p>

        <p>A radial gradient is defined by two circles, one smaller and one larger.</p>

        <p>The createRadialGradient() method has the following parameters:</p>

        <ul>
          <li>x0 - The x-coordinate of the starting circle</li>
          <li>y0 - The y-coordinate of the starting circle</li>
          <li>r0 - The radius of the starting circle</li>
          <li>x1 - The x-coordinate of the ending circle</li>
          <li>y1 - The y-coordinate of the ending circle</li>
          <li>r1 - The radius of the ending circle</li>
        </ul>

        <p>Here is an example of a radial gradient:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");

const ctx = canvas.getContext("2d");

// Create gradient
const grd2 = ctx.createRadialGradient(85, 140, 0, 85, 140, 100);

// Add color stops
grd2.addColorStop(0, "red");
grd2.addColorStop(1, "white");

// Fill with gradient
ctx.fillStyle = grd2;
ctx.fillRect(10, 100, 150, 80);</code></pre>

        <h3>Patterns</h3>

        <p>Patterns are used to fill shapes with images (instead of colors).</p>

        <p>There are two methods used for creating patterns:</p>

        <ul>
          <li>createPattern(image, type) - creates a pattern from an image</li>
          <li>createPattern(canvas, type) - creates a pattern from another canvas</li>
        </ul>

        <p>The createPattern() method has the following parameters:</p>

        <ul>
          <li>image - Specifies the image to use</li>
          <li>type - Repeat the pattern (repeat, repeat-x, repeat-y, no-repeat)</li>
        </ul>

        <p>Here is an example of a pattern:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

// Create a pattern
const img = new Image();
img.src = "https://www.w3schools.com/tags/img_the_scream.jpg";
img.onload = function() {
  const pat = ctx.createPattern(img, "repeat");
  ctx.fillStyle = pat;
  ctx.fillRect(10, 10, 150, 80);
};</code></pre>

        <h3>The clearRect() Method</h3>

        <p>The clearRect() method is used to clear a rectangular area of the canvas. The cleared rectangle is
          transparent.</p>

        <p>The clearRect() method has the following parameters:</p>

        <ul>
          <li>x - The x-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>y - The y-coordinate of the upper-left corner of the rectangle to clear</li>
          <li>width - The width of the rectangle to clear (in pixels)</li>
          <li>height - The height of the rectangle to clear (in pixels)</li>
        </ul>


        <p>Here we use fillRect() to draw a filled 150*100 pixels rectangle, starting in position (10,10). Then use
          clearRect() to clear a rectangular area in the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "pink";
ctx.fillRect(10,10,150,100);
            
ctx.clearRect(60,35,50,50);</code></pre>

        <p>And here is an example of clearing the entire canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas"); 
const ctx = canvas.getContext("2d");

// Clear the canvas
ctx.clearRect(0, 0, canvas.width, canvas.height);</code></pre>



        <p></p>

        <h2>Canvas Coordinates</h2>

        <p>As previously mentioned, The HTML canvas is a two-dimensional grid.</p>


        <p>It is important to understand the coordinate space of canvas, if We want elements to be positioned as
          desired. Top left of canvas represents (0,0) or origin coordinate.</p>

        <p>All the elements on canvas are placed with reference to this origin.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinate%20grid%20space.png?raw=true"
          alt="The Grid or Coordinate Space 1">

        <p>1 point on grid is roughly equivalent to 1px.</p>

        <p>At the example above further elaborates: we have red border around our canvas and we have drawn a rectangle
          of 100px width and height with a stroke of blue.</p>

        <p>This can be achieved by the following code: </p>

        <h3>HTML</h3>

        <pre><code class="html">&lt;canvas id="myCanvas" width="320" height="320" style="border:1px solid rgb(255,0,0);"&gt;&lt;/canvas&gt;</code></pre>

        <h3>JavaScript</h3>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");
ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(0, 0, 100, 100); //draws a rectangle with a blue stroke, starting at (0,0) with a width and height of 100 pixels</code></pre>

        <p>Providing x and y coordinates would translate the element relative to the canvas' origin coordinates.</p>
        <p>As shown in the image below, our rectangle has moved 20 pixels to the right and bottom as we have provided
          the values of x and y as 20.</p>


        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/coordinate%20grid%20space%202.png?raw=true"
          alt="The Grid or Coordinate Space 2">

        <h3>Let's Look and Real Time Coordinates</h3>

        <p class="codepen" data-height="600" data-theme-id="dark" data-default-tab="result" data-slug-hash="emOoLbp"
          data-pen-title="Understanding Coordinates" data-user="amaraauguste"
          style="height: 300px; box-sizing: border-box; display: flex; align-items: center; justify-content: center; border: 2px solid; margin: 1em 0; padding: 1em;">
          <span>See the Pen <a href="https://codepen.io/amaraauguste/pen/emOoLbp">
              Understanding Coordinates</a> by Amara (<a href="https://codepen.io/amaraauguste">@amaraauguste</a>)
            on <a href="https://codepen.io">CodePen</a>.</span>
        </p>
        <script async src="https://public.codepenassets.com/embed/index.js"></script>

        <h2>Shapes</h2>

        <p>It is easy to draw basic shapes like rectangle, triangle, square, circle, polygon or a just a simple line
          between two points. But by default Canvas provides a method only to draw rectangle.</p>

        <p>However, rest of shapes can be created by joining points using path API, and a combination of line and arc
          APIs.</p>

        <h3>Rectangle</h3>

        <p>The three most used methods for drawing rectangles in canvas are:</p>

        <ul>
          <li>The rect(x, y, width, height) method</li>
          <li>The fillRect(x, y, width, height) method</li>
          <li>The strokeRect(x, y, width, height) method</li>
        </ul>

        <p>The rect() method defines a rectangle. Note: the rect() method does not draw the rectangle (it just defines
          it). So, in addition, We have to use the stroke() method (or the fill() method) to actually draw it.</p>


        <p>fill and stroke are ink methods and each case it means to draw a rectangle with a filled color, or to draw a
          rectangular outline of a color. The default color is black.</p>

        <pre><code class="javascript">ctx.fillStyle = "rgb(255 0 0)"; //red color for fill
ctx.fillRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a red rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <p>Similarly, we can draw a rectangle with a stroke:</p>

        <pre><code class="javascript">ctx.strokeStyle = "rgb(0 0 255)"; //blue color for stroke
ctx.strokeRect(20, 20, 150, 100);</code></pre>

        <p>Here, we have drawn a blue rectangle with a top-left corner at (20, 20) and a width and height of 150 and 100
          pixels respectively.</p>

        <h3>Circle</h3>

        <p>As we mentioned earlier there is no straight forward method to create a circle, but we can use a combination
          of path APIs and arc method to draw our circle. Let's understand a little more about path:</p>

        <p>"A path is list of points connected to form different shapes."</p>

        <p>This means a path can be formed between two given points on screen. It can be a straight line or curved arc
          or can be any shape or color.</p>

        <p>There are three steps to follow to create a shape using path:</p>

        <ol>
          <li>Invoke beingPath() method on context a create a new path. Once a path is created all future commands to
            draw are applied on
            this path.</li>
          <li>Next create a path using drawing methods, like lineTo, moveTo, arc, rect, etc.
            <a href="https://developer.mozilla.org/en-US/docs/Web/API/CanvasRenderingContext2D#paths">Refer MDN</a> for
            list of all
            available methods that used with path.
          </li>
          <li>Once path has been created it needs to actually be rendered on canvas; we can do that using ink methods
            fill and stroke.</li>
        </ol>

        <p>Let's draw a circle:</p>

        <p>We have to use arc(x, y, radius, startAngle, endAngle) method on context to draw our circle.</p>

        <p> If we try to recollect basic geometry, to draw a circle using a protractor we need a radius, and a start &
          end angle. A semi-circle starts at angle 0 and ends at 180 degree or PI radians. So a full-circle extends
          further and just ends at 2*PI or 360 degrees.</p>

        <p>This exact concept can be used to draw a circle using arc method.</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, 2 * Math.PI);
ctx.stroke();</code></pre>

        <p>Here, we have drawn a circle with a center at (100, 75) and a radius of 50 pixels.</p>

        <h3>Draw a Half Circle</h3>

        <p>To draw a half circle, we change the endAngle to PI (not 2 * PI):</p>

        <pre><code class="javascript">ctx.beginPath(); //start a new path
ctx.arc(100, 75, 50, 0, Math.PI);
ctx.fill();</code></pre>

        <h3>More About the Angles of an Arc</h3>

        <p>The following image shows some of the angles in an arc:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/angles%20of%20an%20arc.png?raw=true"
          alt="Angles of an Arc">

        <ul>
          <li>Center: arc(<text style="color: rgb(0, 223, 0)">100, 75</text>, 50, 0 * Math.PI, 1.5 * Math.PI)</li>
          <li>Start angle: arc(100, 75, 50, <text style="color: red">0</text>, 1.5 * Math.PI)</li>
          <li>End angle: arc(100, 75, 50, 0 * Math.PI, <text style="color: blue">1.5 * Math.PI</text>)</li>
          <li>Circle : arc(100, 75, 50, 0, 2 * Math.PI)</li>
        </ul>

        <h3>Line</h3>

        <p>To draw a line between two points we use moveTo(x, y)and lineTo(x, y)methods.</p>

        <p>If we consider two points A & B with x and y coordinates respectively,
          then moveTo acts as a position of A on canvas, while lineTo as position of point B.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line
ctx.stroke();</code></pre>

        <h3>The lineWidth Property</h3>

        <p>The lineWidth property defines the width of the line.</p>

        <p>It must be set before calling the stroke() method.</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(250, 50); //start point
ctx.lineTo(200, 100); //end point
ctx.strokeStyle = "rgb(255 105 180)"; //color of line 
ctx.lineWidth = 10; //width of line
ctx.stroke();</code></pre>


        <h3>Triangle</h3>

        <p>A triangle is simply three lines connected together.</p>

        <p>So, to draw a triangle we can use lineTo method to connect three points.</p>

        <p>We are going to use a special path method called as closePath() to complete our triangle. closePath basically
          adds a straight line from end coordinate to the start coordinate inside a path.</p>

        <p>If we assume a triangle is made of three points A, B & C, then we can draw our triangle like:</p>

        <pre><code class="javascript">ctx.beginPath();
ctx.moveTo(235, 114); // Point A
ctx.lineTo(135, 349); // Point B
ctx.lineTo(335, 349); // Point C
            
ctx.closePath(); // Join C & A
ctx.strokeStyle = "rgb(31 24 88)";
ctx.stroke();</code></pre>

        <h3>To Summarize Basic Shape Drawing</h3>

        <p>Apart from drawing specific rectangles and circles, drawing must be broken down into four distinct steps:</p>

        <ol>
          <li>ctx.beginPath(), to let the computer know We're beginning a new line/path</li>
          <li>ctx.moveTo(x, y), to move the 'cursor' to a specific point on the canvas without 'drawing' anything or
            recording any path</li>
          <li>ctx.lineTo(x, y), tells the computer to record a path from the current context position, in this case the
            point described by the ctx.moveTo(x, y) function—to the new coordinates provided</li>
          <li>ctx.stroke(), to then fill the described path. This is the step that actually 'draws' something onto the
            canvas</li>
        </ol>

        <p>In essence, We move the cursor to a starting position, tell the computer We're about to draw, record a path
          to a declared location, and then finally fill that path in.</p>

        <h2>Drawing Text</h2>

        <p>To draw text on the canvas, the most important property and methods are:</p>

        <ul>
          <li>font - defines the font properties for the text</li>
          <li>fillText(text, x, y) - fills a given text at the given (x, y) position</li>
          <li>strokeText(text, x, y) - strokes a given text at the given (x, y) position</li>
        </ul>

        <p>The font property defines the font to be used and the size of the font. The default value for this property
          is "10px sans serif".</p>

        <p>Both methods include an optional fourth parameter: maxwidth, which represents the maximum width of the
          text-string.</p>

        <p>Here is an example of drawing text on a canvas:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.font = "30px Arial";
ctx.fillText("Hello World", 10, 50);</code></pre>

        <p>The fillText() method draws filled text on the canvas. The default color of the text is black.</p>

        <p>The strokeText() method draws text on the canvas (no fill). The default color of the text is black.</p>


        <p><a href="https://codepen.io/amaraauguste/pen/KwPYByQ" target="_blank">Let's take a look at some example
            code</a></p>

        <hr />

        <h2>Exercise: Smiley Face</h2>

        <p>Knowing what we know now about drawing shapes in Canvas</p>

        <p>and given a canvas sized 350x350 ... </p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="350" height="350" style="border:1px solid rgb(0,0,0);"&gt;&lt;/canvas&gt;</code></pre>


        <p>How can we can create this smiley face in the center of the canvas?</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/smiley%20face%20canvas%20example.png?raw=true"
          alt="Smiley Face">


      </article>
      <br />
    </section>


    <hr />
    <section class="main-section" id="Polygons_and_Curves">
      <br />
      <header><b>Polygons and Curves</b></header>
      <article>
        <p>It is impossible for a graphics API to include every possible shape as a basic shape, but there is
          usually some way to create more complex shapes.</p>

        <p>For example, consider polygons. A <b>polygon</b>
          is a closed shape consisting of a sequence of line segments.</p>

        <p>Each line segment is joined to the
          next at its endpoint, and the last line segment connects back to the first. The endpoints are
          called the vertices of the polygon, and a polygon can be defined by listing its vertices.</p>

        <p>In a <b>regular polygon</b>, all the sides are the same length and all the angles between sides are
          equal. Squares and equilateral triangles are examples of regular polygons.</p>

        <p> A <b>convex polygon</b>
          has the property that whenever two points are inside or on the polygon, then the entire line
          segment between those points is also inside or on the polygon. Intuitively, a convex polygon
          has no "indentations" along its boundary. (Concavity can be a property of any shape, not just
          of polygons.)
        </p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/convex%20polygons.png?raw=true"
          alt="Convex Polygons">


        <p>Sometimes, polygons are required to be "simple", meaning that the polygon has no selfintersections. That is,
          all the vertices are different, and a side can only intersect another
          side at its endpoints.</p>

        <p>And polygons are usually required to be "planar", meaning that all the
          vertices lie in the same plane. (Of course, in 2D graphics, everything lies in the same plane, so
          this is not an issue. However, it does become an issue in 3D.)
        </p>


        <p>How then should we draw polygons? That is, what capabilities would we like to have in a
          graphics API for drawing them</p>

        <p>One possibility is to have commands for stroking and for filling
          polygons, where the vertices of the polygon are given as an array of points or as an array of
          x-coordinates plus an array of y-coordinates.</p>

        <p>In fact, that is sometimes done; for example, the
          Java graphics API includes such commands. </p>

        <p>Another, more flexible, approach is to introduce
          the idea of a "path."</p>

        <p>Java, SVG, and the HTML canvas API all support this idea. A path is
          a general shape that can include both line segments and curved segments. Segments can, but
          don't have to be, connected to other segments at their endpoints. </p>

        <p>A path is created by giving a series of commands that tell, essentially, how a pen would be moved to draw the
          path.</p>

        <p>While
          a path is being created, there is a point that represents the pen’s current location. There will
          be a command for moving the pen without drawing, and commands for drawing various kinds
          of segments</p>

        <p>For drawing polygons, we need commands such as: </p>

        <ul>
          <li><b>beginPath()</b> — start a new, empty path</li>
          <li><b>moveTo(x,y)</b> — move the pen to the point (x,y), without adding a segment to the path;
            that is, without drawing anything</li>
          <li><b>lineTo(x,y)</b> — add a line segment to the path that starts at the current pen location
            and ends at the point (x,y), and move the pen to (x,y)</li>
          <li><b>closePath()</b> — add a line segment from the current pen location back to the starting
            point, unless the pen is already there, producing a closed path.</li>
        </ul>

        <p>(For closePath, I need to define “starting point.” A path can be made up of “subpaths” A
          subpath consists of a series of connected segments. A moveTo always starts a new subpath.
          A closePath ends the current segment and implicitly starts a new one. So “starting point”
          means the position of the pen after the most recent moveTo or closePath.)
        </p>

        <p>Suppose that we want a path that represents a five sided polygon (a pentagon!).</p>

        <p>First of all, let's consider the unit circle. This is a circle centred at (0, 0) with radius of 1 unit.</p>


        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%201.png?raw=true"
          alt="Pentagon 1">

        <p>A regular polygon, such as a pentagon, can be drawn inside the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%202.png?raw=true"
          alt="Pentagon 2">

        <p>In order to draw the pentagon we need to be able to identify the 5 points on the unit circle and rotate and
          draw lines between them.</p>

        <p>This is where some understanding of trigonometry comes in useful.</p>

        <p>Let's consider some point, (a, b) on the unit circle as follows:</p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%203.png?raw=true"
          alt="Pentagon 3">

        <p>We know the radius (r), in this case it is 1 because it is the unit circle. However, it could be any length
          we choose.</p>

        <p>The point (a, b) can be written in terms of trigonometric ratios as follows:</p>

        <p>The x-ordinate is given by a = <b>r cos 𝛳</b></p>

        <p>The y-ordinate is given by b = <b>r sin 𝛳</b></p>

        <p>In Javascript we can identify the first point as:</p>

        <text class="center"><b>(x + radius * Math.cos(angle), y + radius * Math.sin(angle))</b></text>

        <p>Note that we add the (x, y) ordinate values since we will not necessarily be centering the circle at (0, 0).
        </p>

        <p>Since we are drawing a pentagon we know that the angle we will need to rotate through will be 360o / 5.
          However, all angles must be given in radians. So the angle will be 2*Pi / 5. In Javascript this is written as:
        </p>

        <text class="center"><b>angle = 2*Math.PI/numberOfSides</b></text>

        <p>We can now declare some variables:</p>

        <ul>
          <li>Since we are drawing a pentagon we set the number of sides to 5</li>
          <li>We define a radius for our circle. The pentagon will be drawn inside the circle, each vertex of the
            pentagon will be on the circumference of the circle.</li>
          <li>We set the x-ordinate of the centre of the circle</li>
          <li>We set the y-ordinate of the centre of the circle</li>
          <li>We calculate the size of the external angle of the pentagon. This is the angle we will need to rotate
            through after each line is drawn</li>
        </ul>

        <p>We can now begin the path and set up a loop to draw each line of the polygon:</p>

        <p>We move to the first point which is directly across from the centre of the circle (indicated in red below):
        </p>



        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/pentagon%204.png?raw=true"
          alt="Pentagon 4">

        <p>We set up a loop to draw each line and finally stroke the path when we are done.</p>

        <pre><code class="javascript">ctx.beginPath(); // start a new path
let numberOfSides = 5; // a pentagon
let radius=100; // radius of the circle
let x = 125; // center (x) of the circle
let y = 125; // center (y)  of the circle
let angle = 2*Math.PI/numberOfSides; // angle between sides
ctx.beginPath(); // start a new path
ctx.moveTo (x + radius*Math.cos(0), y + radius*Math.sin(0)); // first vertex      
for (let i = 1; i <= numberOfSides; i++) { // loop through each vertex
  ctx.lineTo (x + radius*Math.cos(i * angle), y + radius*Math.sin(i * angle)); // draw line to next vertex
}
ctx.stroke();</code></pre>

        <h2>Curves</h2>

        <p>As noted above, a path can contain other kinds of segments besides lines.</p>

        <p>For example,
          it might be possible to include an arc of a circle as a segment. </p>

        <p>Another type of curve is a
          <b>Bezier curve</b>. Bezier curves can be used to create very general curved shapes. They are fairly
          intuitive, so that they are often used in programs that allow users to design curves interactively.
        </p>

        <p>Mathematically, Bezier curves are defined by parametric polynomial equations, but you don’t
          need to understand what that means to use them.</p>

        <p>There are two kinds of Bezier curve in
          common use, cubic Bezier curves and quadratic Bezier curves; they are defined by cubic and
          quadratic polynomials respectively.</p>

        <p>When the general term "Bezier curve" is used, it usually
          refers to cubic Bezier curves.</p>

        <p>A cubic Bezier curve segment is defined by the two endpoints of the segment together with
          two control points. To understand how it works, it's best to think about how a pen would
          draw the curve segment.</p>

        <p>The pen starts at the first endpoint, headed in the direction of the
          first control point. The distance of the control point from the endpoint controls the speed of
          the pen as it starts drawing the curve. The second control point controls the direction and
          speed of the pen as it gets to the second endpoint of the curve. There is a unique cubic curve
          that satisfies these conditions.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/curves.png?raw=true"
          alt="Bezier Curves">


        <p>The illustration above shows three cubic Bezier curve segments.</p>

        <p>The two curve segments on
          the right are connected at an endpoint to form a longer curve. The curves are drawn as thick
          black lines. The endpoints are shown as black dots and the control points as blue squares, with
          a thin red line connecting each control point to the corresponding endpoint. (Ordinarily, only
          the curve would be drawn, except in an interface that lets the user edit the curve by hand.)</p>

        <p>Note that at an endpoint, the curve segment is tangent to the line that connects the endpoint
          to the control point. Note also that there can be a sharp point or corner where two curve
          segments meet. However, one segment will merge smoothly into the next if control points are
          properly chosen.
        </p>

        <!--ADD EXAMPLE -->

        <p><b>Quadratic Bezier</b> curve segments are similar to the cubic version, but in the quadratic case,
          there is only one control point for the segment. The curve leaves the first endpoint heading
          in the direction of the control point, and it arrives at the second endpoint coming from the
          direction of the control point. The curve in this case will be an arc of a parabola.</p>

        <p>The three most used methods for drawing curves in canvas are:</p>

        <ul>
          <li>The arc() method (which we learned to use to draw circles)</li>
          <li>The quadraticCurveTo() method</li>
          <li>The bezierCurveTo() method</li>
        </ul>

        <h3>The quadraticCurveTo() Method</h3>

        <p>The quadraticCurveTo() method is used to define a quadratic Bezier curve.</p>

        <p>The quadraticCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx - The x-coordinate of the control point</li>
          <li>cpy - The y-coordinate of the control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a quadratic Bezier curve:</p>

        <pre><code class="javascript"
>const canvas = document.getElementById("myCanvas");  
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.quadraticCurveTo(60, 10, 100, 100);
ctx.stroke();</code></pre>

        <h3>The bezierCurveTo() Method</h3>

        <p>The bezierCurveTo() method is used to define a cubic Bezier curve.</p>

        <p>The bezierCurveTo() method has the following parameters:</p>

        <ul>
          <li>cpx1 - The x-coordinate of the first control point</li>
          <li>cpy1 - The y-coordinate of the first control point</li>
          <li>cpx2 - The x-coordinate of the second control point</li>
          <li>cpy2 - The y-coordinate of the second control point</li>
          <li>x - The x-coordinate of the end point</li>
          <li>y - The y-coordinate of the end point</li>
        </ul>

        <p>Here is an example of drawing a cubic Bezier curve:</p>

        <pre><code class="javascript
">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.beginPath();
ctx.moveTo(20, 100);
ctx.bezierCurveTo(20, 10, 100, 10, 100, 100);
ctx.stroke();</code></pre>

        <p></p>




      </article>
      <br />
    </section>

    <hr />


    <section class="main-section" id="Mouse_Events">
      <br />
      <header><b>Mouse Events</b></header>
      <article>

        <p>Okay, so we can draw a some shapes, lines, text, and curves. That's great and all, but how do we get from
          that to actually drawing on the
          screen with our mouse?</p>

        <p>Since our use of Canvas is beginning to get more complex, it might help to start using functions.</p>

        <h2>JavaScript Functions</h2>

        <p>Functions are a way to group together a set of instructions that perform a specific task. They allow us to
          reuse code, make our code more organized, and easier to read and maintain.</p>

        <p>Let's start by creating a function that will draw a line on the canvas. We'll call this function drawLine.
        </p>

        <pre><code class="javascript">function drawLine(x1, y1, x2, y2) {
  ctx.beginPath(); // Start a new path
  ctx.moveTo(x1, y1); // Move the pen to the starting point
  ctx.lineTo(x2, y2); // Draw a line to the ending point
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take four arguments: x1, y1, x2, and y2, which represent the starting and ending points
          of
          the line. We then use the canvas API to draw a line between these two points.</p>

        <p>Now that we have our drawLine function, we can use it to draw lines on the canvas. For example:</p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)</code></pre>

        <p>Now, let's create a function that will draw a circle on the canvas. We'll call this function drawCircle.</p>

        <pre><code class="javascript">function drawCircle(x, y, radius) {
  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, radius, 0, Math.PI * 2); // Draw a circle
  ctx.stroke(); // Stroke the path
}</code></pre>

        <p>In this function, we take three arguments: x, y, and radius, which represent the center of the circle and its
          radius. We then use the canvas API to draw a circle at the specified location.</p>

        <p>Now that we have our drawCircle function, we can use it to draw circles on the canvas. For example:</p>

        <pre><code class="javascript">drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>By creating functions like drawLine and drawCircle, we can easily draw shapes on the canvas and reuse our
          code
          to create more complex drawings.</p>

        <p>Now that we know the structure of a basic JavaScript function, let's talk about event listeners.</p>

        <h2>Event Listeners</h2>

        <p>Event listeners are a way to listen for and respond to events that occur in the browser. They allow us to
          create interactive web applications that respond to user actions such as clicks, key presses, and mouse
          movements.</p>

        <p>When an event occurs, the browser triggers the event listener, which calls a function that performs a
          specific
          action. This allows us to create dynamic and interactive web pages that respond to user input.</p>

        <p>There are many different types of events that can be listened for, such as:</p>

        <ul>
          <li>Click events - Triggered when an element is clicked</li>
          <li>Mouseover events - Triggered when the mouse pointer enters an element</li>
          <li>Keydown events - Triggered when a key is pressed down</li>
          <li>Submit events - Triggered when a form is submitted</li>
          <li>Scroll events - Triggered when the user scrolls the page</li>
        </ul>

        <p>Event listeners are added to elements in the DOM using the addEventListener method. This method takes two
          arguments: the name of the event to listen for, and the function to call when the event occurs.</p>

        <p>For example, to listen for a click event on a button element, we can use the following code:</p>

        <pre><code class="javascript">const button = document.getElementById('myButton'); // Get the button element   
button.addEventListener('click', () => { // Listen for click event
  console.log('Button clicked!'); // Log a message to the console
});</code></pre>

        <p>Let's add a button to clear the canvas: </p>

        <p>First, let's create a new canvas and a button:</p>

        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;
&lt;button id="clearButton"&gt;Clear Canvas&lt;/button&gt;</code></pre>

        <p>Next, let's add an event listener to the button that clears the canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas'); // Get the canvas element
const ctx = canvas.getContext('2d'); // Get the 2D drawing context

const clearButton = document.getElementById('clearButton'); // Get the clear button element

clearButton.addEventListener('click', () => { // Listen for click event on clear button
  ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
});</code></pre>

        <p>In this example, we get the canvas element and its 2D drawing context, as well as the clear button element.
          We
          then add an event listener to the clear button that calls the clearRect method on the canvas context to clear
          the
          canvas when the button is clicked.</p>

        <p>So let's draw some shapes on the canvas: </p>

        <pre><code class="javascript">drawLine(100, 100, 200, 200); // Draw a line from (100, 100) to (200, 200)
drawCircle(150, 150, 50); // Draw a circle with center at (150, 150) and radius of 50</code></pre>

        <p>And now we can clear the canvas!</p>

        <h2>Drawing with the Mouse</h2>

        <p>So far, we've used JavaScript to draw shapes on the canvas and an event listener to add button functionalty
          to clear the canvas, but that's not really how we draw, is it?</p>

        <p>A good drawing application has to respond to the mouse. We need to be able to draw lines and shapes by
          clicking and dragging the mouse.</p>

        <p>We need a listener that responds to the canvas being clicked, and another listener that responds to mouse
          movement, but only when the mouse button is pressed down.</p>

        <p>Mouse events in JavaScript allow us to create interactive and dynamic web applications. By capturing mouse
          events, we can respond to user actions such as clicks, movements, and drags. This is particularly useful when
          working with the HTML5 canvas element, as it enables us to create drawing applications, games, and other
          interactive graphics.</p>

        <p>JavaScript provides several mouse events that we can listen for:</p>

        <ul>
          <li><b>mousedown</b> - Triggered when the mouse button is pressed down.</li>
          <li><b>mouseup</b> - Triggered when the mouse button is released.</li>
          <li><b>mousemove</b> - Triggered when the mouse is moved.</li>
          <li><b>click</b> - Triggered when the mouse button is clicked (pressed and released).</li>
          <li><b>dblclick</b> - Triggered when the mouse button is double-clicked.</li>
        </ul>

        <h2>Using Mouse Events with HTML Canvas</h2>

        <p>To use mouse events with the HTML5 canvas, we need to add event listeners to the canvas element. These event
          listeners will call functions that handle the events and perform actions such as drawing on the canvas.</p>

        <h3>Determining Mouse Position</h3>

        <p>To determine the mouse position within the canvas, we need to account for the canvas's position relative to
          the viewport. This can be done using the <b>getBoundingClientRect()</b> method, which returns the size of an
          element and its position relative to the viewport.</p>

        <p>Here is an example of how to determine the mouse position within the canvas:</p>

        <pre><code class="javascript">function getMousePos(canvas, event) {
const rect = canvas.getBoundingClientRect(); // Get the size and position of the canvas
  return {
    x: event.clientX - rect.left, // X coordinate of mouse relative to canvas
    y: event.clientY - rect.top // Y coordinate of mouse relative to canvas
  };
}

// Example usage
canvas.addEventListener('mousemove', (event) => { // Listen for mousemove event
  const mousePos = getMousePos(canvas, event); // Get the mouse position
  console.log('Mouse position: ' + mousePos.x + ',' + mousePos.y); // Log the mouse position
});</code></pre>

        <p>In this example:</p>

        <ul>
          <li>The <b>getMousePos</b> function takes the canvas element and the mouse event as arguments.</li>
          <li>It uses <b>getBoundingClientRect()</b> to get the position of the canvas relative to the viewport.</li>
          <li>It calculates the mouse position by subtracting the canvas's top-left corner coordinates from the mouse's
            <b>clientX</b> and <b>clientY</b> coordinates. The <b>clientX</b> and <b>clientY</b> properties of the mouse
            event provide the horizontal and vertical coordinates of the mouse pointer, respectively, relative to the
            viewport (the visible area of the browser window).
          </li>
          <li>The mouse position is logged to the console whenever the mouse is moved over the canvas.</li>
        </ul>

        <h3>Example: Drawing on Canvas with Mouse</h3>

        <p>Now let's try using the mouse to draw.</p>

        <h4>HTML</h4>
        <pre><code class="html">&lt;canvas id="myCanvas" width="500" height="400" style="border:1px solid #000000;"&gt;&lt;/canvas&gt;</code></pre>

        <h4>JavaScript</h4>
        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function startPosition(e) {
  painting = true;
  draw(e);
}

function endPosition() {
  painting = false;
  ctx.beginPath(); // Begin a new path to avoid connecting lines
}

function draw(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.lineWidth = 5; // Set the line width
  ctx.strokeStyle = 'black'; // Set the line color

  ctx.lineTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Draw a line to the current mouse position
  ctx.stroke(); // Stroke the line
  ctx.beginPath(); // Begin a new path
  ctx.moveTo(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop); // Move to the new starting point
}

canvas.addEventListener('mousedown', startPosition); // Listen for mousedown event
canvas.addEventListener('mouseup', endPosition); // Listen for mouseup event
canvas.addEventListener('mousemove', draw);</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define three functions: <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b>.</li>
          <li>The <b>startPosition</b> function is called when the mouse button is pressed down. It sets the
            <b>painting</b>
            flag to true and calls the <b>draw</b> function.
          </li>
          <li>The <b>endPosition</b> function is called when the mouse button is released. It sets the <b>painting</b>
            flag to
            false and begins a new path to avoid connecting lines.</li>
          <li>The <b>draw</b> function is called when the mouse is moved. It checks if the user is drawing, sets the
            line
            width and color, draws a line to the current mouse position, and moves to the new starting point.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>startPosition</b>, <b>endPosition</b>, and <b>draw</b> functions
            respectively.</li>
        </ul>

        <p>In another example we can use our mouse to draw circles on the canvas: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let painting = false; // Flag to indicate if the user is drawing

function drawCircle(e) {
  if (!painting) return; // Exit the function if the user is not drawing

  ctx.beginPath(); // Start a new path
  ctx.arc(e.clientX - canvas.offsetLeft, e.clientY - canvas.offsetTop, 10, 0, Math.PI * 2); // Draw a circle
  ctx.fill(); // Fill the circle
}

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
  painting = true; // Set the painting flag to true
  drawCircle(e); // Draw a circle
});

canvas.addEventListener('mouseup', () => { // Listen for mouseup event
  painting = false; // Set the painting flag to false
});

canvas.addEventListener('mousemove', drawCircle); // Listen for mousemove event</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define a <b>drawCircle</b> function that draws a circle at the current mouse position.</li>
          <li>We add event listeners for the <b>mousedown</b>, <b>mouseup</b>, and <b>mousemove</b> events to the canvas
            element. These event listeners call the <b>drawCircle</b> function
            when the mouse button is pressed down, released, and moved respectively.</li>
        </ul>

        <p>We can simplify the code a bit by using getBoundingClientRect which gives the size of an element and its
          position
          relative to the viewport:</p>

        <pre><code class="javascript
">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

ctx.beginPath(); // Start a new path
ctx.arc(e.clientX - canvas.getBoundingClientRect().left, e.clientY - canvas.getBoundingClientRect().top, 10, 0, Math.PI * 2); // Draw a circle
ctx.fill(); // Fill the circle</code></pre>

        <p>Although this works, we can achieve the same result a bit simpler with offsetX and offsetY:</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('mousedown', (e) => { // Listen for mousedown event
ctx.beginPath(); // Start a new path
ctx.arc(e.offsetX, e.offsetY, 10, 0, Math.PI * 2); // Draw a circle
ctx.fill(); // Fill the circle
});</code></pre>

        <p>This works because offsetX and offsetY give the position of the mouse <b>relative to the target element</b>,
          in this
          case the canvas.</p>

        <p>So to rewrite our draw function:</p>

        <pre><code class="javascript">function draw(e) {
    if (!painting) return; // Exit the function if the user is not drawing

    ctx.lineWidth = 5; // Set the line width
    ctx.strokeStyle = 'black'; // Set the line color

    ctx.lineTo(e.offsetX, e.offsetY); // Draw a line to the current mouse position
    ctx.stroke(); // Stroke the line
    ctx.beginPath(); // Begin a new path
    ctx.moveTo(e.offsetX, e.offsetY); // Move to the new starting point
}</code></pre>

        <p>And that's it! We can now draw on the canvas with our mouse.</p>

        <p>By combining mouse events with the canvas API, we can create interactive drawing applications that respond to
          user
          input.</p>

        <hr />

        <h2>Exercise: Drawing More Figures in HTML5 Canvas</h2>

        <p>How can we draw the following graphics on a 400 x 400 canvas?</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/Exercise%202.png?raw=true"
          alt="Drawing Exercise">


      </article>
      <br />
    </section>

    <hr />



    <section class="main-section" id="Additional_Events">
      <br />
      <header><b>Additional Events</b></header>
      <article>
        <p>In addition to our mouse down events, there are also a few others that may come in handy (for both 2D and 3D
          graphics)</p>

        <h2>Keyboard Events</h2>

        <p>Keyboard events are triggered when a key is pressed or released on the keyboard. They allow us to respond to
          user input and create interactive web applications that respond to key presses.</p>

        <p>There are several keyboard events that we can listen for:</p>

        <ul>
          <li><b>keydown</b> - Triggered when a key is pressed down.</li>
          <li><b>keyup</b> - Triggered when a key is released.</li>
          <li><b>keypress</b> - Triggered when a key is pressed down and released.</li>
        </ul>

        <p>Keyboard events provide information about the key that was pressed, such as the key code and the key value.
          This information can be used to perform specific actions based on the key that was pressed.</p>

        <p>Here is an example of how to listen for keyboard events:</p>

        <pre><code class="javascript">document.addEventListener('keydown', (event) => { // Listen for keydown event
  console.log('Key pressed: ' + event.key); // Log the key that was pressed

  if (event.key == 'ArrowUp') {
    console.log('Up arrow key pressed'); // Log a message if the up arrow key was pressed
  }

  if (event.key == 'ArrowDown') {
    console.log('Down arrow key pressed'); // Log a message if the down arrow key was pressed
  }

  // Add more key press conditions as needed
});</code></pre>

        <p>Each key on the keyboard has a unique key code and key value. The key code is a numerical value that
          represents
          the key, while the key value is a string that represents the key.</p>

        <p>For example, the key code for the up arrow key is 38, and the key value is 'ArrowUp'.</p>

        <p>Standard key codes are as follows: </p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/keycodes.png?raw=true"
          alt="Key Codes">


        <p>Keyboard events can be used to create keyboard shortcuts, control game characters, and more.</p>

        <p>For example, let's write some code to create a rectangle that we will be able to move with the WASD and
          directional arrow keys: </p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');
let x = 100; // Initial x position of the rectangle
let y = 100; // Initial y position of the rectangle
let speed = 5; // Speed of the rectangle

function drawRectangle() {
    ctx.clearRect(0, 0, canvas.width, canvas.height); // Clear the canvas
    ctx.fillStyle = 'blue'; // Set rectangle color
    ctx.fillRect(x, y, 50, 50); // Draw the rectangle
}

document.addEventListener('keydown', (event) => { // Listen for keydown event
    switch (event.key) {
        case 'ArrowUp':
        case 'w':
            y -= speed; // Move the rectangle up
            break;
        case 'ArrowDown':
        case 's':
            y += speed; // Move the rectangle down
            break;
        case 'ArrowLeft':
        case 'a':
            x -= speed; // Move the rectangle left
            break;
        case 'ArrowRight':
        case 'd':
            x += speed; // Move the rectangle right
            break;
    }

    drawRectangle(); // Redraw the rectangle
});

// Initial call to draw the rectangle when the page loads
drawRectangle();</code></pre>

        <p>In this example:</p>

        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We define the initial x and y positions of the rectangle, as well as the speed at which it will move.</li>
          <li>We define a <b>drawRectangle</b> function that clears the canvas and draws a rectangle at the current
            position.</li>
          <li>We add an event listener for the <b>keydown</b> event that moves the rectangle based on the key that was
            pressed.
          </li>
          <li>We use a <b>switch</b> statement to check which key was pressed and update the x and y positions of the
            rectangle
            accordingly.</li>
          <li>We call the <b>drawRectangle</b> function to redraw the rectangle after it has been moved.</li>
        </ul>


        <h2>Touch Events</h2>

        <p>In the early days of touch-enabled devices, touch events were often interpreted and essentially "translated"
          into mouse events for compatibility with existing web applications, meaning that a touch interaction would
          trigger a corresponding mouse event like a click or hover, although this approach had limitations when dealing
          with multi-touch gesture.</p>

        <p>With the widespread adoption of touchscreen devices, such as a smartphone or tablet, HTML5 brings to the
          table, among many other things, a set of touch-based interaction events. </p>

        <p>Mouse-based events such as hover, mouse in, mouse out etc. aren't able to adequately capture the range of
          interactions possible via touchscreen, so touch events are a welcome and necessary addition to the web
          developer's toolbox.</p>

        <p>They allow us to create web applications that respond to touch gestures, such as tapping, swiping, and
          pinching.</p>

        <p>Use cases for the touch events API include gesture recognition, multi-touch, drag and drop, and any other
          touch-based interfaces.</p>

        <h2>The Touch Events API</h2>

        <p>We'll get some of the technical details of the API out of the way first, before moving on to some real
          examples. The API is
          defined in terms of Touches, TouchEvents, and TouchLists.</p>

        <p>Each Touch describes a touch point, and has the following attributes:</p>

        <ul>
          <li><b>clientX</b> - The x-coordinate of the touch point relative to the viewport.</li>
          <li><b>clientY</b> - The y-coordinate of the touch point relative to the viewport.</li>
          <li><b>screenX</b> - The x-coordinate of the touch point relative to the screen.</li>
          <li><b>screenY</b> - The y-coordinate of the touch point relative to the screen.</li>
          <li><b>pageX</b> - The x-coordinate of the touch point relative to the document.</li>
          <li><b>pageY</b> - The y-coordinate of the touch point relative to the document.</li>
          <li><b>target</b> - The element that the touch point started in.</li>
          <li><b>identifier</b> - A unique identifier for the touch point.</li>
        </ul>

        <p>There are several touch events that we can listen for:</p>

        <ul>
          <li><b>touchstart</b> - Triggered when a touch point is placed on the touch surface.</li>
          <li><b>touchmove</b> - Triggered when a touch point is moved along the touch surface.</li>
          <li><b>touchend</b> - Triggered when a touch point is removed from the touch surface.</li>
          <li><b>touchcancel</b> - Triggered when a touch point is disrupted in some way.</li>
        </ul>

        <p>Touch events provide information about the touch points, such as the touch coordinates and the touch
          identifier. This information can be used to perform specific actions based on the touch gestures.</p>

        <p>Here is an example of how to listen for touch events:</p>

        <pre><code class="javascript">document.addEventListener('touchstart', (event) => { // Listen for touchstart event
  console.log('Touch started at: ' + event.touches[0].clientX + ',' + event.touches[0].clientY); // Log the touch
  coordinates
});</code></pre>

        <p>Let's go one step further and display the touch position information visually, by displaying a dot on the
          canvas at the point where it was touched.</p>

        <pre><code class="javascript">const canvas = document.getElementById('myCanvas');
const ctx = canvas.getContext('2d');

canvas.addEventListener('touchstart', (event) => { // Listen for touchstart event
  const touch = event.touches[0]; // Get the first touch point
  const x = touch.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
  const y = touch.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

  ctx.beginPath(); // Start a new path
  ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the touch point
  ctx.fill(); // Fill the circle
});</code></pre>

        <p>In this example:</p>
        <ul>
          <li>We get the canvas element and its 2D drawing context.</li>
          <li>We add an event listener for the <b>touchstart</b> event to the canvas element.</li>
          <li>We get the first touch point from the <b>touches</b> property of the event.</li>
          <li>We calculate the x and y coordinates of the touch point relative to the canvas.</li>
          <li>We draw a circle at the touch point using the <b>arc</b> method and fill it using the <b>fill</b> method.
          </li>
        </ul>

        <h2>Browser Support and Fallbacks</h2>

        <p>Touch events are widely supported among mobile devices.</p>

        <p>However, unless specifically targeting touch devices, a fallback should be implemented when touchevents are
          not supported. In these cases, the traditional click etc. events can be bound to, but as discussed below, care
          is needed when deciding which events to support instead of the touch events.</p>

        <h2>Touch and Mouse Events</h2>

        <p>Since touch events may not be supported on a user's device - indeed, the user may not even be accessing your
          app on a touchscreen device - this contingency should be planned for. </p>

        <p>You may want to enable your app to support particular mouse events instead. Care should be taken here as
          there is not a one-to-one correspondance between mouse events and touch events, and the behaviour differences
          can be subtle.</p>


        <p>So for our code we should also enable mouse events:</p>

        <pre><code class="javascript">canvas.addEventListener('mousedown', (event) => { // Listen for mousedown event
const x = event.clientX - canvas.offsetLeft; // Calculate the x-coordinate relative to the canvas
const y = event.clientY - canvas.offsetTop; // Calculate the y-coordinate relative to the canvas

ctx.beginPath(); // Start a new path
ctx.arc(x, y, 5, 0, Math.PI * 2); // Draw a circle at the mouse point
ctx.fill(); // Fill the circle
});</code></pre>

        <h2>Best Practices</h2>

        <p>Care should also be taken implementing touch events that the events don't interfere with typical browser
          behaviours such as scrolling and zooming - thus there is an argument for disabling these default browser
          behaviours if you are making use of touch events.</p>

        <p>It is also important to remember that touch events are not the same as mouse events.</p>

        <p>For example, a touchstart event is not the same as a mousedown event. The former is triggered when a touch
          point is placed on the touch surface, while the latter is triggered when a mouse button is pressed down. </p>

        <p>Therefore, it is important to consider the differences between touch and mouse events when designing
          touch-based interfaces.</p>

        <p>Finally, it is important to test touch events on a variety of devices to ensure that they work as expected
          and provide a good user experience.</p>

        <p>In addition, there are also scroll events, resize events, and more. These can all be used to create more
          interactive and dynamic web applications.</p>

      </article>
      <br />
    </section>

    <hr />
    <section class="main-section" id="Transforms">
      <br />

      <header><b>Transforms</b></header>
      <article>
        <p>It is possible to transform
          coordinates from one coordinate system to another. Let's look at how geometric transformations can be used to
          place graphics
          objects into a coordinate system.</p>

        <h2>Viewing and Modeling</h2>

        <p>In a typical application, we have a rectangle made of pixels, with its natural pixel coordinates,
          where an image will be displayed. This rectangle will be called the <b>viewport</b>.</p>

        <p>We also have
          a set of geometric objects that are defined in a possibly different coordinate system, generally
          one that uses real-number coordinates rather than integers. These objects make up the “scene”
          or "world" that we want to view, and the coordinates that we use to define the scene are called
          <b>world coordinates</b>.
        </p>

        <p>For 2D graphics, the world lies in a plane. It's not possible to show a picture of the entire
          infinite plane. We need to pick some rectangular area in the plane to display in the image.
          Let's call that rectangular area the <b>window</b>, or view window.</p>

        <p>A coordinate transform is used
          to map the window to the viewport.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/window%20and%20viewport.png?raw=true"
          alt="Coordinate Transformation">

        <p>In this illustration, T represents the coordinate transformation. T is a function that takes world
          coordinates (x,y) in some window and maps them to pixel coordinates T(x,y) in the viewport</p>

        <p>In this example, as you can
          check,</p>

        <text class="center">T(x,y) = ( 800*(x+4)/8, 600*(3-y)/6 )</text>

        <p>Look at the rectangle with corners at (-1,2) and (3,-1) in the window. When this rectangle is
          displayed in the viewport, it is displayed as the rectangle with corners T(-1,2) and T(3,-1). In
          this example, T(-1,2) = (300,100) and T(3,-1) = (700,400).</p>

        <p>We use coordinate transformations in this way because it allows us to choose a world
          coordinate system that is natural for describing the scene that we want to display, and it is easier to do
          that than to work directly with viewport coordinates. Along the same lines,
          suppose that we want to define some complex object, and suppose that there will be several
          copies of that object in our scene. Or maybe we are making an animation, and we would like the
          object to have different positions in different frames.</p>

        <p>We would like to choose some convenient
          coordinate system and use it to define the object once and for all.</p>

        <p>The coordinates that we
          use to define an object are called <b>object coordinates</b> for the object. </p>

        <p>When we want to place
          the object into a scene, we need to transform the object coordinates that we used to define the
          object into the world coordinate system that we are using for the scene. The transformation that
          we need is called a <b>modeling transformation</b>. </p>

        <p>This picture illustrates an object defined in
          its own object coordinate system and then mapped by three different modeling transformations
          into the world coordinate system:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/modeling%20transformation.png?raw=true"
          alt="Modeling Transformation">

        <p>Remember that in order to view the scene, there will be another transformation that maps the
          object from a view window in world coordinates into the viewport.</p>

        <p>Now, keep in mind that the choice of a view window tells which part of the scene is shown
          in the image. Moving, resizing, or even rotating the window will give a different view of the
          scene. Suppose we make several images of the same car:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/modeling%20transformation%202.png?raw=true"
          alt="Modeling Transformation 2">

        <p>What happened between making the top image in this illustration and making the image on
          the bottom left?</p>

        <p>In fact, there are two possibilities: Either the car was moved to the right, or
          the view window that defines the scene was moved to the left.</p>

        <p>This is important, so be sure
          you understand it. (Try it with your cell phone camera. Aim it at some objects, take a step
          to the left, and notice what happens to the objects in the camera's viewfinder: They move to the right in the
          picture!)</p>

        <p>Similarly, what happens between the top picture and the middle
          picture on the bottom? Either the car rotated counterclockwise, or the window was rotated
          clockwise. (Again, try it with a camera—you might want to take two actual photos so that you
          can compare them.)</p>

        <p>Finally, the change from the top picture to the one on the bottom right
          could happen because the car got smaller or because the window got larger. (On your camera,
          a bigger window means that you are seeing a larger field of view, and you can get that by
          applying a zoom to the camera or by backing up away from the objects that you are viewing.)</p>

        <p>There is an important general idea here. When we modify the view window, we change
          the coordinate system that is applied to the viewport. But in fact, this is the same as leaving
          that coordinate system in place and moving the objects in the scene instead. Except that to
          get the same effect in the final image, you have to apply the opposite transformation to the
          objects (for example, moving the window to the left is equivalent to moving the objects to the
          right).</p>

        <p>So, there is no essential distinction between transforming the window and transforming
          the object. Mathematically, you specify a geometric primitive by giving coordinates in some
          natural coordinate system, and the computer applies a sequence of transformations to those
          coordinates to produce, in the end, the coordinates that are used to actually draw the primitive
          in the image.</p>

        <p>You will think of some of those transformations as modeling transforms and some
          as coordinate transforms, but to the computer, it's all the same.</p>

        <p>We will return to this idea several times later throughout this class, but in any case, you can see that
          geometric transforms are a central concept in computer graphics. Let's look at some basic types
          of transformation in more detail.</p>

        <p>The transforms we will use in 2D graphics can be written in
          the form:</p>

        <text class="center"><b>x1 = a*x + b*y + e</b></text>

        <text class="center"><b>y1 = c*x + d*y + f</b></text>

        <p>where (x,y) represents the coordinates of some point before the transformation is applied, and
          (x1,y1 ) are the transformed coordinates.</p>

        <p>The transform is defined by the six constants a, b, c,
          d, e, and f. Note that this can be written as a function T, where</p>

        <text class="center">T(x,y) = (a*x + b*y + c, d*x + e*y + f)</text>

        <p>A transformation of this form is called an <b>affine transform</b>.</p>
        <p>An affine transform has the
          property that, when it is applied to two parallel lines, the transformed lines will also be parallel.
          Also, if you follow one affine transform by another affine transform, the result is again an affine
          transform.</p>

        <p>There are four basic types of affine transforms that are commonly used in computer graphics:</p>

        <ul>
          <li><b>Translation</b> - Moves an object by a specified distance along the x and y axes.</li>
          <li><b>Rotation</b> - Rotates an object by a specified angle around a specified point.</li>
          <li><b>Scaling</b> - Increases or decreases the size of an object by a specified factor along the x and y
            axes.</li>
          <li><b>Shearing</b> - Skews an object by a specified angle along the x or y axis.</li>
        </ul>

        <h2>Translation</h2>

        <p>A translation transform simply moves every point by a certain amount horizontally and a
          certain amount vertically.</p>

        <p>If (x,y) is the original point and (x1,y1) is the transformed point,
          then the formula for a translation is:</p>

        <text class="center"><b>x1 = x + e</b></text>

        <text class="center"><b>y1 = y + f</b></text>

        <p>where e is the number of units by which the point is moved horizontally and f is the amount by
          which it is moved vertically. (Thus for a translation, a = d = 1, and b = c = 0 in the general
          formula for an affine transform.)</p>

        <p>A 2D graphics system will typically have a function such as:</p>

        <text class="center"><b>translate(e, f)</b></text>

        <p>to apply a translate transformation. The translation would apply to everything that is drawn
          <b>after</b> the command is given. That is, for all subsequent drawing operations, e would be added
          to the x-coordinate and f would be added to the y-coordinate.
        </p>

        <p>Let's look at an example: Suppose that you draw an "F" using coordinates in which the "F" is centered at
          (0,0).</p>

        <p>If you say translate(4,2) before drawing the "F", then every point of the "F" will be moved
          horizontally by 4 units and vertically by 2 units before the coordinates are actually used, so
          that after the translation, the "F" will be centered at (4,2):</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/translation.png?raw=true"
          alt="Translation Transformation">

        <p>The light gray "F" in this picture shows what would be drawn without the translation; the dark
          red "F" shows the same "F" drawn after applying a translation by (4,2).</p>

        <p>The top arrow shows
          that the upper left corner of the "F" has been moved over 4 units and up 2 units. Every point
          in the "F" is subjected to the same displacement.</p>

        <p>Note that in these examples, we are assuming
          that the y-coordinate increases from bottom to top. That is, the y-axis points up.</p>

        <p>Remember that when you give the command translate(e,f), the translation applies to <b>all</b> the
          drawing that you do after that, not just to the next shape that you draw.</p>

        <p>If you apply another
          transformation after the translation, the second transform will not replace the translation.
          It will be combined with the translation, so that subsequent drawing will be affected by the
          combined transformation.</p>

        <p>For example, if you combine translate(4,2) with translate(-1,5), the
          result is the same as a single translation, translate(3,7). This is an important point, and there
          will be a lot more to say about it later.
        </p>

        <p>Also remember that you don't compute coordinate transformations yourself. You just
          specify the original coordinates for the object (that is, the object coordinates), and you specify
          the transform or transforms that are to be applied. The computer takes care of applying the
          transformation to the coordinates.</p>

        <p>You don't even need to know the exact equations that are used
          for the transformation; <b>you just need to understand what it does geometrically</b>.</p>

        <p>HTML5 Canvas has a built in translate() method - used to move an object by x and y, where:</p>

        <ul>
          <li>x is the amount to move the object horizontally.</li>
          <li>y is the amount to move the object vertically.</li>
        </ul>

        <p>For example, here is how you might use the translate function in HTML5 Canvas:</p>

        <p>First, draw one rectangle in position (10,10), then set translate() to (70,70) (This will be the new start
          point). Then draw another rectangle in position (10,10). Notice that the second rectangle now starts in
          position (80,80):</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(10, 10, 100, 50);

ctx.translate(70, 70);

ctx.fillStyle = "blue";
ctx.fillRect(10, 10, 100, 50);</code></pre>

        <p>Our first rectangle is placed at position (10, 10) but when we use translate(70,70), even though our next
          drawn rectangle is also placed
          at position (10, 10), it is actually placed at position (80, 80) because our x-position is now (70 + 10) and
          y-position is now (70 + 10).</p>
        </p>

        <h2>Rotation</h2>

        <p>A rotation transform, for our purposes here, rotates each point about the origin, (0,0). Every
          point is rotated through the same angle, called the angle of rotation.</p>

        <p>For this purpose, angles can be measured either in degrees or in radians.</p>

        <p>A rotation with a positive angle rotates objects in the direction from the positive x-axis towards
          the positive y-axis.</p>

        <p>This is counterclockwise in a coordinate system (cartesian) where the y-axis points up,
          as it does in my examples here, but it is clockwise in the usual pixel coordinates, where the
          y-axis points down rather than up.</p>

        <p>Although it is not obvious, when rotation through an angle of r radians about the origin is applied to the
          point (x,y), then the resulting point (x1,y1 ) is
          given by: </p>

        <text class="center">x1 = cos(r) * x - sin(r) * y</text>
        <text class="center">y1 = sin(r) * x + cos(r) * y</text>

        <p>That is, in the general formula for an affine transform, e = f = 0, a = d = cos(r), b = -sin(r),
          and c = sin(r).</p>

        <p>Here is a picture that illustrates a rotation about the origin by the angle
          negative 135 degrees:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rotation.png?raw=true"
          alt="Rotation Transformation">


        <p>Again, the light gray "F" is the original shape, and the dark red "F" is the shape that results
          if you apply the rotation. The arrow shows how the upper left corner of the original “F” has
          been moved.</p>

        <p>A 2D graphics API would typically have a command rotate(r) to apply a rotation. The
          command is used before drawing the objects to which the rotation applies.</p>

        <p>For example, HTML5 Canvas
          has a rotate method that takes an angle in radians as a parameter. The rotation is applied to all
          subsequent drawing operations.</p>

        <p>As a reminder, angles are in radians, as opposed to degrees. So we use <b>(Math.PI/180)*[degree]</b> to
          convert.</p>

        <p>Let's rotate a rectangle in canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.fillStyle = "red";
ctx.fillRect(50, 10, 100, 50);

ctx.rotate((Math.PI/180)*20); // Rotate 20 degrees

ctx.strokeStyle = "blue";
ctx.strokeRect(70, 30, 100, 50);</code></pre>

        <p>Remember that the rotation is applied to all subsequent drawing operations, so we need to
          translate to the center of the rectangle to rotate it around its center.</p>

        <h2>Scaling</h2>

        <p>A scaling transform can be used to make objects bigger or smaller.</p>

        <p>Mathematically, a scaling
          transform simply multiplies each x-coordinate by a given amount and each y-coordinate by a
          given amount.</p>

        <p>That is, if a point (x,y) is scaled by a factor of a in the x direction and by a
          factor of d in the y direction, then the resulting point (x1,y1 ) is given by:</p>

        <text class="center">x1 = a*x</text>
        <text class="center">y1 = d*y</text>

        <p>If you apply this transform to a shape that is centered at the origin, it will stretch the shape
          by a factor of a horizontally and d vertically.</p>

        <p>Here is an example, in which the original light
          gray "F" is scaled by a factor of 3 horizontally and 2 vertically to give the final dark red "F":</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/scaling.png?raw=true"
          alt="Scaling Transformation">


        <p>The common case where the horizontal and vertical scaling factors are the same is called
          <b>uniform scaling</b>. Uniform scaling stretches or shrinks a shape without distorting it.
        </p>

        <p>When scaling is applied to a shape that is not centered at (0,0), then in addition to being
          stretched or shrunk, the shape will be moved away from 0 or towards 0. In fact, the true
          description of a scaling operation is that it pushes every point away from (0,0) or pulls every
          point towards (0,0). If you want to scale about a point other than (0,0), you can use a sequence
          of three transforms, similar to what was done in the case of rotation.</p>

        <p>A 2D graphics API can provide a function scale(a,d) for applying scaling transformations.
          As usual, the transform applies to all x and y coordinates in subsequent drawing operations.</p>

        <p>One unit on the canvas is one pixel. If we set the scaling factor to 2, one unit becomes two pixels, and
          shapes will be drawn twice as large. If we set a scaling factor to 0.5, one unit becomes 0.5 pixels, and
          shapes will be drawn at half size.</p>

        <p>Note that negative scaling factors are allowed and will result in reflecting the shape as well
          as possibly stretching or shrinking it. For example, scale(1,-1) will reflect objects vertically,
          through the x -axis.</p>

        <p>As with the other transformations, a scaling transform is applied to all subsequent drawing
          operations. In HTML5 Canvas, you can use the scale method to apply a scaling transform:</p>

        <p>The scale() method has the following parameters: </p>

        <ul>
          <li><b>x</b> - The scaling factor for the x-axis (1 is the original size).</li>
          <li><b>y</b> - The scaling factor for the y-axis (1 is the original size).</li>
        </ul>

        <p>One unit on the canvas is one pixel.</p>

        <p>So, if we set the scaling factor to 2, one unit becomes two pixels, and
          shapes will be drawn twice as large. If we set a scaling factor to 0.5, one unit becomes 0.5 pixels, and
          shapes will be drawn at half size.</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.scale(2, 2);

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>It is a fact that every affine transform can be created by combining translations,
          rotations about the origin, and scalings about the origin. </p>

        <p>Also note that a transform that is made from translations and rotations, with no scaling,
          will preserve length and angles in the objects to which it is applied. It will also preserve aspect
          ratios of rectangles.</p>

        <p>Transforms with this property are called <b>"Euclidean"</b>. If you also allow
          uniform scaling, the resulting transformation will preserve angles and aspect ratio, but not
          lengths.
        </p>

        <h2>Shearing</h2>

        <p>We will look at one more type of basic transform, a <b>shearing transform</b>.</p>

        <p>Although shears
          can in fact be built up out of rotations and scalings if necessary, it is not really obvious how
          to do so.</p>

        <p>A shear will "tilt" objects. A horizontal shear will tilt things towards the left (for
          negative shear) or right (for positive shear). A vertical shear tilts them up or down. </p>

        <p>Here is an
          example of horizontal shear:</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/shearing.png?raw=true"
          alt="Shearing Transformation">


        <p>A horizontal shear does not move the x-axis. Every other horizontal line is moved to the left or
          to the right by an amount that is proportional to the y-value along that line.</p>

        <p>When a horizontal shear is applied to a point (x,y), the resulting point (x1,y1) is given by:
        </p>

        <text class="center">x1 = x + b * y</text>
        <text class="center">y1 = y</text>

        <p>for some constant shearing factor b. Similarly, a vertical shear with shearing factor c is given
          by the equations:</p>

        <text class="center">x1 = x</text>
        <text class="center">y1 = c * x + y</text>

        <p>Shear is occasionally called "skew", but skew is usually specified as an angle rather than as a
          shear factor.</p>















        <h2>Combining Transformations</h2>

        <p>As we just saw, we are now in a position to see what can happen when you combine two transformations.
          Suppose that before drawing some object, you say:</p>

        <text class="center"><b>translate(4,0)</b></text>
        <text class="center"><b>rotate(90)</b></text>

        <p>Assume that angles are measured in degrees.</p>

        <p>The translation will then apply to all subsequent
          drawing. But, because of the rotation command, the things that you draw after the translation
          are <b>rotated</b> objects.</p>

        <p>That is, the translation applies to objects that have <b>already</b> been rotated.</p>

        <p>An example is shown on the left in the illustration below, where the light gray "F" is the original
          shape, and red "F" shows the result of applying the two transforms to the original.</p>

        <p>The original "F" was first rotated through a 90 degree angle, and then moved 4 units to the right.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/combining%20transforms.png?raw=true"
          alt="Combining Transformations">


        <p>Note that transforms are applied to objects in the reverse of the order in which they are given
          in the code (because the first transform in the code is applied to an object that has already
          been affected by the second transform). And note that the order in which the transforms are
          applied is important.</p>

        <p>If we reverse the order in which the two transforms are applied in this
          example, by saying: </p>

        <text class="center"><b>rotate(90)</b></text>
        <text class="center"><b>translate(4,0)</b></text>

        <p>then the result is as shown on the right in the above illustration. In that picture, the original
          "F" is first moved 4 units to the right and the resulting shape is then rotated through an angle
          of 90 degrees about the origin to give the shape that actually appears on the screen.</p>

        <p>For another example of applying several transformations, suppose that we want to rotate
          a shape through an angle r about a point (p,q) instead of about the point (0,0).</p>

        <p>We can do
          this by first moving the point (p,q) to the origin, using translate(-p,-q). Then we can do a
          standard rotation about the origin by calling rotate(r). Finally, we can move the origin back
          to the point (p,q) by applying translate(p,q).</p>

        <p>Keeping in mind that we have to write the code
          for the transformations in the reverse order, we need to say: </p>

        <text class="center"><b>translate(p,q)</b></text>
        <text class="center"><b>rotate(r)</b></text>
        <text class="center"><b>translate(-p,-q)</b></text>

        <p>before drawing the shape. (In fact, some graphics APIs let us accomplish this transform with a
          single command such as rotate(r,p,q). This would apply a rotation through the angle r about
          the point (p,q).)
        </p>

        <p>In HTML5 Canvas, we have a method called transform().</p>

        <p>The transform() method replaces the current transformation matrix with the matrix described by the
          arguments of this method. The transform() method multiplies the current transformation matrix with the
          matrix described by the arguments of this method. This is useful for applying multiple transformations to
          the same shape.</p>

        <p>The transform method takes the following parameters: </p>

        <ul>
          <li><b>a</b> - Horizontal scaling. 1 is no scaling.</li>
          <li><b>b</b> - Horizontal skewing.</li>
          <li><b>c</b> - Vertical skewing.</li>
          <li><b>d</b> - Vertical scaling. 1 is no scaling.</li>
          <li><b>e</b> - Horizontal moving.</li>
          <li><b>f</b> - Vertical moving.</li>
        </ul>
        <p>Here is an example of how to use the transform() method in HTML5 Canvas:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.transform(2, 0, 0, 2, 40, 40); // Scale by 2 and move 40 units
ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>

        <p>We can scale, rotate, and translate using a mixture of these methods as well:</p>

        <pre><code class="javascript">const canvas = document.getElementById("myCanvas");
const ctx = canvas.getContext("2d");

ctx.strokeRect(5, 5, 100, 75);

ctx.translate(200, 40); // Move 200 units to the right and 40 units down
ctx.rotate((Math.PI/180)*45); // Rotate 45 degrees
ctx.scale(2, 2); // Scale by 2

ctx.strokeStyle = "blue";
ctx.strokeRect(5, 5, 100, 75);</code></pre>


        <p>Note that we will talk again about transformations in the context of 3D graphics,
          where they
          become even more important.</p>


      </article>
      <br />
      <hr />
    </section>


    <section class="main-section" id="Linear_Algebra">
      <br />
      <header><b>Linear Algebra</b></header>
      <article>
        <p>Linear algebra is a branch of mathematics that deals with vectors and matrices. It is a fundamental tool in
          computer graphics, as it allows us to perform transformations on objects in 2D and 3D space.</p>

        <h2>Vectors</h2>

        <p>A vector is a quantity that has a length and a direction.</p>

        <p>A vector can be visualized as an arrow,
          as long as you remember that it is the length and direction of the arrow that are relevant, and
          that its specific location is irrelevant.</p>
        <p>Vectors are often used in computer graphics to represent
          directions, such as the direction from an object to a light source or the direction in which a
          surface faces. In those cases, we are more interested in the direction of a vector than in its
          length.
        </p>

        <p>If we visualize a 3D vector V as an arrow starting at the origin, (0,0,0), and ending at a
          point P, then we can, to a certain extent, identify V with P—at least as long as we remember
          that an arrow starting at any other point could also be used to represent V. If P has coordinates
          (a,b,c), we can use the same coordinates for V.</p>

        <p>When we think of (a,b,c) as a vector, the value
          of a represents the change in the x -coordinate between the starting point of the arrow and its
          ending point, b is the change in the y-coordinate, and c is the change in the z -coordinate. For
          example, the 3D point (x,y,z ) = (3,4,5) has the same coordinates as the vector (dx,dy,dz ) =
          (3,4,5). </p>

        <p>For the point, the coordinates (3,4,5) specify a position in space in the xyz coordinate
          system.</p>

        <p>For the vector, the coordinates (3,4,5) specify the change in the x, y, and z coordinates
          along the vector.</p>

        <p>If we represent the vector with an arrow that starts at the origin (0,0,0), then
          the head of the arrow will be at (3,4,5). But we could just as well visualize the vector as an
          arrow that starts at the point (1,1,1), and in that case the head of the arrow would be at the
          point (4,5,6).</p>

        <p>The distinction between a point and a vector is subtle. For some purposes, the distinction
          can be ignored; for other purposes, it is important. Often, all that we have is a sequence of
          numbers, which we can treat as the coordinates of either a vector or a point, whichever is more
          appropriate in the context.</p>

        <p>One of the basic properties of a vector is its length. In terms of its coordinates, the length
          of a 3D vector (x,y,z) is given by sqrt(x^2+y^2+z^2). (This is just the Pythagorean theorem in
          three dimensions.) If v is a vector, its length is denoted by |v|. </p>

        <p>The length of a vector is also
          called its norm. (We are considering 3D vectors here, but concepts and formulas are similar
          for other dimensions.)
        </p>

        <p>Vectors of length 1 are particularly important. They are called <b>unit vectors</b>. If v = (x,y,z )
          is any vector other than (0,0,0), then there is exactly one unit vector that points in the same
          direction as v.</p>

        <p>That vector is given by:</p>

        <text class="center"><b>( x/length, y/length, z/length )</b></text>

        <p>where length is the length of v. Dividing a vector by its length is said to normalize the vector:
          The result is a unit vector that points in the same direction as the original vector.</p>

        <p>Two vectors can be added. Given two vectors v1 = (x1,y1,z1) and v2 = (x2,y2,z2), their
          sum is defined as:</p>

        <text class="center"><b>v1 + v2 = ( x1+x2, y1+y2, z1+z2 );</b></text>

        <p>The sum has a geometric meaning:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/Vectors%201.png?raw=true"
          alt="Vectors 1" />

        <p>Multiplication is more complicated. The obvious definition of the product of two vectors,
          similar to the definition of the sum, does not have geometric meaning and is rarely used.</p>

        <p>However, there are three kinds of vector multiplication that are used: the <b>scalar product</b>, the
          <b>dot product</b>, and the <b>cross product</b>.
        </p>

        <p>If v = (x,y,z ) is a vector and a is a number, then the <b>scalar product</b> of a and v is defined
          as:</p>

        <text class="center"><b>av = ( a*x, a*y, a*z );</b></text>

        <p>Assuming that a is positive and v is not zero, av is a vector that points in the same direction as
          v, whose length is a times the length of v. If a is negative, av points in the opposite direction
          from v, and its length is |a| times the length of v. </p>

        <p>This type of product is called a scalar product
          because a number like a is also referred to as a "scalar", perhaps because multiplication by a
          scales v to a new length.
        </p>

        <p>Given two vectors v1 = (x1,y1,z1 ) and v2 = (x2,y2,z2 ), the <b>dot product</b> of v1 and v2 is
          denoted by v1 ·v2 and is defined by:</p>

        <text class="center"><b>v1·v2 = x1*x2 + y1*y2 + z1*z2;</b></text>

        <p>Note that the dot product is a number, not a vector.</p>

        <p>The dot product has several very important
          geometric meanings. First of all, note that the length of a vector v is just the square root of
          v·v. Furthermore, the dot product of two non-zero vectors v1 and v2 has the property that:</p>

        <text class="center"><b>cos(angle) = v1·v2 / (|v1|*|v2|)</b></text>

        <p>where angle is the measure of the angle between v1 and v2.</p>

        <p> In particular, <b>in the case of two
            unit vectors, whose lengths are 1, the dot product of two unit vectors is simply the cosine
            of the angle between them</b>.</p>

        <p>Furthermore, since the cosine of a 90-degree angle is zero, two
          non-zero vectors are perpendicular if and only if their dot product is zero. Because of these
          properties, the dot product is particularly important in lighting calculations, where the effect
          of light shining on a surface depends on the angle that the light makes with the surface.</p>

        <p>The scalar product and dot product are defined in any dimension.</p>

        <p>For vectors in 3D, there
          is another type of product called the <b>cross product</b>, which also has an important geometric
          meaning. For vectors v1 = (x1,y1,z1) and v2 = (x2,y2,z2), the cross product of v1 and v2 is
          denoted v1×v2 and is the vector defined by:</p>

        <text class="center"><b>v1×v2 = ( y1*z2 - z1*y2, z1*x2 - x1*z2, x1*y2 - y1*x2 )</b></text>

        <p>If v1 and v2 are non-zero vectors, then v1×v2 is zero if and only if v1 and v2 point in the same
          direction or in exactly opposite directions.</p>

        <p>Assuming v1×v2 is non-zero, then it is perpendicular
          both to v1 and to v2 ; furthermore, the vectors v1, v2, v1×v2 follow the right-hand rule (in
          a right-handed coordinate system); that is, if you curl the fingers of your right hand from v1
          to v2, then your thumb points in the direction of v1×v2.</p>

        <p>If v1 and v2 are perpendicular unit
          vectors, then the cross product v1×v2 is also a unit vector, which is perpendicular both to v1
          and to v2.</p>

        <p>Finally, I will note that given two points P1 = (x1,y1,z1 ) and P2 = (x2,y2,z2 ), the difference
          P2−P1 is defined by:</p>

        <text class="center"><b>P2−P1 = ( x2−x1, y2−y1, z2−z1 )</b></text>

        <p>This difference is a vector that can be visualized as an arrow that starts at P1 and ends at P2.</p>

        <p>Now, suppose that P1, P2, and P3 are vertices of a polygon. Then the vectors P1−P2 and
          P3−P2 lie in the plane of the polygon, and so the cross product:</p>

        <text class="center"><b>(P3−P2) × (P1−P2)</b></text>

        <p>is a vector that is perpendicular to the polygon.</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/Vectors%202.png?raw=true"
          alt="Vectors 2" />

        <p>This vector is said to be a <b>normal vector</b> for the polygon.</p>

        <p>A normal vector of length one is
          called a <b>unit normal</b>. Unit normals will be important in lighting calculations, and it will be
          useful to be able to calculate a unit normal for a polygon from its vertices.</p>

        <h2>Matrices and Transformations</h2>

        <p>A matrix is just a two-dimensional array of numbers.</p>

        <p>A matrix with r rows and c columns is
          said to be an r -by-c matrix. If A and B are matrices, and if the number of columns in A is
          equal to the number of rows in B, then A and B can be multiplied to give the matrix product
          AB. If A is an n-by-m matrix and B is an m-by-k matrix, then AB is an n-by-k matrix. In
          particular, two n-by-n matrices can be multiplied to give another n-by-n matrix.</p>

        <p>An n-dimensional vector can be thought of an n-by-1 matrix. If A is an n-by-n matrix and
          v is a vector in n dimensions, thought of as an n-by-1 matrix, then the product Av is again an
          n-dimensional vector.</p>

        <p>The product of a 3-by-3 matrix A and a 3D vector v = (x,y,z) is often
          displayed like this:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/matrix%20product.png?raw=true"
          alt="Matrix Vector Multiplication">

        <p>Note that the i-th coordinate in the product Av is simply the dot product of the i-th row of
          the matrix A and the vector v.</p>

        <p>Using this definition of the multiplication of a vector by a matrix, a matrix defines a
          transformation that can be applied to one vector to yield another vector.</p>

        <p>Transformations
          that are defined in this way are <b>linear transformations</b>, and they are the main object of study
          in linear algebra. A linear transformation L has the properties that for two vectors v and w,
          L(v+w) = L(v) + L(w), and for a number s, L(sv) = sL(v).
        </p>

        <p>Rotation and scaling are linear transformations, but translation is <b>not</b> a linear transformation. </p>

        <p>To include translations, we have to widen our view of transformation to include affine
          transformations. An affine transformation can be defined, roughly, as a linear transformation
          followed by a translation.</p>

        <p>Geometrically, an affine transformation is a transformation that
          preserves parallel lines; that is, if two lines are parallel, then their images under an affine
          transformation will also be parallel lines.</p>

        <p>For computer graphics, we are interested in affine
          transformations in three dimensions. However—by what seems at first to be a very odd trick—
          we can narrow our view back to the linear by moving into the fourth dimension.</p>

        <p>Note first of all that an affine transformation in three dimensions transforms a vector
          (x1,y1,z1) into a vector (x2,y2,z2) given by formulas:</p>

        <text class="center"><b>x2 = a1*x1 + a2*y1 + a3*z1 + t1</b></text>
        <text class="center"><b>y2 = b1*x1 + b2*y1 + b3*z1 + t2</b></text>
        <text class="center"><b>z2 = c1*x1 + c2*y1 + c3*z1 + t3</b></text>

        <p>These formulas express a linear transformation given by multiplication by the 3-by-3 matrix:</p>

        <img class="center" style="width: 45%; height: 45%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/3x3%20matrix%2001.png?raw=true"
          alt="3x3 Matrix">

        <p>followed by translation by t1 in the x direction, t2 in the y direction and t3 in the z direction.</p>

        <p>The trick is to replace each three-dimensional vector (x,y,z) with the four-dimensional vector
          (x,y,z,1), adding a "1" as the fourth coordinate. And instead of the 3-by-3 matrix, we use the
          4-by-4 matrix:
        </p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/4x4%20matrix.png?raw=true"
          alt="4x4 Matrix">


        <p>If the vector (x1,y1,z1,1) is multiplied by this 4-by-4 matrix, the result is precisely the vector
          (x2,y2,z2,1). That is, instead of applying an affine transformation to the 3D vector (x1,y1,z1),
          we can apply a linear transformation to the 4D vector (x1,y1,z1,1).</p>

        <p>This might seem pointless to you, but nevertheless, that is what is done in OpenGL
          and other 3D computer graphics systems: <b>An affine transformation is represented as a 4-
            by-4 matrix in which the bottom row is (0,0,0,1), and a three-dimensional vector is changed
            into a four dimensional vector by adding a 1 as the final coordinate</b>.</p>

        <p>The result is that all
          the affine transformations that are so important in computer graphics can be implemented as
          multiplication of vectors by matrices.</p>

        <p>The identity transformation, which leaves vectors unchanged, corresponds to multiplication
          by the identity matrix, which has ones along its descending diagonal and zeros elsewhere.</p>

        <p>The OpenGL function glLoadIdentity() sets the current matrix to be the 4-by-4 identity matrix.</p>

        <p>An OpenGL transformation function, such as glTranslatef (tx,ty,tz), has the effect of multiplying
          the current matrix by the 4-by-4 matrix that represents the transformation.</p>

        <p>Multiplication is on the right; that is, if M is the current matrix and T is the matrix that represents the
          transformation, then the current matrix will be set to the product matrix MT.</p>

        <p>For the record,
          the following illustration shows the identity matrix and the matrices corresponding to various
          OpenGL transformation functions:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/all%20matrices.png?raw=true"
          alt="4x4 Matrix">


        <p>So if we want to translate the vector (10,10,10,1) of 10 units in the X direction, we get:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/translation%20matrix%20example.png?raw=true"
          alt="Translation Matrix Example">


        <p>and we get a (20,10,10,1) homogeneous vector! Remember, the 1 means that it is a position, not a direction.
          So our transformation didn't change the fact that we were dealing with a position, which is good.</p>

        <p>If we want to scale a vector (position or direction, it doesn't matter) by 2.0 in all directions:</p>


        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/scaling%20matrix%20example.png?raw=true"
          alt="Scaling Matrix Example">

        <p>and the w still didn't change. You may ask: what is the meaning of "scaling a direction"? Well, often, not
          much, so you usually don't do such a thing, but in some (rare) cases it can be handy.</p>


        <h2>Homogeneous Coordinates</h2>

        <p>There is one common transformation in computer graphics that is not an
          affine transformation: In the case of a perspective projection, the projection transformation is
          not affine.</p>

        <p>In a perspective projection, an object will appear to get smaller as it moves farther
          away from the viewer, and that is a property that no affine transformation can express, since
          affine transforms preserve parallel lines and parallel lines will seem to converge in the distance
          in a perspective projection.</p>

        <p>Surprisingly, we can still represent a perspective projection as a 4-by-4 matrix, provided
          we are willing to stretch our use of coordinates even further than we have already.</p>

        <p>We have already represented 3D vectors by 4D vectors in which the fourth coordinate is 1. We now
          allow the fourth coordinate to be anything at all, except for requiring that at least one of
          the four coordinates is non-zero. </p>

        <p>When the fourth coordinate, w, is non-zero, we consider the
          coordinates (x,y,z,w) to represent the three-dimensional vector (x/w,y/w,z/w). Note that this
          is consistent with our previous usage, since it considers (x,y,z,1) to represent (x,y,z), as before.</p>

        <p>When the fourth coordinate is zero, there is no corresponding 3D vector, but it is possible to
          think of (x,y,z,0) as representing a 3D “point at infinity” in the direction of (x,y,z).</p>

        <p>Coordinates (x,y,z,w) used in this way are referred to as <b>homogeneous coordinates</b>.</p>

        <p> If we use homogeneous coordinates, then any 4-by-4 matrix can be used to transform threedimensional vectors,
          including matrices whose bottom row is not (0,0,0,1).</p>

        <p>Among the transformations that can be represented in this way is the projection transformation for a
          perspective projection. And in fact, this is what OpenGL does internally.</p>

        <p>It represents all three-dimensional points and vectors using homogeneous coordinates, and it represents
          all transformations as 4-by-4 matrices. You can even specify vertices using homogeneous
          coordinates.</p>

        <p>For example, the command:</p>

        <text class="center"><b>glVertex4f(x,y,z,w);</b></text>

        <p>with a non-zero value for w, generates the 3D point (x/w,y/w,z/w). Fortunately, you will almost
          never have to deal with homogeneous coordinates directly (we'll talk more about that again later).</p>




















      </article>
      <br />
    </section>


    <hr />
    <section class="main-section" id="Intro_to_3D_Graphics">
      <br />
      <header><b>Intro to 3D Graphics</b></header>
      <article>
        <p>Nowadays 3D Computer graphics, or CG, are everywhere. From video games to medical applications.</p>

        <p>The film industry is dominated by computers, and it's not just sci-fi and animation. While filming the
          Irishman, Martin Scorsese used computer effects to de-age actors Robert De Niro, Joe Pesci, and Al Pacino.</p>

        <p>It's kind of crazy when you realize that the first feature film to incorporate computer generated imagery was
          Westworld, starring Yul Brynner and James Brolin. That was in 1973, more than 50 years ago!</p>

        <p>Today, we have devices like the Meta Quest 3 and the Apple Vision Pro, which blend digital content with your
          physical space.</p>

        <p>With all of this, it's clear that 3D computer graphics have become an integral component of our everyday
          lives. But, how are these computer graphics created?</p>

        <p>The process usually begins with an artist or designer using 3D modeling software like Maya, Cinema4D, or
          Blender, just to name a few.</p>

        <p>Artists usually start off with a simple shape like a box or a sphere and then use different tools to modify
          this geometry.</p>

        <p>If not working with an artist (or being an artist yourself), there are various Graphics APIs.</p>

        <p>Graphics APIs and pipelines are crucial in computer graphics. They enable developers to create visuals for
          video games, animations, or simulations. These systems are important for managing how images are drawn and
          displayed on the screen.</p>

        <h2>What are Graphics APIs?</h2>

        <p>A Graphics API (Application Programming Interface) is a set of functions that allows programs to perform
          operations like drawing images and 3D surfaces.</p>

        <p>Essentially, APIs provide a way for software to communicate with hardware. Graphics APIs focus specifically
          on creating and displaying visual content.</p>

        <p>Every graphics program requires two key types of APIs:</p>

        <ul>
          <li>A Graphics API to handle the visual output</li>
          <li>A User-Interface API to manage user input</li>
        </ul>

        <h2>Types of Graphics APIs</h2>

        <p>There are two approaches for using graphics APIs:</p>

        <ol>
          <li>Integrated Approach</li>
          <p>Some languages, such as Java, have built-in graphics and user-interface APIs. This ensures portability.
            Where the code will work across different systems. Everything is standardized, making development simpler
            for beginners.</p>
          <li>Library-Based Approach</li>
          <p>Direct3D and OpenGL are example of these types, where drawing commands are part of a software library. This
            approach is more powerful but can vary from system to system. It offers more control, portability can be an
            issue unless developers use an additional library to handle user interface differences.</p>
        </ol>

        <h2>Examples of Popular Graphics APIs</h2>

        <p>Direct3D and OpenGL are the most popular and widely used Graphics APIs.</p>

        <ul>
          <li><b>Direct3D</b> - It is used in video games, the Direct3D is a graphics API that belongs to the DirectX
            suite by
            Microsoft. It focuses on rendering 3D graphics, allowing games to render realistic environments. The popular
            game Halo uses Direct3D to render its 3D environments.</li>
          <li><b>OpenGL</b> - It is a cross-platform graphics API that is widely used for developing both 2D and 3D
            graphics.
            It is particularly popular in CAD applications and scientific visualization. The graphics in Google Earth
            use OpenGL to render detailed maps and 3D landscapes.</li>
        </ul>

        <h2>What are Graphics Pipelines?</h2>

        <p>After the graphics APIs, let's look at the concept of graphics pipelines.</p>

        <p>The following image shows a monkey head model created using the free and open source software Blender:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/monkey%20head%201.png?raw=true"
          alt="monkey head 1">

        <p>If we inspect the model closer, we can see that it's made up of a collection of points joined together with
          simple geometries.
          This structure of joined points is known as a <b>Mesh</b>.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/monkey%20head%202.png?raw=true"
          alt="monkey head 2">

        <p>Each individual point is known as a <b>vertex</b>. A vertex is a point in space that has coordinates x, y,
          and z which determine its position in the 3D world.</p>

        <p>So, meshes are made up of vertices and vertices are made up of coordinate values. But how do we go from three
          numerical values to something on the screen?</p>

        <p>A <b>graphics pipeline</b> is a sequence of stages that transform data from a mathematical representation to
          something on a screen.</p>

        <p>In essence, all 3D objects are just data. Data can live in a 3D space, however, our screen is not 3D.
          We need to take vertices in 3D space through a series of stages in order to transform them to a 2D space.</p>

        <p>In other words, these are the process that computers follow to create 3D images and display them on a 2D
          screen. It is a series of steps that take the input, such as the geometry of 3D models, and turn it into
          pixels on the screen.</p>

        <p>Let's start with the first stage: <b>the Vertex Shader</b>.</p>

        <h3>The Vertex Shader</h3>

        <p>This is the first step where the positions of 3D points (vertices) are calculated. These vertices form the
          basic structure of 3D objects. For example, if we are rendering a car in a racing game, the position of each
          vertex of the car model is calculated in this stage

        <p>Imagine we have a simple cube. This cube geometry is defined as a list of 8 vertices.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20vertex%20shader.png?raw=true"
          alt="the vertex shader">

        <p>The vertex shader first transforms these vertices from a space where they are relative to their own origin,
          to a new space where everything is in relation to a camera's position and orientation.</p>

        <p>Finally, we take vertices from a 3D coordinate system into a 2D plane by using <b>perspective projection</b>.
        </p>

        <p>Projection creates the illusion of depth by scaling an object's coordinates based on their distance from the
          camera. This simulates how objects appear smaller as they move away from the viewer.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/perspective%20projection.png?raw=true"
          alt="perspective projection">

        <p>Now that our vertices have been transformed to a 2D space, it's time for the second stage: <b>the primitive
            assembly</b>.</p>

        <h3>The Primitive (or Triangle) Assembly</h3>

        <p>Once vertices have been transformed, it's time for the primitive assembly.</p>

        <p>In this stage, vertices are connected through geometric primitives. We can choose to use lines, points, or
          triangles.</p>

        <p>Most modern graphics hardware are optimized to process triangles efficiently, that is why we usually choose
          them over lines and points.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/primitive%20assembly.gif?raw=true"
          alt="primitive assembly">

        <p>The primitive assembly lays the groundwork for the next step, which is <b>rasterization</b>.</p>

        <h3>Rasterization</h3>

        <p>The next important step is rasterization. This is where the 3D models are converted into pixels or fragments.
          These pixels represent the final image that will appear on the screen. For example, the 3D model of a tree in
          a game is converted into a collection of colored pixels that represent the tree on the screen.</p>

        <p>We now have geometry made up of 3 dimensional vertices projected onto a 2D screen.</p>

        <p>The rasterization step determines which pixels are inside the triangle. It breaks the shape into small
          fragments or pixels.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/rasterization.png?raw=true"
          alt="rasterization">

        <p>Rasterization not only determines which pixels are inside the shape, but also improves performance by
          discarding triangles which are not visible or occluded. This is known as <b>Culling</b>.</p>

        <p>Culling determines which triangles are facing away from the camera, are outside of the camera's view, or
          simply hidden by another object.</p>

        <p>In this case the triangle gets discarded and the computational load on the GPU (Graphical Processing Unit )
          is reduced.</p>

        <h3>The Fragment Shader</h3>

        <p>The next stage in the pipeline is known as the fragment shader.</p>

        <p>After rasterization, the triangle is broken into pixel fragments. These individual pixel fragments are then
          processed by the fragment shader.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/fragment%20shader.png?raw=true"
          alt="the fragment shader">

        <p>In this step we determine a color for each of the pixels that belong to the triangle. We do this by taking
          into account the material properties of the object, textures, and light sources in the scene.</p>

        <p>With all of these information, we perform some calculations that determine the final color of the fragment.
        </p>

        <h3>Framebuffer</h3>

        <p>Finally, all of the shaded fragments are copied to the framebuffer, which is basically the image displayed on
          the screen.</p>

        <p>You can think of the framebuffer as a canvas where everything gets stored before being displayed on the
          screen.</p>

        <img class="center" style="width: 90%; height: 90%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/the%20graphics%20pipeline.png?raw=true"
          alt="the graphics pipeline">

        <p>This is a general overview of a typical graphics pipeline. Note that there may be different pipelines, which
          have additional steps that we ignored.</p>

        <p>Earlier versions of graphics libraries offered "fixed" pipeline. This means that the process had predefined
          stages and operations. Specifically, the vertex and fragment shaders. Developers had limited control over how
          these specific stages behaved.</p>

        <p>With older pipelines, we would simply define the vertex data, material data, and lighting. Then, we would
          send this data to the GPU and we'd get a result. This was easier for beginners, but provided little room for
          modification and customization. For example, the lighting model used could not be changed.</p>

        <p>In modern graphics libraries, "fixed" pipelines are deprecated in favor of "programmable" pipelines. Modern
          pipelines provide greater flexibility and control over the rendering process. However, you must provide the
          code for the vertex and fragment shader yourself. This means more control for the developer, but an additional
          layer of difficulty for beginners.</p>

        <p>Shader programs are written in a C style language called GLSL (OpenGL Shading Language). Shader programs run
          on the GPU, which is different from how programs run on the CPU. The CPU executes tasks sequentially, while
          the GPU executes tasks in parallel.</p>

        <p>The fact is that this extra layer is worth it! We can achieve great results with these new pipelines. There
          is also a thriving community of creative individuals that are constantly pushing this technology even further.
        </p>





      </article>
      <br />
    </section>




    <hr />
    <section class="main-section" id="Graphic_APIs">
      <br />
      <header><b>Graphic APIs</b></header>
      <article>
        <p></p>

        <h2>OpenGL</h2>


        <p>OpenGL is generally considered to be a cross-platform graphics API for high-performance 2D and 3D graphics
          rendering. In fact, OpenGL is not an API. It is a specification developed and maintained by the Khronos
          organization.</p>

        <p>The specification strictly defines how each function should be executed and what their outputs should be. How
          each function is implemented internally is up to the developer.</p>



        <p>It is widely used in video games, CAD, virtual reality, scientific visualization, and more.</p>

        <h2>WebGL</h2>

        <p>WebGL (Web Graphics Library) is a JavaScript API for rendering interactive 2D and 3D graphics within any
          compatible web browser without the use of plug-ins.</p>

        <p>It is based on OpenGL ES (a subset of OpenGL for embedded systems).</p>

        <p>Major browser vendors Apple (Safari), Google (Chrome), Microsoft (Edge), and Mozilla (Firefox) are members of
          the WebGL Working Group.</p>

        <h2>Three.js</h2>

        <p>Three.js is a popular JavaScript library that simplifies the creation and display of 3D graphics in a web
          browser using WebGL.</p>

        <p>It supports VR and AR, offers cross-browser compatibility via WebGL, provides extensive tools for adding
          materials, textures, and animations, and allows for the integration of models from other 3D modeling software.
        </p>

        <h3>Key Features</h3>

        <ul>
          <li><b>Scene Graph</b>: It uses a scene graph structure, allowing developers to create and manage 3D objects,
            cameras, lights, and other elements in a hierarchical manner.</li>
          <li><b>Geometries and Materials</b>: Three.js provides a variety of built-in geometries (e.g., cubes, spheres,
            planes) and materials (e.g., basic, lambert, phong, standard) that can be easily customized and combined.
          </li>
          <li><b>Animation</b>: The library supports animations, including skeletal animations, morph targets, and
            keyframe animations, making it suitable for creating animated 3D content.</li>
          <li><b>Shaders and Post-Processing</b>: Three.js allows the use of custom shaders written in GLSL and supports
            post-processing effects such as bloom, depth of field, and motion blur.</li>
        </ul>

      </article>
      <br />
    </section>

    <hr />
    <section class="main-section" id="Intro_to_Three.js">
      <br />
      <header><b>Intro to Three.js</b></header>
      <article>
        <p>All modern browsers became more powerful and more accessible directly using JavaScript. They have adopted
          WebGL (Web Graphics Library) and as previously mentioned, WebGL is a Javascript API that allows you to render
          2D and 3D graphics without the use of plugins.
          Trying to create 3D elements with WebGL would involve writing lots of code and can get fairly complex.</p>

        <p>Three.js helps simplify the process and is essentially an abstracted layer on top of WebGL that makes it
          easier to use.</p>

        <p>Three.js is an open-source, lightweight, cross-browser, general-purpose JavaScript library. Three.js uses
          WebGL behind the scenes, so you can use it to render Graphics on an HTML canvas element in the browser.</p>

        <p>Since Three.js uses JavaScript, you can interact with other web page elements, add animations and
          interactions,
          and even create a game with some logic.</p>

        <p>Three.js works with the HTML canvas element, the same thing that we used for 2D graphics. In almost all web
          browsers, in addition to its 2D Graphics API, a canvas also
          supports drawing in 3D using WebGL, which is used by three.js and which is about as different
          as it can be from the 2D API.</p>

        <h2>Why use Three.js?</h2>

        <p>The following features make Three.js an excellent library to use:</p>

        <ul>
          <li>You can create complex 3D graphics by just using JavaScript.</li>
          <li>You can create Virtual Reality (VR) and Augmented Reality (AR) scenes inside the browser.</li>
          <li>Since it uses WebGL, it has cross-browser support. Many browsers support it.</li>
          <li>You can add various materials, textures and animate 3D objects.</li>
          <li>You can also load and work on objects from other 3D modeling software.</li>
        </ul>

        <p>With a couple of lines of JavaScript and simple logic, you can create anything, from highperformance
          interactive 3D models to photorealistic real-time scenes.</p>

        <p>We'll start by adding the following to our HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Before you can use three.js, you need somewhere to display it.</p>
        <p>To actually be able to display anything with three.js, we need three things:</p>

        <ul>
          <li>Scene</li>
          <li>Camera</li>
          <li>Renderer</li>
        </ul>

        <pre><code class="javascript">const scene = new THREE.Scene(); 
const camera = new THREE.PerspectiveCamera(75, 500 / 400, 0.1, 1000);  
const renderer = new THREE.WebGLRenderer(); 
renderer.setSize(500, 400); 
document.body.appendChild(renderer.domElement);</code></pre>

        <p>Now, let's break down each of these components.</p>

        <h2>Scene</h2>

        <p>A scene is the container object that includes everything we will be rendering, such as geometries, lights,
          and cameras. In a sense, it's much like a stage where you place all the elements you want to display.</p>

        <p>We create an instance of the scene object using the constructor with no parameters: <b>THREE.Scene()</b></p>

        <p>The function scene.add(item) can be used to add cameras, lights, and graphical objects to
          the scene. It is probably the only scene function that you will need to call. The function
          scene.remove(item), which removes an item from the scene, is also occasionally useful.</p>

        <h2>Camera</h2>

        <p>Next, we create a camera which will be how we view the scene.</p>

        <p>The camera displays the objects in the scene, with varying points of view depending on which type of camera
          is used.</p>

        <p>The Three.js library provides two main cameras: <b>Orthographic</b> and <b>Perspective</b>.</p>

        <ul>
          <li><b>OrthographicCamera</b>: uses orthographic projection, meaning that elements viewed with this camera
            <b>maintain a constant size, regardless of their distance from the camera</b>. <br /><br />
            This can be popular in top down 2D games like SimCity and UI elements, amongst other
            things.
          </li>
          <br />
          <li><b>PerspectiveCamera</b>: uses perspective projection. This type of camera is designed to mimic the
            way the human eye sees. <b>So the closer an object is, the larger it will appear and the farther it is, the
              smaller it will appear</b>.
            <br /><br />It is the most common projection mode used for rendering a 3D scene.
          </li>
        </ul>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/orthographic%20v%20perspective.png?raw=true"
          alt="orthographic vs perspective">

        <p>In a perspective view (left), <b>edges that are farther away appear shorter</b>.</p>

        <p>In an Orthographic view (right), <b>far-away edges are the same size as nearby ones</b>.</p>

        <p>The constructors specify
          the projection, using parameters that are familiar from OpenGL:</p>

        <text><b>camera = new THREE.OrthographicCamera(left, right, top, bottom, near, far);</b></text>

        <p>or: </p>

        <text><b>camera = new THREE.PerspectiveCamera(fieldOfViewAngle, aspect, near, far);</b></text>

        <p>Since the PerspectiveCamera is more common, let's go over its four attributes:</p>

        <ul>
          <li>Field of View (FOV)</li>
          <li>Aspect</li>
          <li>Near</li>
          <li>Far</li>
        </ul>

        <p>The first parameter it takes is the <b>field of view (FOV)</b>, <b>the maximum area of the scene that can be
            viewed</b>. To help visualize, picture a cone coming out of a camera lens, and the flat end of the cone is
          the space the camera can see. This value is measured in degrees.</p>

        <p>Next is the <b>aspect ratio</b>, which is the proportion of the width to the height of the element.</p>

        <p>You almost always want to use <b>the width of the element divided by the height</b>, or you'll get the same
          result as
          when you play old movies on a widescreen TV - the image looks squished.</p>

        <p>The <b>near and far attributes</b> are the <b>minimum and maximum distance from the camera at which the
            camera will render scene objects</b>.
          Anything beyond these values, either too close or too far from the camera, will not be rendered.</p>

        <p>Finally, once the camera is set up, you need to add it to the scene.</p>

        <p>Remember to append everything to your container to make it show on your scene.</p>

        <p>Keep in mind that by default, when you add something to the scene, it will be placed in the center at the XYZ
          coordinates 0, 0, 0 respectively. So if you have trouble seeing anything you have added render in the scene,
          chances are you might need to position your camera further out of view.</p>

        <p>For example, setting the camera's Z coordinate to 5:</p>

        <pre><code class="javascript">camera.position.z = 5;</code></pre>

        <h2>Renderer</h2>

        <p>The last step is to render the scene. The renderer is what compiles it all together and draws the scene.</p>

        <p>We need to create the renderer instance, we will use WebGLRenderer() but
          three.js comes with others as well, often used as fallbacks for users with
          older browsers/who don't have WebGL support.</p>

        <p>We also need to set the size at which we want it to render our app.</p>

        <p>In the following examples we will use a width of 500 and a height of 400, though it is
          common to use the width and height of the browser window (window.innerWidth, window.innerHeight) as well.</p>

        <p>Lastly, we add the renderer element to our HTML document. This is a canvas element the renderer uses to
          display the scene to us.</p>

        <p>A renderer is an instance of the class THREE.WebGLRenderer. Its constructor has one
          parameter, which is a JavaScript object containing settings that affect the renderer.</p>

        <p>All together our code should look something like this:</p>

        <pre><code class="javascript">const scene = new THREE.Scene(); //Creates a new instance of the Scene class.
const camera = new THREE.PerspectiveCamera(50, 500 / 400, 0.1, 1000); //Creates a new instance of the PerspectiveCamera class with a field of view of 50 degrees, an aspect ratio of 500/400, a near clipping plane of 0.1, and a far clipping plane of 1000.
camera.position.z = 5; //Moves the camera 5 units away from the center of the scene.
const renderer = new THREE.WebGLRenderer(); //Creates a new instance of the WebGLRenderer class.
renderer.setSize(500, 400); //Sets the size of the renderer to 500px by 400px.
document.body.appendChild(renderer.domElement); //Appends the renderer's canvas element to the document body to display the rendered scene.</code></pre>

        <p>However, just defining an instance of the renderer is not enough to see the results of our code yet, because
          we have not actually rendered anything as of
          yet.</p>

        <p>To achieve this, we will need to create a render or animate loop, like this:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}

animate();</code></pre>

        <p>We have now created a loop named animate() that causes the renderer to
          draw the scene every time the screen is refreshed (on a typical screen, this is
          about 60 times per second), and called it immediately after.</p>

        <p>Using requestAnimationFrame() has a number of advantages, such as
          pausing when the user navigates to another browser tab, saving processing
          power and battery life.</p>

        <p>Now we've successfully set up out scene! Now for the fun part, we can start creating the shapes that we would
          like
          to add to our scene.</p>
        <p>Much like how we need three elements to set a scene, objects require three elements:
        </p>

        <ul>
          <li>Geometry</li>
          <li>Material</li>
          <li>Mesh</li>
        </ul>

        <p>Let's start with the first element, the geometry.</p>

        <h2>Geometry</h2>

        <p>The geometry defines the shape of the objects we draw in Three.js.</p>

        <p>It is made up of a collection of vertices and faces which combine
          three vertices into a triangle face.</p>

        <p>There are two main types of geometries in Three.js:</p>

        <ul>
          <li><b>Custom Geometry</b>: created by defining the vertices and faces of an object from
            scratch (by using THREE.BufferGeometry())</li>
          <br />
          <li><b>Built-in Geometry</b>: the Three.js library provides a number of different geometries you can use and
            customize yourself. Each geometry has its own set of parameters. (i.e. a BoxGeometry accepts attributes for
            width, height, and depth while a SphereGeometry's parameters are radius, widthSegments, and heightSegments,
            etc.)</li>
        </ul>

        <img class="center" style="width: 80%; height: 80%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/three%20js%20geometries.png?raw=true"
          alt="geometry">

        <p>Here are some constructors, listing all
          parameters (but keep in mind that most of the parameters are optional):</p>

        <ul>
          <li>new THREE.BoxGeometry(width, height, depth,
            widthSegments, heightSegments, depthSegments)</li>
          <li>new THREE.PlaneGeometry(width, height, widthSegments, heightSegments)</li>
          <li>new THREE.RingGeometry(innerRadius, outerRadius, thetaSegments, phiSegments,
            thetaStart, thetaLength)
          </li>
          <li>new THREE.ConeGeometry(radiusBottom, height, radiusSegments,
            heightSegments, openEnded, thetaStart, thetaLength)</li>
          <li>new THREE.SphereGeometry(radius, widthSegments, heightSegments,
            phiStart, phiLength, thetaStart, thetaLength)</li>
          <li>new THREE.TorusGeometry(radius, tube, radialSegments, tubularSegments, arc)
          </li>
        </ul>

        <p>The class <b>BoxGeometry</b> represents the geometry of a rectangular box centered at the origin.</p>

        <p>Its constructor has three parameters to give the size of the box in each direction; their default
          value is one.</p>

        <p>The last three parameters give the number of subdivisions in each direction, with
          a default of one; values greater than one will cause the faces of the box to be subdivided into
          smaller triangles.</p>

        <p>The class <b>PlaneGeometry</b> represents the geometry of a rectangle lying in the xy-plane,
          centered at the origin. Its parameters are similar to those for a cube. </p>

        <p>A <b>RingGeometry</b> represents an annulus, that is, a disk with a smaller disk removed from its center. The
          ring lies in the
          xy-plane, with its center at the origin. You should always specify the inner and outer radii of
          the ring.</p>

        <p>The constructor for <b>ConeGeometry</b> has exactly the same form and effect as the constructor
          for <b>CylinderGeometry</b>, with the radiusTop set to zero. That is, it constructs a cone with axis
          along the y-axis and centered at the origin</p>

        <p>For <b>SphereGeometry</b>, all parameters are optional.</p>

        <p>The constructor creates a sphere centered
          at the origin, with axis along the y-axis. </p>

        <p>The first parameter, which gives the radius of the
          sphere, has a default of one.</p>

        <p>The next two parameters give the numbers of slices and stacks,
          with default values 32 and 16.</p>

        <p> The last four parameters allow you to make a piece of a sphere;
          the default values give a complete sphere. The four parameters are angles measured in radians.
          phiStart and phiLength are measured in angles around the equator and give the extent in
          longitude of the spherical shell that is generated.</p>

        <p>For example:</p>

        <text><b>new THREE.SphereGeometry(5, 32, 16, 0, Math.PI)</b></text>

        <p>creates the geometry for the "western hemisphere" of a sphere.</p>

        <p>The last two parameters are
          angles measured along a line of latitude from the north pole of the sphere to the south pole.</p>

        <p>For example, to get the sphere's "northern hemisphere":</p>

        <text><b>new THREE.SphereGeometry(5, 32, 16, 0, 2*Math.PI, 0, Math.PI/2)</b></text>

        <p>For <b>TorusGeometry</b>, the constructor creates a torus lying in the xy-plane, centered at the
          origin, with the z -axis passing through its hole.</p>

        <p>The parameter radius is the distance from the
          center of the torus to the center of the torus's tube, while tube is the radius of the tube. The
          next two parameters give the number of subdivisions in each direction.</p>

        <p>The last parameter,
          arc, allows you to make just part of a torus. It is an angle between 0 and 2*Math.PI, measured
          along the circle at the center of the tube.</p>

        <p>There are also geometry classes representing the regular polyhedra: THREE.TetrahedronGeometry,
          THREE.OctahedronGeometry, THREE.DodecahedronGeometry, and THREE.IcosahedronGeometry.
          (For a cube use a BoxGeometry.) </p>

        <p>The constructors for these four classes take two parameters.
          The first specifies the size of the polyhedron, with a default of 1. The size is given as the radius
          of the sphere that contains the polyhedron. </p>

        <p>The second parameter is an integer called detail.
          The default value, 0, gives the actual regular polyhedron. Larger values add detail by adding
          additional faces. </p>

        <p>As the detail increases, the polyhedron becomes a better approximation for a
          sphere. This is easier to understand with an illustration:</p>

        <!--ADD IMAGE HERE-->

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/mesh%20objects%20example.png?raw=true"
          alt="icosahedral geometries">

        <p>The image shows four mesh objects that use icosahedral geometries with detail parameter equal
          to 0, 1, 2, and 3.</p>

        <h2>Material</h2>

        <p>Materials describe the appearance of objects.</p>

        <p>They are defined in a (mostly) renderer-independent way, so you don't have
          to rewrite materials if you decide to use a different renderer.</p>

        <p>There are different types of materials:</p>

        <ul>
          <li><b>MeshBasicMaterial</b>: This is the most boring material you can get in Three.js world, but the benefit
            is, it doesn't need a light for it to show.
            <br /><br />
            Just pass a color in as a parameter to get a solid colored
            object, which has no shading and is not affected by lights
          </li>
          <br />
          <li><b>MeshNormalMaterial</b>: colors the faces of the mesh differently based on the
            face's normal or what direction they are facing</li>
          <p>The next materials however, DO require lights in order to be seen:</p>
          <li><b>MeshLambertMaterial</b>: responds to lights and gives our geometry shading
            with a dull surface, computes lighting only at the vertices</li>
          <br />
          <li><b>MeshPhongMaterial</b>: similar to Lambert, responds to lights but adds a
            metallic luster to the surface, reflecting light with more intensity, computes
            lighting at every pixel</li>
          <br />
          <li><b>MeshStandardMaterial</b>: combines Lambert and Phong into a single material,
            has properties for roughness and metalness and adjusting these can create
            both dull or metallic looking surfaces
          </li>
          <br />
          <li><b>MeshDepthMaterial</b>: draws the mesh grayscale from black to white based on
            the depth of the content</li>
          <br />
          <li><b>MeshToonMaterial</b>: toon shading or cel shading is a type of non-photorealistic rendering technique
            designed to make 3D computer graphics appear more cartoonish by using less shading color instead of a smooth
            gradient effect.</li>
        </ul>

        <img class="center" style="width: 80%; height: 80%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/three%20js%20materials.png?raw=true"
          alt="material">

        <h2>Mesh</h2>

        <p>A Mesh is a class representing triangular polygon mesh based objects.</p>

        <p>A Mesh pairs a Geometry and a Material in order to draw/create an object.</p>

        <p>Both Material objects and Geometry objects can be used by multiple Mesh
          objects.</p>

        <img class="center" style="width: 80%; height: 80%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/three%20js%20mesh.png?raw=true"
          alt="mesh">

        <p>Now that we have our scene, camera, renderer, geometry, material, and mesh, we can start creating our
          objects.</p>

        <h2>Example: Creating a Basic Triangle</h2>

        <p>Although we'll mostly focus on 3D objects in Three.js, let's start with a simple 2D triangle.</p>

        <p>We can create this triangle using BufferGeometry:</p>

        <pre><code class="javascript">var size = 2;

function createTriGeometry(size) {
    const triGeom = new THREE.BufferGeometry();

    // Define the vertices of the triangle
    const vertices = new Float32Array([
        0, 0, 0, // Vertex 1
        size, 0, 0, // Vertex 2
        0, size, 0  // Vertex 3
    ]);

    // Set the vertices as attribute
    triGeom.setAttribute('position', new THREE.BufferAttribute(vertices, 3));

    // Define the indices for the single triangle face
    const indices = new Uint16Array([
        0, 1, 2 // A single face made up of the three vertices
    ]);

    // Set the indices
    triGeom.setIndex(new THREE.BufferAttribute(indices, 1));

    return triGeom;
}

// Create the triangle geometry
var triGeom1 = createTriGeometry(size);

// Define the material for the triangle
var triMaterial = new THREE.MeshBasicMaterial({ 
    color: new THREE.Color("red"), 
    side: THREE.DoubleSide // Render both sides of the triangle
});

// Create the mesh with geometry and material
var triMesh1 = new THREE.Mesh(triGeom1, triMaterial);

// Add triangle to the scene
scene.add(triMesh1);</code></pre>

        <p>In this code we:</p>

        <ul>
          <li>Create a BufferGeometry object for the triangle.</li>
          <li>Define the vertices of the triangle and set them as an attribute of the geometry.</li>
          <li>Define the indices for the single triangle face and set them.</li>
          <li>Create a MeshBasicMaterial object for the triangle.</li>
          <li>Create a Mesh object by passing in the geometry and material objects, and add it to the scene.</li>
        </ul>

        <p>Now, when we run the code, we should see a red triangle rendered in the scene.</p>

        <p>Let's adjust the background color:</p>

        <pre><code class="javascript
">scene.background = new THREE.Color("blue");</code></pre>

        <p>Now our red triangle should be displayed on a blue background.</p>

        <h2>Example: Creating a Simple Cube</h2>

        <p>Let's create a simple square using BufferGeometry:</p>

        <pre><code class="javascript">const geometry = new THREE.BufferGeometry();
// create a simple square shape. We duplicate the top left and bottom right
// vertices because each vertex needs to appear once per triangle.
const vertices = new Float32Array( [
	-1.0, -1.0,  1.0, // v0
	 1.0, -1.0,  1.0, // v1
	 1.0,  1.0,  1.0, // v2

	 1.0,  1.0,  1.0, // v3
	-1.0,  1.0,  1.0, // v4
	-1.0, -1.0,  1.0  // v5
]);

geometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));
const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });
const cube = new THREE.Mesh(geometry, material);

scene.add(cube);</code></pre>

        <p>Now, when you run the code, you should see a green square (cube) rendered in the scene.</p>

        <p>Here we create a BufferGeometry object and pass in an array
          of vertices that define the square's shape.</p>

        <p>Next, we create a MeshBasicMaterial object and pass in a color
          parameter to give the square a green color.</p>

        <p>Finally, we create a Mesh object by passing in the geometry and
          material objects, and add it to the scene.</p>

        <p>Notice this only makes a 2D square out of two triangles, but we can
          make a 3D cube by adding more vertices and faces.</p>

        <pre><code class="javascript">// Define vertices for a cube
const vertices = new Float32Array([
    // Front face
    -1.0, -1.0,  1.0,
     1.0, -1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0, -1.0,  1.0,

    // Back face
    -1.0, -1.0, -1.0,
    -1.0,  1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0, -1.0, -1.0,
    -1.0, -1.0, -1.0,

    // Top face
    -1.0,  1.0, -1.0,
    -1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0, -1.0,
    -1.0,  1.0, -1.0,

    // Bottom face
    -1.0, -1.0, -1.0,
     1.0, -1.0, -1.0,
     1.0, -1.0,  1.0,
     1.0, -1.0,  1.0,
    -1.0, -1.0,  1.0,
    -1.0, -1.0, -1.0,

    // Right face
     1.0, -1.0, -1.0,
     1.0,  1.0, -1.0,
     1.0,  1.0,  1.0,
     1.0,  1.0,  1.0,
     1.0, -1.0,  1.0,
     1.0, -1.0, -1.0,

    // Left face
    -1.0, -1.0, -1.0,
    -1.0, -1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0,  1.0,  1.0,
    -1.0,  1.0, -1.0,
    -1.0, -1.0, -1.0
]);

// Create geometry and material
const geometry = new THREE.BufferGeometry();
geometry.setAttribute('position', new THREE.BufferAttribute(vertices, 3));

const material = new THREE.MeshBasicMaterial({ color: 0x00ff00 });

// Create cube mesh
const cube = new THREE.Mesh(geometry, material);
scene.add(cube);</code></pre>

        <p>Now we should be able to see a cube since we have defined the vertices of each of the six faces.</p>

        <p>Understanding this is important, although not entirely necessary since we are merely creating a cube.
          We can simplify this process by using BoxGeometry()
          instead of BufferGeometry().</p>

        <p>BoxGeometry is a geometry class for a rectangular cuboid with a given 'width', 'height',
          and 'depth'. How can we obtain the same scaled 2D shape using BoxGeometry()?</p>

        <p>To create a cube without declaring all vertices, we use BoxGeometry. This is an object that contains all the
          points (vertices) and fill (faces) of the cube.</p>

        <p>Let's set the width, height, and depth to 2 respectively, because our previous square had a width, height,
          and depth of 2 as well! (if we do not set it at all, the w/h/d defaults to 1, try changing the values! How
          does this change the object?)</p>

        <pre><code class="javascript">const geometry = new THREE.BoxGeometry(2, 2, 2);</code></pre>

        <p>Notice how since our structure is a simple cube we have no need for a
          group of vertices since the same result can be obtained in one line of
          code!</p>

        <p>We'll definitely use vertices for more irregular polygons but for
          now, BoxGeometry works just fine!</p>

        <p>Let's add X and Y-axis rotation to our cube. We can do this simply by adding
          the following to animate():</p>

        <pre><code class="javascript">//Now to make it more interesting and dimensional, let's add a rotation animation
cube.rotation.x += 0.01;
cube.rotation.y += 0.01;</code></pre>

        <p>We can increase/decrease the speed of rotation by changing the values of
          cube.rotation.x and cube.rotation.y</p>

        <p>Now, when you run the code, you should see the cube rotating on both the X and Y axes.</p>

        <p>Let's make a multi-colored cube:</p>

        <p>We can achieve this using an array of materials.</p>

        <pre><code class="javascript
">const materials = [
    new THREE.MeshBasicMaterial({ color: "green" }),
    new THREE.MeshBasicMaterial({ color: "blue" }),
    new THREE.MeshBasicMaterial({ color: "red" }),
    new THREE.MeshBasicMaterial({ color: "yellow" }),
    new THREE.MeshBasicMaterial({ color: "orange" }),
    new THREE.MeshBasicMaterial({ color: "purple" })
];</code></pre>

        <p>Now, when we run the code, we should see a cube with different colored faces.</p>

        <p>Let's revert back to a singularly colored cube.</p>

        <p>We can also change our cube to simply display the wireframe of our geometry by modifying our material:</p>

        <pre><code class="javascript
">const material = new THREE.MeshBasicMaterial({ color: "green", wireframe: true });</code></pre>

        <p>Now let's change the material of our cube to MeshNormalMaterial:</p>

        <pre><code class="javascript">const material = new THREE.MeshNormalMaterial();</code></pre>

        <p>Now, let's look at a wireframe to our cube:</p>

        <pre><code class="javascript">const wireframe = new THREE.WireframeGeometry(geometry);
const line = new THREE.LineSegments(wireframe);
line.material.depthTest = false;
line.material.opacity = 1;
line.material.transparent = true;
scene.add(line);</code></pre>

        <p>This is good if we want a wireframe in addition to our solid object.</p>

        <p>If we want it to rotate remember to update the rotations as well.</p>

        <h2>Additional Tools</h2>

        <p>A helpful tool to implement when creating your projects is <b>dat.GUI</b>.</p>

        <p>In Javascript, dat.GUI is a lightweight graphics controller API. It allows the user to manipulate variables
          easily and activate functions inside the application by providing a graphical controller interface.</p>

        <p>This API is frequently used in Three.js for making changes to a scene without having to refer directly to the
          code.</p>

        <p>It will be displayed as a small box in the corner of your screen and you can play around with the parameters
          and see the newly applied changes happen live.</p>

        <p>It's a great tool to use when you want to experiment with different values and see how they affect your
          project.</p>

        <p>Here's how we can add dat.GUI to our project:</p>

        <p>First, we need to ensure that we have access to dat.GUI by adding this line to our HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdnjs.cloudflare.com/ajax/libs/dat-gui/0.7.6/dat.gui.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Next, we can add the following code to our JavaScript:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
gui.add(cube.rotation, 'x', 0, Math.PI * 2);
gui.add(cube.rotation, 'y', 0, Math.PI * 2);</code></pre>

        <p>Now, when we run the code, we should see the dat.GUI box in the corner of our screen.</p>

        <p>By changing the values of the sliders, we can adjust the rotation of the cube on the X and Y axes.</p>

        <p>And that's it! We've successfully added dat.GUI to our project.</p>

        <p>Let's add a color changer to our dat.GUI as well:</p>

        <pre><code class="javascript">// Add GUI for controls
const gui = new dat.GUI();
const cubeFolder = gui.addFolder('Cube Properties');

const cubeParams = {
    color: `#${material.color.getHexString()}`,
    rotationX: 0,  // New parameter for X rotation angle
    rotationY: 0   // New parameter for Y rotation angle
};

// Color control
cubeFolder.addColor(cubeParams, 'color').onChange((value) => {
    cube.material.color.set(value);
});

// X rotation control
cubeFolder.add(cubeParams, 'rotationX', -Math.PI, Math.PI).name('Rotation X').onChange((value) => {
    cube.rotation.x = value;
});

// Y rotation control
cubeFolder.add(cubeParams, 'rotationY', -Math.PI, Math.PI).name('Rotation Y').onChange((value) => {
    cube.rotation.y = value;
});

cubeFolder.open();</code></pre>

        <h2>Exercise: Let's Create a Sphere: </h2>

        <p>Now that we've created a cube, let's create a sphere using SphereGeometry().</p>

        <pre><code class="javascript">const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(50, 500 / 400, 0.1, 1000);
const renderer = new THREE.WebGLRenderer();
renderer.setSize(500, 400);
document.body.appendChild(renderer.domElement);

camera.position.z = 7;

//sphere:
const material = new THREE.MeshBasicMaterial({
    color: "orange"
});
const geometry = new THREE.SphereGeometry(2, 50, 50); //radius, widthSegments, heightSegments
const sphere = new THREE.Mesh(geometry, material);
scene.add(sphere);

function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}

animate();</code></pre>

        <p>We should see a basic Three.js code snippet creating an orange sphere on canvas. Nothing fancy. The sphere is
          made from a basic material which does not respond to light, therefore we can see the model as is.</p>

        <p>We can turn this sphere into a wireframe like we did before, by using WireframeGeometry or we can simply add
          another specification to our original material.</p>

        <p>Let's add a wireframe to our sphere:</p>

        <pre><code class="javascript">const material = new THREE.MeshBasicMaterial({
    color: "orange",
    wireframe: true //sets the wireframe
});</code></pre>

        <p>Now, when we run the code, we should see an orange wireframe sphere rendered in the scene.</p>

        <p>Let's rotate it by adding the following lines to our animate function:</p>

        <pre><code class="javascript">sphere.rotation.x += 0.01;
sphere.rotation.y += 0.01;</code></pre>

        <p>Now, when you run the code, you should see the sphere rotating on both the X and Y axes.</p>

        <p>Let's add dat.GUI to our sphere that only includes a checkbox that helps us toggle from our original orange
          color to a blue wireframe:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
const sphereFolder = gui.addFolder('Sphere Properties');

const sphereParams = {
    blue: false
};

// Wireframe color control
sphereFolder.add(sphereParams, 'blue').name('blue wireframe').onChange((value) => {
    if (value) {
        sphere.material.color.set('blue');    
    } else {
        sphere.material.color.set('orange');    
    }
});

sphereFolder.open(); //to start with the folder open</code></pre>

        <p>Now when you run the code, you should see a checkbox in the dat.GUI box that allows you to toggle between the
          original orange color and a blue wireframe for the sphere.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Lights_and_Interactivity">
      <br />
      <header><b>Lights</b></header>
      <article>

        <p>So we've seen objects with simple materials like MeshBasicMaterial and MeshNormalMaterial that did not need
          lighting, however
          to render a scene realistically we need to add lights.</p>

        <p>There are several types of lights in Three.js:</p>

        <ul>
          <li><b>AmbientLight</b>: This light is used to simulate global illumination. It lights up all objects in the
            scene equally.</li>
          <li><b>HemisphereLight</b>: This light is used to simulate the light coming from the sky. It is used to create
            a gradient sky color.</li>
          <li><b>DirectionalLight</b>: This light is used to simulate light that is coming from a specific direction. It
            is similar to sunlight.</li>
          <li><b>PointLight</b>: This light is used to simulate light that is coming from a specific point in space. It
            is similar to a light bulb.</li>
          <li><b>SpotLight</b>: This light is used to simulate light that is coming from a specific point in space in a
            specific direction. It is similar to a flashlight.</li>
        </ul>

        <h2>Ambient Light</h2>

        <p>AmbientLight does not point or direct light toward a direction, so it cannot
          cast shadows, contributes just a little light/brightness to the scene. </p>

        <p>The first parameter is the color and the second parameter is the intensity:</p>

        <text class="center"><b>var light = new THREE.AmbientLight(color, intensity);</b></text>

        <pre><code class="javascript">// Ambient light
const ambientLight = new THREE.AmbientLight("green", 1); //color, intensity
scene.add(ambientLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/abByRov">Ambient Light Example</a></p>

        <p>Notice how if we remove the light from our scene we cannot see the cube anymore. That is because it is made
          out of MeshPhongMaterial, we'll talk more about that a bit later with shading.</p>

        <p>Just know that MeshPhongMaterial is a material that responds to lights and adds a metallic luster to the
          surface, reflecting
          light with more intensity. It computes lighting at every pixel.</p>

        <p>If all you have is an AmbientLight, you'll have the same effect as for a MeshBasicMaterial because all faces
          of the geometries will be lit equally.</p>

        <h2>Hemisphere Light</h2>

        <p>Like ambient light, hemisphere light has no position or direction. But unlike ambient light it has two
          colors.</p>

        <ul>
          <li>One to simulate the color coming from above, like a sun or ceiling light source.</li>
          <li>The other is the color of light coming from below, to simulate the light reflected off the floor or ground
            surface.</li>
        </ul>

        <p>This is to provide a bit more realism than a globally applied ambient light.</p>

        <text class="center"><b>var light = new THREE.HemisphereLight(skyColor, groundColor, intensity);</b></text>

        <pre><code class="javascript
">// Hemisphere light
const hemisphereLight = new THREE.HemisphereLight("blue", "green", 1); //skyColor, groundColor, intensity
scene.add(hemisphereLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/qBqXJBN">Hemisphere Light Example</a>
        </p>

        <p>Notice how the cube is lit from the top with a blue light and from the bottom with a green light.</p>

        <h2>Directional Light</h2>

        <p>Direcional lights resemble the Sun. They're a distant powerful light source pointing in one direction.</p>

        <p>They are positioned infinitely far away and emit light in a specific direction.</p>

        <p>They have a color and intensity.</p>

        <p>Directional lights are useful for simulating sunlight in a scene.</p>

        <text class="center"><b>var light = new THREE.DirectionalLight(color, intensity);</b></text>

        <pre><code class="javascript
">// Directional light
const directionalLight = new THREE.DirectionalLight("white", 1); //color, intensity
directionalLight.position.set(0, 1, 0); //x, y, z
scene.add(directionalLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/rNWzqaN">Directional Light Example</a>
        </p>

        <p>Notice how the cube is lit from the top right corner. This is because the directional light is positioned at
          (0, 1, 0).</p>

        <p>Let's add a second directional light to our scene:</p>

        <pre><code class="javascript
">// Directional light 2
const directionalLight2 = new THREE.DirectionalLight("red", 1); //color, intensity
directionalLight2.position.set(0, -1, 0); //x, y, z
scene.add(directionalLight2);</code></pre>

        <p>Now how does the cube look?</p>

        <h2>Point Light</h2>

        <p>Point lights are like light bulbs in your scene.</p>

        <p>They are positioned at a specific location and radiate light outwards in all directions from that postion.
          They have a color and intensity.</p>

        <p>Point lights allow you to set the distance from the light at which point it's intensity is zero. You can also
          set the decay which is the amount the light dims along the distance.</p>

        <text class="center"><b>var light = new THREE.PointLight(color, intensity, distance, decay);</b></text>

        <pre><code class="javascript">// Point light
const pointLight = new THREE.PointLight("white", 1); //color, intensity
pointLight.position.set(5, 5, 5); //x, y, z
scene.add(pointLight);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/ZEBJqYg">Point Light Example</a></p>

        <p>Notice how the cube is lit from the top right corner. This is because the point light is positioned at (5, 5,
          5).</p>

        <p>Let's add a second point light to our scene:</p>

        <pre><code class="javascript">// Point light 2
const pointLight2 = new THREE.PointLight("blue", 1); //color, intensity
pointLight2.position.set(-5, -5, -5); //x, y, z
scene.add(pointLight2);</code></pre>

        <h2>Spot Light</h2>

        <p>Spotlights are just that, spotlights.</p>

        <p>They are lights that point in one direction radiating out in a cone shape.</p>

        <p>We can pass an angle to the light as a parameter which is the maximum angle of the light cone in radians.</p>

        <p>Penumbra affects the intensity or softness of outer shadow or light fall-off as it fades to darkness.</p>

        <p>Because spotlights have a direction they suffer from the same rotation behavior as directional lights,
          requiring them to point to a target object's position.</p>

        <text class="center"><b>var light = new THREE.SpotLight(color, intensity, distance, angle, penumbra, decay);</b>
        </text>

        <pre><code class="javascript">// Spot light
const spotLight = new THREE.SpotLight("white", 1); //color, intensity
spotLight.position.set(5, 5, 5); //x, y, z this is the position of the light
spotLight.target.position.set(0, 0, 0); //x, y, z this is the direction the light is pointing
scene.add(spotLight);
scene.add(spotLight.target);</code></pre>

        <p>Let's look at an example: <a href="https://codepen.io/amaraauguste/pen/wvoqYKx">Spot Light Example</a></p>

        <p>Notice how the cube is lit from the top right corner. This is because the spot light is positioned at (5, 5,
          5).</p>

        <p>Let's add a second spot light to our scene:</p>

        <pre><code class="javascript
">// Spot light 2 
const spotLight2 = new THREE.SpotLight("green", 1); //color, intensity
spotLight2.position.set(-5, -5, -5); //x, y, z this is the position of the light
spotLight2.target.position.set(0, 0, 0); //x, y, z this is the direction the light is pointing
scene.add(spotLight2);
scene.add(spotLight2.target);</code></pre>

        <h2>Phong Shading</h2>

        <p>As previously mentioned, MeshPhongMaterial is a material for shiny surfaces
          with specular highlights.</p>

        <p>Shading is calculated using a <b>Phong shading</b> model. The Phong shading model calculates <b>shading per
            pixel</b> (i.e. in the fragment shader, AKA pixel shader) which gives more accurate results/displays more
          realistic highlights.</p>

        <p>The Phong model is made up of three distinct components:</p>

        <ul>
          <li>Ambient</li>
          <li>Diffuse</li>
          <li>Specular</li>
        </ul>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/Phong_components.png?raw=true"
          alt="Phong shading model" />

        <p>And uses four vectors:</p>

        <ul>
          <li>To source: <b>L</b></li>
          <li>To viewer: <b>V</b></li>
          <li>Normal: <b>N</b></li>
          <li>Perfect reflector: <b>R</b></li>
        </ul>

        <img class="center" style="width: 60%; height: 60%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/phong%20vector.png?raw=true"
          alt="Phong shading model vectors" />

        <p>The following equation breaks it down:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/phong%20equation.png?raw=true"
          alt="Phong shading model equation" />

        <p>The first part of the equation computes the <b>diffuse light</b>: shows the
          direction of the light and can show depth in an object.</p>

        <p>The second part of the equation computes the <b>specular highlight</b>: for
          shiny objects and it reflects the bright spot on the surface. And the
          <b>shininess coefficient α</b>
        </p>

        <p>The third part of the equation computes the <b>ambient light</b>: the
          background light for the object.</p>

        <p>In summary:</p>

        <ul>
          <li><b>kd</b> is the diffuse reflection coefficient</li>
          <li><b>Id</b> is the diffuse light intensity</li>
          <li><b>ks</b> is the specular reflection coefficient</li>
          <li><b>Is</b> is the specular light intensity</li>
          <li><b>α</b> is the shininess coefficient</li>
          <li><b>ka</b> is the ambient reflection coefficient</li>
          <li><b>Ia</b> is the ambient light intensity</li>
        </ul>

        <h2>Blinn-Phong</h2>

        <p>There is also a <b>Blinn-Phong</b> shading model which is a modification of the Phong shading model.</p>

        <p>Blinn-Phong, also known as the modified Phong model, uses the same
          equation, but measures the angle between the surface normal and the
          halfway vector instead of finding the angle between the view vector and the
          reflection vector.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/c79773fa83ed58d460ee5273f4ac98a1a428f766/courses/CISC3620/images/Blinn_Vectors.svg"
          alt="Blinn-Phong shading model vectors" />

        <p>where:</p>

        <img class="center" style="width: 100%; height: 100%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/blinn%20phong%20equation.png?raw=true"
          alt="Blinn-Phong shading model" />

        <p>Although the material is called Phong it actually uses the Blinn-Phong reflection model rather than a pure
          Phong Reflection model.</p>

        <p>Let's create a new material to see this in action:</p>

        <pre><code class="javascript">const material = new THREE.MeshPhongMaterial({
   color: "red", // color to use for light, default is white
   shininess : 60, // shininess default is 30
   specular: "blue" // color of the specular highlight, default is white
});</code></pre>

        <p>Let's add some point lights so we can actually see the cube:</p>

        <pre><code class="javascript">//Create point light and add to scene 
const pointLight = new THREE.PointLight("white", 1); //color, intensity
pointLight.position.set(50, 50, 50); //x, y, z
scene.add(pointLight);


// Point light 2
const pointLight2 = new THREE.PointLight("white", 1); //color, intensity
pointLight2.position.set(-50, -50, -50); //x, y, z
scene.add(pointLight2);</code></pre>

        <p>Now, when you run the code, you should see a red cube with a blue specular highlight rendered in the scene.
        </p>

        <h2>Interactivity</h2>

        <p>So far, we made the cube rotate automatically as it is rendered. This
          time let's try to control its rotation with user interaction.</p>

        <p>To do this, we will comment out the rotational behavior and write our own.</p>

        <p>We will use the arrow keys to control how to cube rotates. How do we make
          sure that the program is aware when we press a specific key and react
          accordingly?</p>

        <p>As we saw a bit in 2D graphics, we will have to add an<b>event listener</b> to attach an event handler to a
          specified element. In this case, we do not need to specify a particular
          element so we can attach an event handler to the document directly.</p>

        <p>This is proper syntax for creating an event listener for the arrow keys:</p>

        <pre><code class="javascript">// Add listener for keyboard
document.addEventListener('keydown', keyPressed);

//behavior for directional keys
function keyPressed(e){
  switch(e.key) {
    case "ArrowLeft": //left arrow
      //move left on the y-axis
    	cube.rotation.y -= 0.1; 
    	break;
  	case "ArrowUp": //up arrow
      //move up on the x-axis
    	cube.rotation.x -= 0.1; 
    	break;
    case "ArrowRight": //right arrow
      //move right on the y-axis
    	cube.rotation.y += 0.1;
    	break;  
    case "ArrowDown": //down arrow
      //move down on the x-axis 
    	cube.rotation.x += 0.1; 
    	break;
  }
}</code></pre>

        <p>Now, when we run the code, we should be able to control the rotation of the cube using the arrow keys.</p>

        <p>We've successfully added basic interactivity to our project.</p>

        <h2>Orbit Controls</h2>

        <p>Another way to add interactivity to your Three.js project is by using OrbitControls.</p>

        <p>OrbitControls is a utility that allows you to control the camera in your scene using the mouse.</p>

        <p>It allows you to rotate, zoom, and pan the camera around the scene.</p>

        <p>Here's how you can add OrbitControls to your project:</p>

        <p>First, you need to include the OrbitControls script in your HTML:</p>

        <pre><code class="HTML5">&lt;script src="https://cdn.jsdelivr.net/npm/three@0.132.2/examples/js/controls/OrbitControls.js"&gt;&lt;/script&gt;</code></pre>


        <p>Next, you can add the following code to your JavaScript:</p>

        <pre><code class="javascript">const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>Now, when you run the code, you should be able to control the camera in your scene using the mouse.</p>

        <p>And that's it! We've successfully added both mouse and keyboard interaction.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Shadows">
      <br />
      <header><b>Shadows</b></header>
      <article>
        <p>Shadows are an important part of creating a realistic 3D scene.</p>

        <p>The light that is coming from a specific direction can cast shadows. First, we should make the scene ready
          for casting shadows.</p>

        <p>We should first tell the renderer that we want to enable shadows. Casting shadows is an expensive operation.
          WebGLRenderer only supports this functionality.</p>

        <p>It uses Shadow mapping, a technique specific to WebGL, performed directly on the GPU.</p>

        <pre><code class="javascript">renderer.shadowMapEnabled = true;</code></pre>

        <p>The above line of code tells the renderer to cast shadows in the scene.</p>

        <p>Note - Three.js, by default, uses shadow maps. Shadow map works for light that casts shadows.
          The scene renders all objects marked to cast shadows from the point of view of the light.</p>

        <p>There are two types of shadows in Three.js:</p>

        <p></p><b>Shadow Mapping</b>: This is the most common technique used to create shadows in 3D graphics. It works
        by rendering the scene from the perspective of the light source and storing the depth values of the scene in a
        texture called the shadow map. When rendering the scene from the camera's perspective, the depth values of
        the scene are compared to the depth values in the shadow map to determine if a pixel is in shadow or not.
        </p>


        <p>Shadow mapping is the method of choice for creating shadows in high-end rendering for motion pictures and
          television. However, it has been problematic to use shadow mapping in real-time applications, such as video
          games, because of aliasing problems in the form of magnified jaggies. </p>

        <p>Shadow mapping involves projecting a shadow map on geometry and comparing the shadow map values with the
          light-view depth at each pixel. If the projection causes the shadow map to be magnified, aliasing in the form
          of large, unsightly jaggies will appear at shadow borders.</p>

        <p>Aliasing can usually be reduced by using higher-resolution shadow maps and increasing the shadow map
          resolution,
          using techniques such as perspective shadow maps (Stamminger and Drettakis 2002). </p>

        <p>However, using perspective shadow-mapping techniques and increasing shadow map resolution does not work when
          the light is traveling nearly parallel to the shadowed surface, because the magnification approaches infinity.
        </p>

        <p>High-end rendering software solves the aliasing problem by using a technique called <b>percentage-closer
            filtering</b>.</p>


        <p><b>PCF (Percentage-Closer Filtering)</b>: This is a technique used to smooth out the edges of shadows by
          taking multiple samples around each pixel and averaging the results. This helps to reduce the jagged
          appearance of shadows and create a more realistic effect.</p>

        <p>Unlike normal textures, shadow map textures cannot be prefiltered to remove aliasing. Instead, multiple
          shadow map comparisons are made per pixel and averaged together.</p>

        <p>This technique is called percentage-closer filtering (PCF) because it calculates the percentage of the
          surface that is closer to the light and, therefore, not in shadow.</p>

        <p>The original PCF algorithm, described in Reeves et al. 1987, called for mapping the region to be shaded into
          shadow map space and sampling that region stochastically (that is, randomly). </p>

        <p>The algorithm was first implemented using the REYES rendering engine, so the region to be shaded meant a
          four-sided micropolygon. The figure below shows an example of that implementation.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/percentage-closer%20filtering.jpg?raw=true"
          alt="PCF shadow mapping" />

        <p>However, this algorithm advances over time.</p>

        <p>For instance, NVIDIA GPUs have changed the PCF algorithm slightly to make it easy and efficient to apply.
          Instead of
          calculating the region to be shaded in shadow map space, they simply use a 4x4-texel sample region everywhere.
        </p>

        <p>This region is large enough to significantly reduce aliasing, but not so large as to require huge numbers of
          samples or stochastic sampling techniques to achieve good results. Note that the sampling region is not
          aligned to texel boundaries. </p>

        <p>PCF is a simple and effective way to reduce aliasing in shadow maps. It is not perfect, but it is a good
          compromise between quality and performance.</p>

        <p>Since we are on the topic of antialiasing, we should also make sure that our scene is rendered with it
          enabled.
        </p>

        <pre><code class="javascript
">renderer.antialias = true;</code></pre>

        <p>By doing this we can reduce the jagged edges of our objects.</p>

        <p>Let's create a scene with shadows:</p>

        <p>First, we need to enable shadows in our renderer:</p>

        <pre><code class="javascript
">renderer.shadowMap.enabled = true;</code></pre>

        <p>Next, we need to enable shadows for each light that we want to cast shadows:</p>

        <pre><code class="javascript
">// Enable shadows for directional light
const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(5, 5, 5); // Position the light
light.castShadow = true; // Enable shadow casting for the light
scene.add(light);</code></pre>

        <p>We should configure objects to cast shadows. We can inform Three.js which objects can cast shadows and which
          objects can receive shadows.</p>

        <pre><code class="javascript">object1.castShadow = true;
object2.recieveShadow = true;</code></pre>

        <p>Let's create a simple cube that casts a shadow on a floor:</p>

        <pre><code class="javascript
">// Create a cube that casts a shadow
const cubeGeometry = new THREE.BoxGeometry(2, 2, 2);
const cubeMaterial = new THREE.MeshStandardMaterial({ color: "red" });
const cube = new THREE.Mesh(cubeGeometry, cubeMaterial);
cube.position.set(0, -1, 0);
cube.castShadow = true;
scene.add(cube);

// Create a floor that receives shadows
const floorGeometry = new THREE.PlaneGeometry(20, 20);
const floorMaterial = new THREE.MeshStandardMaterial({ color: "gray" });
const floor = new THREE.Mesh(floorGeometry, floorMaterial);
floor.rotation.x = -Math.PI / 2;
floor.position.y = -2;
floor.receiveShadow = true;

scene.add(floor);</code></pre>

        <p>Let's see how the shadow looks with a BasicShadowMap: </p>

        <pre><code class="javascript">renderer.shadowMapType = THREE.BasicShadowMap;</code></pre>

        <p>If our shadow looks a bit blocky around its edges, it means the shadow map is too small.</p>

        <p>To increase the shadow map size, we can define shadowMapHeight and shadowMapWidth properties for the light.
        </p>

        <p>Alternatively, we can also try to change the shadowMapType property of WebGLRenderer. We can set this to
          THREE.BasicShadowMap, THREE.PCFShadowMap, or THREE.PCFSoftShadowMap.</p>

        <p>Now let's try to antialias the shadow:</p>

        <pre><code class="javascript">// to antialias the shadow
renderer.shadowMapType = THREE.PCFSoftShadowMap;
// or
directionalLight.shadowMapWidth = 2048;
directionalLight.shadowMapHeight = 2048;</code></pre>

        <h2>Exercise: Let's Create a Scene with Shadows</h2>

        <p>Now that we've learned how to add shadows to our scene, add another object that casts a shadow on the floor:
        </p>

        <p>Let's add a sphere that casts a shadow:</p>

        <pre><code class="javascript
">const sphereGeometry = new THREE.SphereGeometry(1, 32, 32);
const sphereMaterial = new THREE.MeshStandardMaterial({ color: "blue" });
const sphere = new THREE.Mesh(sphereGeometry, sphereMaterial);
sphere.position.set(5, -1, 0);
sphere.castShadow = true;
scene.add(sphere);</code></pre>

        <p>Now, when we run the code, we should see a red cube and a blue sphere casting shadows on a gray floor in
          the scene.</p>

        <p>And that's it! We've successfully created a scene with shadows.</p>

      </article>
      <br />
    </section>
    <hr />
    <section class="main-section" id="Textures">
      <br />
      <header><b>Textures</b></header>
      <article>
        <p>When we create a mesh, such as our humble cube, we pass in two components: a geometry and a material.</p>

        <pre><code class="javascript">const geometry = new THREE.BoxGeometry(2, 2, 2);
const material = new THREE.MeshStandardMaterial({color: 'purple'});
const cube = new THREE.Mesh(geometry, material);
scene.add(cube);</code></pre>

        <p>The geometry defines the mesh's shape, and the material defines various surface properties of the mesh, in
          particular, how it reacts to light. The geometry and the material, along with any light and shadows affecting
          the mesh, control the appearance of the mesh when we render the scene.</p>

        <p>Currently, our scene contains a single mesh with a shape defined by a BoxGeometry and a surface defined by a
          MeshStandardMaterial with the color parameter set to purple.</p>

        <p>Let's illuminate the scene by a single DirectionalLight, so when we render the scene, the result is this
          simple purple box:</p>

        <pre><code class="javascript">//Create a DirectionalLight
const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(5,2,15); //x, y, z
scene.add(light);</code></pre>

        <p>Compare this to a concrete box in the real world - or a wooden box, or a metal box, or a box made from nearly
          any substance except smooth plastic, and we can immediately see that our 3D box is not at all realistic.
          Objects in the real world are usually scratched, broken, and dirty.</p>

        <p>However, the material applied to our box doesn't look like this. Rather, it consists of a single color
          applied smoothly over the entire surface of the mesh. Unless we want all of our creations to look like
          brand-new plastic, this won't do.</p>

        <p>Materials have many parameters besides color, and we can use these to adjust various attributes of an
          object's surface, like the roughness, metalness, opacity, and so on.</p>

        <p>However, just like the color parameter, these parameters are applied uniformly over the entire surface of the
          mesh.</p>

        <p>If we increase the material's .roughness property, for example, the entire surface of the object will become
          rougher. Just like if we set the .color to red, the entire object will become red.</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({color: 'purple', roughness: 0.0});</code></pre>

        <p>When we set the roughness property to 0.0, the surface of the object becomes perfectly smooth. If we set it
          to 1.0, the surface becomes perfectly rough.</p>

        <p>How does this affect the cube that we are currently looking at?</p>

        <p>By contrast, the surface properties of most real-world objects change from one point to the next.</p>

        <p>Once again, it consists of a geometry and a material, just like our cube mesh.</p>

        <p>The large scale features, like the eyes, nose, ears, neck, and chin, are defined by the geometry.</p>

        <p>However, a lot more than a well-crafted geometry goes into creating a realistic face. Looking closely at the
          skin, we can see there are many small bumps, wrinkles, and pores, not to mention eyebrows, lips, and a slight
          beard.</p>

        <p>When creating a complex model like a face, an artist must decide what parts of the model to represent using
          geometry, and what parts to represent at the material level, bearing in mind that it's usually cheaper to
          represent things using the material than the geometry.</p>

        <p>This is an especially important consideration when the model has to run on a mobile device, where high
          performance is paramount.</p>

        <p>For example, while it would be possible to model every hair in the eyebrows in geometry, doing so would make
          this model unsuitable for real-time use on all but the most powerful of devices. Instead, we must represent
          small features like hair at the material level, and reserve the geometry for large scale features like the
          eyes, nose, and ears.</p>

        <p>Note, also, that this face is made from a single geometry. We usually want to avoid splitting a geometry up
          more than necessary since every mesh can have only one geometry, so each separate geometry corresponds to a
          new mesh in our scene.</p>

        <p>Having fewer objects in a scene usually results in better performance, and it's also easier for both the
          developer and the 3D artist to work with. In other words, we don't want to be forced to create different
          geometries for the ears, and eyes. In any case, this wouldn't be practical. Looking closely at the lips, we
          can see there is no sharp divide between the red of the lips and the skin tone of the chin. This means we need
          some way of modifying material properties so that they can change smoothly across the surface of an object.
        </p>

        <p>We need to be able to say things like this:</p>

        <ul>
          <li>the part of the geometry making up the lips is red</li>
          <li>the part of the geometry making up the chin is a skin tone overlaid by a slight beard</li>
          <li>the part of the geometry making up the eyebrows is hair colored</li>
        </ul>

        <p>… and so on. And this doesn't only apply to color. The skin is shinier than the hair and lips, for example.
          So, we also need to be able to specify how other properties like roughness change from one point to the next
          across the geometry.</p>

        <p>This is where <b>texture mapping</b> comes in. In the simplest possible terms, texture mapping means taking
          an image
          and stretching it over the surface of a 3D object.</p>

        <p>We refer to an image used in this manner as a texture, and we can use textures to represent material
          properties like color, roughness, and opacity.</p>

        <p>While it's easy to take a 2D texture and stretch it over a regular shape like a cube, it's much harder to do
          that with an irregular geometry like a face, and over the years, many texture mapping techniques have been
          developed. Perhaps the simplest technique is <b>projection mapping</b>, which projects the texture onto an
          object (or scene) as if it has been shone through a film projector. Imagine holding your hand in front of a
          film
          projector and seeing the image projected onto your skin.</p>

        <p>While projection mapping and other techniques are still widely used for things like creating shadows (or
          simulating projectors), that's not going to work for attaching the face's color texture to the face geometry.
        </p>

        <p>Instead, we use a technique called <b>UV mapping</b> which allows us to create a connection between points on
          the geometry and points on the face.</p>

        <p>Using UV mapping, we divide the texture up into a 2D grid with the point (0,0) at the bottom left and the
          point (1,1) at the top right. Then, the point (0.5,0.5) will be at the exact center of the image.</p>

        <img class="center" style="width: 50%; height: 50%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/texture%20example%201.png?raw=true"
          alt="UV mapping" />

        <p>Likewise, every point in a geometry has a position in the 3D local space of the mesh. UV mapping, then, is
          the process of assigning 2D points in the texture to 3D points in the geometry.</p>

        <p>For example, suppose the lips in the face model are at the point (0,0,0). We can see that the lips in the
          texture are close to the center, somewhere around (0.5,0.5).</p>

        <p>So, we'll create a mapping:</p>

        <text class="center"><b>(0.5,0.5)⟶(0,0,0)</b></text>

        <p>Now, when we assign the texture as a color map in the material, the center of the texture will be mapped onto
          the lips.</p>

        <p>Next, we must do the same for many other points in the geometry, assigning the ears, eyes, eyebrows, nose,
          and chin to the appropriate points of the texture. If this sounds like a daunting procedure, don't worry,
          because it's rare to do this manually.</p>

        <p>For this model, the UV mapping was created in an external program, and in general, that's the recommended way
          to create UV mappings.</p>

        <p>Data representing the UV mapping is stored on the geometry. The Three.js geometries like the BoxGeometry have
          already got UV mapping set up, and in most cases, when you load a model like a face that was created in an
          external program, it will also have UV mapping ready for use.</p>

        <p>Once we have a geometry with a UV mapping, we can take any texture and apply it to the geometry and it will
          immediately work.</p>

        <p>However, it might be hard to find other textures that will look good with a face model since the UV map must
          be carefully coordinated to match the texture to the correct points on the face, and doing this well is the
          work of a skilled 3D artist.</p>

        <p>However, for simple shapes like a cube we can use nearly any image as a texture, turning the box into a
          wooden box, or a concrete box, or a crate, and so on.</p>

        <p>Before we proceed with loading a texture and applying it to our cube, let's go over all the technical terms
          that we'll be using when working with textures.</p>

        <h2>What's the Difference Between an Image and a Texture?</h2>

        <p>We'll see the terms <b>texture</b> and <b>image</b> a lot in computer graphics literature. These are even
          often stored in the same format, such as PNG or JPG. What’s the difference?</p>

        <ul>
          <li>An <b>image</b> is a 2D picture designed to be viewed by a human</li>
          <li>A <b>texture</b> is specially prepared data used for various purposes in 3D graphics.</li>
        </ul>

        <p>The individual pixels that make up an image represent color. Another way of looking at this is that an image
          is a 2D array of colors.</p>

        <p>In the early days of computer graphics, that was the case for textures too, but over time more and more uses
          were found for textures and now it's more correct to say that a texture is a 2D array of data.</p>

        <p>This data can represent anything. Nowadays it's even possible to store geometry or animations in a texture.
        </p>

        <p>When we use a texture in Three.js, we're usually using it to represent material properties like color,
          roughness, and opacity.</p>

        <p>Textures are applied to a mesh by creating a Texture object and passing it to the map property of the
          material used to render the mesh.</p>

        <h2>Texture Map</h2>

        <p>Although technically incorrect, a texture is also often referred to as a <b>map</b>, or even a texture map,
          although
          map is most commonly used when assigning a texture to a material.</p>

        <p>When using a texture to represent color, we'll say that we are assigning a texture to the color map slot on a
          material.</p>

        <h2>UV Mapping</h2>

        <p>UV mapping is a method for taking a 2-dimensional texture and mapping it onto a 3-dimensional geometry.</p>

        <p>Imagine a 2D coordinate system on top of the texture, with (0,0) in the bottom left and (1,1) in the top
          right. Since we already use the letters X, Y and Z for our 3D coordinates, we'll refer to the 2D texture
          coordinate using the letters U and V.</p>

        <p>This is where the name UV mapping comes from.</p>

        <p>Here's the formula used in UV mapping:</p>

        <text class="center"><b>(u,v)⟶(x,y,z)</b></text>

        <p>(u,v) represents a point on the texture, and (x,y,z) represents a point on the geometry, defined in local
          space. Technically, a point on a geometry is called a vertex.</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/3c4901326e43478f13f54ae0d1b60d7ca2b4c6ef/courses/CISC3620/images/geometry_uv_map.svg"
          alt="UV mapping" />

        <p>In the figure above, the top left corner of the texture has been mapped to a vertex on the corner of the cube
          with coordinates (−1,1,1):</p>

        <text class="center"><b>(0,1)⟶(−1,1,1)</b></text>

        <p>Similar mappings are done for the other five faces of the cube, resulting in one complete copy of the texture
          on each of the cube's six faces:</p>

        <img class="center" style="width: 75%; height: 75%"
          src="https://github.com/amaraauguste/amaraauguste.github.io/blob/master/courses/CISC3620/images/texture%20map%20example.png?raw=true"
          alt="UV mapping example" />

        <p>Note that there is no mapping for the point (0.5,0.5), the center of the texture. Only the corners of the
          texture are mapped, onto the eight corners of the cube, and the rest of the points are "guessed" from these.
        </p>

        <p>By contrast, a complex model like a face must have many more UV coordinates defined to map the parts of the
          texture representing the nose, ears, eyes, lips, and so on, to the correct points of the geometry.</p>

        <p>Fortunately, we rarely need to set up UV mapping manually since all the three.js geometries, including the
          BoxGeometry, have UV mapping built-in. We only need to load the texture and apply it to our material and
          everything will work.</p>

        <h2>How to Load a Texture</h2>

        <p>Three.js has a built-in function <b>TextureLoader()</b> to load textures into your Three.js project.</p>

        <p>First, we should create a loader:</p>

        <pre><code class="javascript">const loader = new THREE.TextureLoader();</code></pre>

        <p>Then we can load any texture or image by specifying its path in the load() function:</p>

        <pre><code class="javascript">const texture = loader.load('path/to/texture.jpg');</code></pre>

        <p>Now, we can apply the texture to the material by setting the map property of the material to the texture:</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ map: texture });</code></pre>


        <p>Now, let's add a basic texture to our cube:</p>

        <p>This is a basic image of a side of a wooden crate:</p>

        <img class="center" style="width: 30%; height: 30%"
          src="https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate.gif"
          alt="crate texture" />

        <p>First, we should create a loader, then we can load our texture or image by specifying its path in the load()
          function.</p>

        <pre><code class="javascript">const loader = new THREE.TextureLoader();
const texture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate.gif');</code></pre>

        <p>Now, we can apply the texture to the material by setting the map property of the material to the texture.</p>

        <pre><code class="javascript">const material = new THREE.MeshStandardMaterial({ map: texture });</code></pre>

        <p>And add a light source so we can actually see our crate:</p>

        <pre><code class="javascript">//Create a AmbientLight
const light = new THREE.AmbientLight( 0xffffff, 1 );
scene.add(light);</code></pre>

        <p>Now, when we render the scene, we should see the cube with the crate texture applied to its surface.</p>

        <p>Let's now create a crate positioned on a floor (plane)</p>

        <p>First let's set our scene, camera, and renderer:</p>

        <pre><code class="javascript">let width = 500;
let height = 400;
// Create the scene
const scene = new THREE.Scene();
scene.background = new THREE.Color("green"); //green screen to better view environment
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
// Position the camera
camera.position.set(0, 3, 10); // Move it back on the z-axis and up on the y-axis
          
const renderer = new THREE.WebGLRenderer();</code></pre>

        <p>Now let's enable our renderer to include shadows:</p>

        <pre><code class="javascript">renderer.shadowMap.enabled = true;</code></pre>

        <p>And set size and add our renderer to view:</p>

        <pre><code class="javascript">renderer.setSize(width, height);
document.body.appendChild(renderer.domElement); //add renderer to view</code></pre>

        <p>Don't forget our animate function:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>Now above our animate function, let's create a plane to act as our floor:</p>

        <pre><code class="javascript">// Create plane geometry and material
const planeGeometry = new THREE.PlaneGeometry(50, 50);
const planeTexture = new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/concrete%20floor.jpg');
const planeMaterial = new THREE.MeshStandardMaterial({ map: planeTexture });
const plane = new THREE.Mesh(planeGeometry, planeMaterial);
plane.rotation.x = -Math.PI / 2; // Rotate the plane to be horizontal
plane.receiveShadow = true; // Allow the plane to receive shadows
scene.add(plane);</code></pre>

        <p>Now let's create a crate to place on the floor:</p>

        <pre><code class="javascript">// Create crate geometry and material
const geometry = new THREE.BoxGeometry(5, 5, 5);
const crateTexture = new THREE.TextureLoader().load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/crate3.jpg');
const material = new THREE.MeshStandardMaterial({ map: crateTexture });

const crate = new THREE.Mesh(geometry, material);
crate.position.y = 2.5; // Position it above the plane
crate.castShadow = true; // Allow the cube to cast shadows
scene.add(crate);</code></pre>

        <p>Now when you run the code, you should see a cube positioned on a floor, however due to lack of lights it is
          hard to make out.</p>

        <p>Now let's create a directional light so we can actually illuminate our scene:</p>

        <pre><code class="javascript">const light = new THREE.DirectionalLight(0xffffff, 1);
light.position.set(7, 7, 7); // Position the light
light.castShadow = true; // Enable shadow casting for the light
scene.add(light);</code></pre>

        <p>Now we should see a cube textured to look like a wooden crate positioned on a concrete floor, illuminated by
          a directional light.</p>

        <p>Let's add some orbit controls to take a closer look at the scene:</p>

        <pre><code class="javascript
">// Create orbit controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>It looks good but upon further inspection, notice how our floor texture looks more pixelated the closer we
          get. This is because the texture is being stretched over a large area.</p>

        <p>One way to fix this is to increase the resolution of the texture. However, this can be expensive in terms of
          memory and performance.</p>

        <p>Another way to fix this is to use a technique called <b>texture tiling</b>. Texture tiling involves repeating
          the texture multiple times over the surface of the mesh.</p>

        <p>Let's add texture tiling to our floor:</p>

        <pre><code class="javascript">//to improve by repeating texture
planeTexture.wrapS = THREE.RepeatWrapping; // Repeat the texture in the x-direction
planeTexture.wrapT = THREE.RepeatWrapping; // Repeat the texture in the y-direction
planeTexture.repeat.set(4, 4); // Repeat the texture 4 times in both directions</code></pre>

        <p>Now we see the floor texture repeated multiple times over the surface of the plane, which improves the
          appearance of the texture.</p>

      </article>
      <br />
    </section>
    <hr />

    <section class="main-section" id="A_More_Complex_Scene">
      <br />
      <header><b>A More Complex Scene</b></header>
      <article>
        <p>So far, we've learned how to create a scene, add objects to it, add lights, cast shadows, and apply textures
          to those objects.</p>

        <p>For example, we've played around with cubes but if we want realistic scenes that are not just made up of
          crates and boxes we need to start building more complex objects.</p>

        <p>Let's say we want to create a table. What does a table actually look like?</p>

        <p>A basic table has a flat surface and four legs. How can we create this in Three.js?</p>

        <p>One way to do this is to use <b>primitive geometries</b> like BoxGeometry to create
          custom models.</p>

        <p>For example, to create a table, we can use a BoxGeometry for the table top and four additional BoxGeometries
          for the legs.</p>

        <p>First, let's set up our scene:</p>

        <pre><code class="javascript">let width = 500;
let height = 400;
// Create the scene
const scene = new THREE.Scene();
const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
// Position the camera
camera.position.z = 10; // Move it back on the z-axis
camera.position.y = 2; // Move it up on the y-axis
const renderer = new THREE.WebGLRenderer();
renderer.shadowMap.enabled = true; // Enable shadows in the renderer
renderer.setSize(width, height);
renderer.antialias = true; // Enable antialiasing
document.body.appendChild(renderer.domElement); //add renderer to view</code></pre>

        <p>Let's also add orbit controls so we can get a better view of our scene:</p>

        <pre><code class="javascript">// Create orbit controls
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();</code></pre>

        <p>And don't forget our animate() function so we can actually see our scene:</p>

        <pre><code class="javascript">function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>Let's add a floor for our objects to sit atop:</p>

        <pre><code class="javascript">// Create a floor that receives shadows
const floorGeometry = new THREE.PlaneGeometry(20, 20);
const floorMaterial = new THREE.MeshBasicMaterial({ color: "gray" });
const floor = new THREE.Mesh(floorGeometry, floorMaterial);
floor.rotation.x = -Math.PI / 2; // Rotate the floor to be horizontal
floor.position.y = -2;
floor.receiveShadow = true;
scene.add(floor);</code></pre>

        <p>Now let's actually create our table. First we need a table top:</p>

        <pre><code class="javascript
">// Create table top geometry and material
const tableTopGeometry = new THREE.BoxGeometry(8.5, 0.75, 4); //width, height, depth
const tableTopMaterial = new THREE.MeshBasicMaterial({ color: "brown" });
const tableTop = new THREE.Mesh(tableTopGeometry, tableTopMaterial);
tableTop.position.y = 0.85; // Position the table top above the floor
tableTop.castShadow = true; // Allow the table top to cast shadows
scene.add(tableTop);</code></pre>

        <p>Now let's create the legs of the table:</p>

        <pre><code class="javascript">const legGeometry = new THREE.BoxGeometry(0.5, 3, 0.5); //width, height, depth
const legMaterial = new THREE.MeshBasicMaterial({ color: "brown" });
const leg1 = new THREE.Mesh(legGeometry, legMaterial);
const leg2 = new THREE.Mesh(legGeometry, legMaterial);
const leg3 = new THREE.Mesh(legGeometry, legMaterial);
const leg4 = new THREE.Mesh(legGeometry, legMaterial);
leg1.position.set(-4, -1, 1.75); // Position the legs at the corners of the table top
leg2.position.set(-4, -1, -1.75);
leg3.position.set(4, -1, 1.75);
leg4.position.set(4, -1, -1.75);
leg1.castShadow = true;
leg2.castShadow = true;
leg3.castShadow = true;
leg4.castShadow = true;
scene.add(leg1, leg2, leg3, leg4);</code></pre>

        <p>This is okay but what if we needed to reposition the table now? Because we added each part of the table to
          the scene as it's own individual object, we would have to manually adjust the
          position of each leg. This is not ideal.</p>

        <p>Instead of adding each part of the table as a separate object, we can group them together using a
          <b>Group</b> object.
        </p>

        <p>Let's group the table top and legs together:</p>

        <pre><code class="javascript">// Create a group to hold the table top and legs
const table = new THREE.Group();
table.add(tableTop, leg1, leg2, leg3, leg4);
scene.add(table);</code></pre>

        <p>Now, when we want to move the table, we can simply move the table object and all of its children will move
          with it.</p>

        <p>Let's better position the table so the legs are not going through the floor:</p>

        <pre><code class="javascript">table.position.y = 0.5;</code></pre>

        <p>We should now have a simple table in our scene. We can add more objects to the scene in a similar way to
          create more complex scenes.</p>

        <p>Notice that we used MeshBasicMaterial since we did not set up any lights. Let's fix that:</p>

        <p>First, we need to change our material to something that uses lights, i.e MeshStandardMaterial.</p>

        <p>When we change the material of the table, we should see a simple black silhouette of the brown table we once
          had. Don't worry we will see it again once we add light.</p>

        <p>Now, let's add some light:</p>

        <pre><code class="javascript
">// Create a directional light
const light = new THREE.DirectionalLight("white", 1);
light.position.set(7, 7, 7);
light.castShadow = true;

// Add the light to the scene
scene.add(light);</code></pre>

        <p>Now we should see a table with legs positioned on a floor, illuminated by a directional light.</p>

        <p>Let's make sure to update our floor material as well. Now we should see a shadow of the table being cast onto
          the floor.</p>

        <p>Our table looks good but it's a bit plain. Let's add a texture to the table top:</p>

        <p>First, let's load our texture:</p>

        <pre><code class="javascript">// Create a texture loader
const loader = new THREE.TextureLoader();
const tableTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/wood%20table%20top%20texture.jpg');</code></pre>

        <p>Now, let's apply the texture to the table top:</p>

        <pre><code class="javascript">const tableTopMaterial = new THREE.MeshStandardMaterial({ map: tableTexture });</code></pre>

        <p>And do the same to the legs:</p>

        <pre><code class="javascript">const legMaterial = new THREE.MeshStandardMaterial({ map: tableTexture });</code></pre>

        <p>Now our table should have a wooden texture applied to the table top and legs and look more realistic.</p>

        <p>In addition to our standard primitive geometries like BoxGeometry and PlaneGeometry, Three.js also provides
          a number of built-in geometries for creating more complex objects, for example: TeapotGeometry</p>

        <p>We'll add a teapot to our scene but first let's understand TeapotGeometry:</p>

        <p>TeapotGeometry is a geometry that represents a teapot. It's a complex geometry that is not built into
          Three.js by default, but we can add it by importing it from the Three.js examples.</p>

        <p>TeapotGeometry has multiple parameters that can be set to customize the teapot, such as size, segments, and
          detail.</p>

        <p>However, the only required parameter is the size of the teapot.</p>

        <p>We need to add the following script to our HTML file to import the TeapotGeometry:</p>

        <pre><code class="HTML">&lt;script src="https://cdn.jsdelivr.net/npm/three@0.128.0/examples/js/geometries/TeapotGeometry.js"&gt;&lt;/script&gt;</code></pre>

        <p>Now we can create a teapot and add it to our scene:</p>

        <pre><code class="javascript
">// Create teapot geometry and material
const teapotGeometry = new THREE.TeapotGeometry(0.75); // Set the size of the teapot
const teapotMaterial = new THREE.MeshStandardMaterial({ color: "white" });
const teapot = new THREE.Mesh(teapotGeometry, teapotMaterial);
teapot.position.y = 2.5; // Position the teapot to sit atop the table
teapot.castShadow = true;
scene.add(teapot);</code></pre>

        <p>We should now see a white teapot sitting atop our wooden table. However, there is no shadow on the table from
          our teapot. This is because although we have enable shadow casting for the teapot, we have not enabled shadow
          receiving for the table top.</p>

        <p>Let's enable shadow receiving for the table top:</p>

        <pre><code class="javascript">// Enable shadow receiving for the table
tableTop.receiveShadow = true;</code></pre>

        <p>Now we should see a shadow of the teapot being cast onto the table.</p>

        <p>Now we have a simple scene with a table and a teapot. We can add more objects to the scene in a similar way
          to create more complex scenes.</p>

        <p>Let's add texture to our floor:</p>

        <pre><code class="javascript">const floorTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/wood%20floor.jpg');</code></pre>

        <p>And remember to update our floorMaterial to apply the texture:</p>

        <pre><code class="javascript
">const floorMaterial = new THREE.MeshStandardMaterial({ map: floorTexture });</code></pre>

        <p>And now our scene looks more realistic than it did when we began!</p>

        <p>We can modify this scene by adding more objects, such as walls, furniture, lights, etc.</p>

        <p>Let's create walls for our scene so we have a room. We will only need three walls as the fourth will be
          occupied by our camera.</p>

        <p>First, let's create a wall:</p>

        <pre><code class="javascript
">// Create wall geometry and material
const wallGeometry = new THREE.PlaneGeometry(20, 15);
const wallMaterial = new THREE.MeshStandardMaterial({ color: "#a0d6b4" }); // Turquoise Green
const wall = new THREE.Mesh(wallGeometry, wallMaterial);
wall.position.z = -10; // Position the wall behind the table
wall.position.y = 5.5; // Position the wall at the same height as the table
wall.receiveShadow = true; // Allow the wall to receive shadows
scene.add(wall);</code></pre>

        <p>We need to ensure that the wall has two sides or else we will only see the wall from one side. We can do this
          by setting the side property of the material to THREE.DoubleSide:</p>


        <pre><code class="javascript">wall.material.side = THREE.DoubleSide;</code></pre>

        <p>While on the topic, let's also make sure our floor has two sides:</p>

        <pre><code class="javascript">floor.material.side = THREE.DoubleSide;</code></pre>


        <p>Now we should see a turquoise green wall behind our table. Let's add two more walls:</p>

        <pre><code class="javascript">const wall2 = wall.clone(); // Clone the wall
wall2.rotation.y = Math.PI / 2; // Rotate the wall to be perpendicular to the first wall
wall2.position.x = -10; // Position the wall to the left of the table
wall2.position.z = 0.0001; // Position so edges of the walls meet
scene.add(wall2);

const wall3 = wall.clone();
wall3.rotation.y = Math.PI / 2; // Rotate the wall to be perpendicular to the first wall
wall3.position.x = 10; // Position the wall to the right of the table
wall3.position.z = 0.0001; // Position so edges of the walls meet
scene.add(wall3);</code></pre>

        <p>Now how our right wall is shadowed, that is due to lack of light on that side. Let's add a point light above
          (like a ceiling light) to view better:</p>

        <pre><code class="javascript">// Create a point light
const light2 = new THREE.PointLight("white", 1);
light2.position.set(0, 20, 0);
light2.castShadow = true;
scene.add(light2);</code></pre>

        <p>Now our scene should be more illuminated.</p>

        <p>How does having these two different lights in different positions affect our scene? How does it affect our
          shadows?</p>

        <p>What does our scene look like if we rid of our directional light? How does having just the center point light
          change the scene?</p>


        <p>If our teapot's shadow looks a bit blocky from the point light, we can fix this by applying some antialiasing:</p>
        <pre><code class="javascript">// Antialias the point light shadow
light2.shadow.mapSize.width = 2048; // Increase shadow map size for better quality
light2.shadow.mapSize.height = 2048;
light2.shadow.radius = 4; // Increase shadow radius for softer shadows</code></pre>

        <p>This should make our shadow look smoother.</p>

        <p>And there we have it! We've used what we've learned so far to build a more complex scene with a table,
          teapot, walls, and lights.</p>

        <p>Now think about tables that we have seen in reality, they can vary from material. A standard table like our
          object can be made out of wood (like our example), marble (for which we would use a different texture and a
          shinier material),
          or even glass.</p>

        <p>Let's talk about the properties of glass.</p>

        <p>Glass is transparent, meaning light can pass through it. This is different from our current table material
          which is opaque, meaning light cannot pass through it.</p>

        <p>In order to create transparent objects, first we need to set the transparent property of our material to
          true.
          We also need to set the opacity property to a value between 0 and 1, where 0 is completely transparent and 1
          is
          completely opaque.</p>

        <p>Let's create a glass table:</p>

        <pre><code class="javascript">const tableMaterial = new THREE.MeshPhongMaterial({ color : "pink", 
    transparent : true, 
    opacity: 0.75
}); //opacity will not work if transparent is not set to true</code></pre>

        <p>Now we should see a pink glass table in our scene.</p>

        <p>Let's add a GUI to toggle our lights on and off:</p>

        <p>First, we need to add the following script to our HTML file to import the GUI library:</p>

        <pre><code class="HTML">&lt;script src="https://cdn.jsdelivr.net/npm/dat.gui/build/dat.gui.min.js"&gt;&lt;/script&gt;</code></pre>

        <p>Now we can create a GUI to toggle our lights on and off:</p>

        <pre><code class="javascript">// GUI setup
const gui = new dat.GUI();
const guiParams = {
    directionalLight: true, // Directional light toggle
    pointLight: true, // Point light toggle
};</code></pre>

        <p>Now we can add controls to our GUI to toggle our lights:</p>

        <pre><code class="javascript">gui.add(guiParams, 'directionalLight').name('Directional Light').onChange((value) => {
    light.visible = value;
});
      
gui.add(guiParams, 'pointLight').name('Point Light').onChange((value) => {
    light2.visible = value;
});</code></pre>

        <p>Now we should see a GUI with toggles for our directional and point lights.</p>

        <p>Let's try a different scene, a tree on some grass.</p>

        <p>Let's set our scene:</p>

        <pre><code class="javascript">// Scene setup
let width = 500;
let height = 400;
const scene = new THREE.Scene();
scene.background = new THREE.Color("skyblue"); // Set sky color

const camera = new THREE.PerspectiveCamera(75, width / height, 0.1, 1000);
const renderer = new THREE.WebGLRenderer();
renderer.setSize(width, height);
renderer.shadowMap.enabled = true;
renderer.antialias = true;
document.body.appendChild(renderer.domElement);
        
// Camera position
camera.position.set(0, 1, 10); // Set camera position so we can view the scene
       
// Add OrbitControls to enable mouse movement
const controls = new THREE.OrbitControls(camera, renderer.domElement);
controls.update();




function animate() {
    requestAnimationFrame(animate);
    renderer.render(scene, camera);
}
animate();</code></pre>

        <p>And ensure that we have an light source:</p>

        <pre><code class="javascript">// Create a directional light
const light = new THREE.DirectionalLight("white", 1);
light.position.set(-10, 7, 7);
light.castShadow = true;
scene.add(light);</code></pre>

        <p>Now we need our grass ground:</p>

        <pre><code class="javascript">// Create grass geometry and material
const groundGeometry = new THREE.PlaneGeometry(100, 100);
const groundMaterial = new THREE.MeshStandardMaterial({ color: "green", side: THREE.DoubleSide });
const ground = new THREE.Mesh(groundGeometry, groundMaterial);
ground.rotation.x = -Math.PI / 2; // Rotate to make it flat
ground.receiveShadow = true;
scene.add(ground);</code></pre>

        <p>And now we'll write a function to make a pine tree. First, think about how a pine tree looks. A standard one
          has a triangular shape with a brown trunk and green leaves. We can create this using a ConeGeometry for the
          leaves and a CylinderGeometry for the trunk.</p>

        <p>Let's create a pine tree:</p>

        <pre><code class="javascript">// Function to create a pine tree
function createPineTree() {
    const tree = new THREE.Group(); // Group to hold tree components

    // Create the trunk of the tree
    const trunkGeometry = new THREE.CylinderGeometry(0.1, 0.25, 2, 8); // Radius top, radius bottom, height, number of segments
    const trunkMaterial = new THREE.MeshStandardMaterial({ color: "brown" }); // Brown color for the trunk
    const trunk = new THREE.Mesh(trunkGeometry, trunkMaterial);
    trunk.castShadow = true;
    trunk.position.y = 0.5;  // Position trunk at the bottom of the tree
    tree.add(trunk);  // Add trunk to the tree group

    // Create foliage using cones
    const foliageGeometry = new THREE.ConeGeometry(0.8, 1.5, 8); // Radius, height, number of segments
    const foliageMaterial = new THREE.MeshStandardMaterial({ color: "green" }); // Green color for foliage

    // Create multiple layers of foliage for a better shape
    for (let i = 0; i < 3; i++) {
        const foliage = new THREE.Mesh(foliageGeometry, foliageMaterial);
        foliage.position.y = 1 + i * 0.5;  // Adjust position for layers
        foliage.rotation.y = Math.random() * Math.PI;  // Random rotation
        foliage.castShadow = true;
        tree.add(foliage);  // Add foliage layer to tree
    }

    return tree;  // Return the complete tree object
}</code></pre>

        <p>Now we can add our pine tree to the scene:</p>

        <pre><code class="javascript">// Create a pine tree
const pineTree = createPineTree();
pineTree.scale.set(2,2,2); // Scale the tree to make it larger
pineTree.position.y = 1; // Position the tree above the ground
pineTree.position.x = -5; // Position the tree to the left
scene.add(pineTree);</code></pre>

        <p>Now we should see a pine tree (and it's shadow) on some grass in our scene.</p>

        <p>Let's add a simple house next to the tree:</p>

        <pre><code class="javascript">// Create a simple house
const houseStructure = new THREE.Group(); // to group all our objects
const houseGeometry = new THREE.BoxGeometry(4, 4, 4);
const houseMaterial = new THREE.MeshStandardMaterial({ color: 0xffcc00 });
const house = new THREE.Mesh(houseGeometry, houseMaterial);
house.castShadow = true;
house.position.set(0, 2, 0); 
houseStructure.add(house);

// Create a simple roof for the house
const roofGeometry = new THREE.ConeGeometry(2.8, 2, 4);
const roofMaterial = new THREE.MeshStandardMaterial({ color: 0x8B0000 });
const roof = new THREE.Mesh(roofGeometry, roofMaterial);
roof.castShadow = true;
roof.position.set(0, 5, 0);
roof.rotation.y = Math.PI / 4; // Rotate to place it correctly on top of the house
houseStructure.add(roof);

// Create a door for the house
const doorGeometry = new THREE.BoxGeometry(1, 2, 0.1);
const doorMaterial = new THREE.MeshStandardMaterial({ color: 0x654321 }); // Door color
const door = new THREE.Mesh(doorGeometry, doorMaterial);
door.position.set(0, 1, 2); // Position the door
houseStructure.add(door);

// Create windows for the house
const windowGeometry = new THREE.BoxGeometry(1, 1, 0.1);
const windowMaterial = new THREE.MeshPhongMaterial({ color: 0xadd8e6 }); // Window color

// Left window
const leftWindow = new THREE.Mesh(windowGeometry, windowMaterial);
leftWindow.position.set(-1.25, 2, 2); // Position left window
houseStructure.add(leftWindow);
        
// Right window
const rightWindow = new THREE.Mesh(windowGeometry, windowMaterial);
rightWindow.position.set(1.25, 2, 2); // Position right window
houseStructure.add(rightWindow);

scene.add(houseStructure);</code></pre>

        <p>Let's move the house forward a bit:</p>

        <pre><code class="javascript
">houseStructure.position.z = 2;</code></pre>

        <p>And we should move out camera back on the z-axis to reflect this.</p>

        <p>Now we should see a pine tree, a house, and some grass in our scene.</p>

        <p>Let's add a GUI to toggle our objects on and off:</p>

        <pre><code class="javascript">const gui = new dat.GUI();
const guiParams = {
    tree: true, // Tree toggle
    house: true, // House toggle
};

gui.add(guiParams, 'tree').name('Pine Tree').onChange((value) => {
    pineTree.visible = value;
});

gui.add(guiParams, 'house').name('House').onChange((value) => {
    houseStructure.visible = value;
});</code></pre>

        <p>Now we should be able to toggle our pine tree and house visible by our GUI.</p>

        <p>Let's modify our house with some textures for the base, roof, and door.</p>

        <p>Let's create our texture loader:</p>

        <pre><code class="javascript">// Create a texture loader
const loader = new THREE.TextureLoader();</code></pre>

        <p>Now let's load our textures:</p>

        <pre><code class="javascript"">const houseTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/brown%20house%20texture.jpg');
const roofTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/red%20roof%20texture.png');
const doorTexture = loader.load('https://raw.githubusercontent.com/amaraauguste/amaraauguste.github.io/refs/heads/master/courses/CISC3620/textures/pixel_door.png');</code></pre>

        <p>Now let's apply the textures to our house:</p>

        <pre><code class="javascript">const houseMaterial = new THREE.MeshStandardMaterial({ map: houseTexture });
const roofMaterial = new THREE.MeshStandardMaterial({ map: roofTexture });
const doorMaterial = new THREE.MeshStandardMaterial({ map: doorTexture });</code></pre>

        <p>And now our house should have textures applied to the base, roof, and door.</p>

        <p>Let's improve our roof with some texture tiling:</p>

        <pre><code class="javascript
">roofTexture.wrapS = THREE.RepeatWrapping; // Repeat the texture in the x-direction
roofTexture.wrapT = THREE.RepeatWrapping; // Repeat the texture in the y-direction
roofTexture.repeat.set(4, 4); // Repeat the texture 4 times in both directions</code></pre>

        <p>Now our roof should have a tiled texture applied to it.</p>

        <p>And there we have it! We've used what we've learned so far to successfully build a more complex (although
          still rather simple) scene both indoors and outdoors.</p>


      </article>
      <br />
    </section>
















    <hr />
    <section class="main-section" id="Reference">
      <br />
      <header><b>Reference</b></header>
      <article>
        <p>Documentation on this page is taken from the following:</p>
        <ul>
          <li>
            Eck, D. J. (2023). Introduction to computer graphics (Version 1.4). Hobart and William Smith Colleges.
            Retrieved from https://math.hws.edu/graphicsbook/
          </li>
          <li>
            <a href="https://www.tutorialspoint.com/computer_graphics/index.htm" target="_blank">Tutorials Point</a>
          </li>
          <li>
            <a href="https://learn.leighcotnoir.com/artspeak/elements-color/hue-value-saturation/"
              target="_blank">Learn. (Leigh Cotnoir's art and design courses)</a>
          </li>
          <br />

        </ul>
      </article>
    </section>
  </main>
  <script>
    var dropdown = document.getElementsByClassName("dropdown-btn");
    var i;

    for (i = 0; i < dropdown.length; i++) {
      dropdown[i].addEventListener("click", function () {
        this.classList.toggle("active");
        var dropdownContent = this.nextElementSibling;
        if (dropdownContent.style.display === "block") {
          dropdownContent.style.display = "none";
        } else {
          dropdownContent.style.display = "block";
        }
      });
    }
  </script>
  <script>
    // Function to close the dropdown menu
    function closeDropdown() {
      var dropdown = document.querySelector(".dropdown-container");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 2 dropdown menu
    function closeDropdown2() {
      var dropdown = document.querySelector(".container2");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 3 dropdown menu
    function closeDropdown3() {
      var dropdown = document.querySelector(".container3");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown5() {
      var dropdown = document.querySelector(".container5");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 7 dropdown menu
    function closeDropdown7() {
      var dropdown = document.querySelector(".container7");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 9 dropdown menu
    function closeDropdown9() {
      var dropdown = document.querySelector(".container9");
      dropdown.style.display = "none";
    }
    // Function to close the chapter 4 and chapter 6 dropdown menu
    function closeDropdown46() {
      var dropdown = document.querySelector(".container46");
      dropdown.style.display = "none";
    }

    // Function to close the chapter 8 dropdown menu
    function closeDropdown8() {
      var dropdown = document.querySelector(".container8");
      dropdown.style.display = "none";
    }

    // Function to close the Number Systems dropdown menu
    function closeDropdownNS() {
      var dropdown = document.querySelector(".containerNS");
      dropdown.style.display = "none";
    }
  </script>
  <script>
    // Check for saved dark mode preference
    if (localStorage.getItem('darkMode') === 'enabled') {
      document.body.classList.add('dark-mode');
      document.querySelector('.change').textContent = 'ON';
    }

    // Dark mode toggle functionality
    document.querySelector('.mode').addEventListener('click', function () {
      document.body.classList.toggle('dark-mode');

      // Get the change element
      const changeModeText = document.querySelector('.change');

      // Update the text based on dark mode status
      if (document.body.classList.contains('dark-mode')) {
        changeModeText.textContent = 'ON'; // Change the text to 'ON'
        localStorage.setItem('darkMode', 'enabled'); // Save preference
      } else {
        changeModeText.textContent = 'OFF'; // Change the text to 'OFF'
        localStorage.setItem('darkMode', 'disabled'); // Save preference
      }
    });

  </script>
  <script>
    function toggleMode() {
      const lightModeLink = document.getElementById('light-mode');
      const darkModeLink = document.getElementById('dark-mode');
      const modeText = document.querySelector('.change');

      if (lightModeLink.disabled) {
        lightModeLink.disabled = false;
        darkModeLink.disabled = true;
        modeText.textContent = "OFF"; // Update the mode text to show Dark mode is off
      } else {
        lightModeLink.disabled = true;
        darkModeLink.disabled = false;
        modeText.textContent = "ON"; // Update the mode text to show Dark mode is on
      }
    }

    // Call highlight.js to apply syntax highlighting
    document.addEventListener('DOMContentLoaded', () => {
      hljs.highlightAll();
    });
  </script>
</body>

</html>
